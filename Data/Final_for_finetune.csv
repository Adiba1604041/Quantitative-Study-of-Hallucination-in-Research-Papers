,Title,Abstract,Paragraph,Label
0,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Figure 1 shows that with MLE, distractor sentences are assigned lower probabilities than the references at the first few time steps, but are assigned similar, potentially even higher probabilities at later time steps. This establishes a connection between exposure bias and the beam search problem, i.e. the problem that increasing the search space can lead
6The uncertainty of the baseline is due to label smoothing. 7For intermediate checkpoints, see Appendix, Figure 2. 8Figures are shown in the Appendix (Figure 3).
to worse model performance.9 With larger beam size, it is more likely that hallucinations survive pruning at the first few time steps, and with high probabilities assigned to them at later time steps, there is a chance that they become the top-scoring translation.
We investigate whether the beam search problem is mitigated by MRT. In Table 6, we report OOD BLEU and the proportion of hallucinations with beam sizes of 1, 4 and 50. While MRT does not eliminate the beam search problem, performance drops less steeply as beam size increases. With beam size 4, our MRT models outperform the MLE baseline by 0.5-0.8 BLEU; with beam size 50, this difference grows to 0.6-1.5 BLEU. Our manual evaluation (N=200 for each system for beam size 1 and 50) shows that the proportion of hallucinations increases with beam size, and that MRT consistently reduces the proportion by 11-21% (relative). For the system with label smoothing, the relative increase in hallucinations with increasing beam size is also smaller with MRT (+33%) than with MLE (+44%).
9The beam search problem has previously been linked to length bias (Yang et al., 2018; Murray and Chiang, 2018) and the copy mode (Ott et al., 2018). We consider hallucinations another result of using large search spaces with MLE models.",positive
1,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","An alternative way is to approximate the expectation term by using the reparameterized form of a sample s from qÃÂÃÂÃÂÃÂ(s|x), as was done in the previous uncorrelated generative hashing models (see (6) and (7)). Compared to existing simple variational distributions, there is no existing work on how to reparameterize the complicated Boltzmannmachine distribution. To this end, we first show that the Boltzmann-machine distribution can be equivalently written as the composition of an approximate correlated Gaussian distribution and a Bernoulli distribution. Proposition 1. A Boltzmann-machine distribution b(s) = 1Z e 1 2 sTÃÂÃÂÃÂÃÂ£s+ÃÂÃÂÃÂÃÂµT s with ÃÂÃÂÃÂÃÂ£ 0 can be equivalently expressed as the composition of two distributions, that is,
b(s) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ« p(s|r)p(r)dr, (12)
where p(r) = 1Z ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm i=1(e
ri + 1) ÃÂÃÂÃÂÃÂ· N (r;ÃÂÃÂÃÂÃÂµ,ÃÂÃÂÃÂÃÂ£); p(s|r) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm i=1 p(si|ri) with si and ri denoting the i-th element of s and r; and p(si|ri) , Bernoulli(ÃÂÃÂÃÂÃÂ(ri)) with ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ·) being the sigmoid function.
Proof. See Appendix A.1 for details.
Based on Proposition 1, we can see that a sample from the Boltzmann-machine distribution qÃÂÃÂÃÂÃÂ(s|x) in (10) can be sampled hierarchically as
r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ qÃÂÃÂÃÂÃÂ(r|x) and s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Bernoulli(ÃÂÃÂÃÂÃÂ(r)), (13)
where
qÃÂÃÂÃÂÃÂ(r|x)= 1
Z mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 (eri + 1) ÃÂÃÂÃÂÃÂ· N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x))
(14) and ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ·) is applied to its argument element-wise. From the expression of qÃÂÃÂÃÂÃÂ(r|x), we can see that for small values of ri, the influence of (eri + 1) on the overall distribution is negligible, and thus qÃÂÃÂÃÂÃÂ(r|x) can be well approximated by the Gaussian distribution N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)). For relatively large ri, the term (eri + 1) will only influence the distribution mean, roughly shifting the Gaussian distribution N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)) by an amount approximately equal to its variance. For problems of interest in this paper, the variances of posterior distribution are often small, hence it is reasonable to approximate samples from qÃÂÃÂÃÂÃÂ(r|x) by those from N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)).
With this approximation, we can now draw samples from Boltzmann-machine distribution qÃÂÃÂÃÂÃÂ(s|x) in (10) approximately by the two steps below
r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)), (15) s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Bernoulli(ÃÂÃÂÃÂÃÂ(r)). (16)
For the Gaussian sample r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (r;ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x),ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)), similar to (6), it can be reparameterized as
r = ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) + LÃÂÃÂÃÂÃÂ(x) ÃÂÃÂÃÂÃÂ· , (17)
where LÃÂÃÂÃÂÃÂ(x) is the Cholesky decomposition matrix of ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x) with ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x) = LÃÂÃÂÃÂÃÂ(x)LTÃÂÃÂÃÂÃÂ (x); and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rm with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (0, I). It should be noted that in practice, we can define the function LÃÂÃÂÃÂÃÂ(x) in advance and then obtain ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x) as ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x) = LÃÂÃÂÃÂÃÂ(x)LTÃÂÃÂÃÂÃÂ (x), thus the Cholesky decomposition is not needed.
Given the Gaussian sample r, similar to the reparameterization of Bernoulli variables in (7), we can reparameterize the Bernoulli sample s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼
Bernoulli(ÃÂÃÂÃÂÃÂ(r)) as s = sign(ÃÂÃÂÃÂÃÂ(r)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂu)+12 , where u ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rm with each element ui ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ uniform(0, 1). By combining the above reparameterizations, a sample from the Boltzmann-machine distribution qÃÂÃÂÃÂÃÂ(s|x) can then be approximately reparameterized as
sÃÂÃÂÃÂÃÂ = sign (ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x)+LÃÂÃÂÃÂÃÂ(x) ÃÂÃÂÃÂÃÂ· )ÃÂÃÂ¢ÃÂÃÂÃÂÃÂu)+1
2 , (18)
where the subscript ÃÂÃÂÃÂÃÂ is to explicitly indicate that the sample s is expressed in terms of ÃÂÃÂÃÂÃÂ.
With the reparameterization sÃÂÃÂÃÂÃÂ, the expectation term in (11) can be approximated as log
pÃÂÃÂÃÂÃÂ¸(x|sÃÂÃÂÃÂÃÂ)p(sÃÂÃÂÃÂÃÂ) e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂ)
. Consequently, the gradients of this term w.r.t. both ÃÂÃÂÃÂÃÂ¸ and ÃÂÃÂÃÂÃÂ can be evaluated efficiently by backpropagation, with the only difficulty lying at the non-differentiable function sign(ÃÂÃÂÃÂÃÂ·) of sÃÂÃÂÃÂÃÂ in (18). Many works have been devoted to estimate the gradient involving discrete random variables (Bengio et al., 2013; Jang et al., 2017; Maddison et al., 2017; Tucker et al., 2017; Grathwohl et al., 2018; Yin and Zhou, 2019). Here, we adopt the simple straight-through (ST) technique (Bengio et al., 2013), which has been found performing well in many applications. By simply treating the hard threshold function sign(ÃÂÃÂÃÂÃÂ·) as the identity function, the ST technique estimates the gradient as
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ [ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) + LÃÂÃÂÃÂÃÂ(x) )ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ u] ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ . (19)
Then, the gradient of the first term in ELBO L w.r.t. ÃÂÃÂÃÂÃÂ can be computed efficiently by backpropagation.",positive
2,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Relation extraction aims to detect relations among entities in the text and plays a significant role in a variety of natural language processing applications. Early research efforts focus on predicting relations between entities within the sentence (Zeng et al., 2014; Xu et al., 2015a,b). However, valuable relational information between entities, such as biomedical findings, is expressed by multiple mentions across sentence boundaries in real-world scenarios (Peng et al., 2017). Therefore, the scope
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Equally Contributed. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ  Work done during internship at SUTD.
of extraction in biomedical domain has recently been expanded to cross-sentence level (Quirk and Poon, 2017; Gupta et al., 2018; Song et al., 2019).
A more challenging, yet practical extension, is the document-level relation extraction, where a system needs to comprehend multiple sentences to infer the relations among entities by synthesizing relevant information from the entire document (Jia et al., 2019; Yao et al., 2019). Figure 1 shows an example adapted from the recently proposed document-level dataset DocRED (Yao et al., 2019). In order to infer the inter-sentence relation (i.e., country of citizenship) between Yulia Tymoshenko and Ukrainian, one first has to identify the fact that Lutsenko works with Yulia Tymoshenko. Next we identify that Lutsenko manages internal affairs, which is a Ukrainian authority. After incrementally connecting the evidence in the document and performing the step-by-step reasoning, we are able to infer that Yulia Tymoshenko is also a Ukrainian.
Prior efforts show that interactions between mentions of entities facilitate the reasoning process in the document-level relation extraction. Thus, Verga et al. (2018) and Jia et al. (2019) leverage MultiInstance Learning (Riedel et al., 2010; Surdeanu
et al., 2012). On the other hand, structural information has been used to perform better reasoning since it models the non-local dependencies that are obscure from the surface form alone. Peng et al. (2017) construct dependency graph to capture interactions among n-ary entities for cross-sentence extraction. Sahu et al. (2019) extend this approach by using co-reference links to connect dependency trees of sentences to construct the document-level graph. Instead, Christopoulou et al. (2019) construct a heterogeneous graph based on a set of heuristics, and then apply an edge-oriented model (Christopoulou et al., 2018) to perform inference.
Unlike previous methods, where a documentlevel structure is constructed by co-references and rules, our proposed model treats the graph structure as a latent variable and induces it in an end-to-end fashion. Our model is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Using a variant of Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007), our model is able to generate task-specific dependency structures for capturing non-local interactions between entities. We further develop an iterative refinement strategy, which enables our model to dynamically build the latent structure based on the last iteration, allowing the model to incrementally capture the complex interactions for better multi-hop reasoning (Welbl et al., 2018).
Experiments show that our model significantly outperforms the existing approaches on DocRED, a large-scale document-level relation extraction dataset with a large number of entities and relations, and also yields new state-of-the-art results on two popular document-level relation extraction datasets in the biomedical domain. The code and pretrained model are available at https: //github.com/nanguoshun/LSR 1.
Our contributions are summarized as follows:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We construct a document-level graph for inference in an end-to-end fashion without relying on co-references or rules, which may not always yield optimal structures. With the iterative refinement strategy, our model is able to dynamically construct a latent structure for improved information aggregation in the entire document.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We perform quantitative and qualitative analyses to compare with the state-of-the-art mod-
1Our model is implemented in PyTorch (Paszke et al., 2017)
els in various settings. We demonstrate that our model is capable of discovering more accurate inter-sentence relations by utilizing a multi-hop reasoning module.",positive
3,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","We construct three types of nodes for a documentlevel graph: mention nodes, entity nodes and meta dependency paths (MDP) nodes as shown in Figure 2. Mention nodes correspond to different mentions of entities in each sentence. The representation of an entity node is computed as the average of its mentions. To build a document-level graph, existing approaches use all nodes in the dependency tree of a sentence (Sahu et al., 2019) or one sentence-level node by averaging all token representations of the sentence (Christopoulou et al., 2019). Alternatively, we use tokens on the shortest dependency path between mentions in the sentence. The shortest dependency path has been widely used in the sentence-level relation extraction as it is able to effectively make use of relevant information while ignoring irrelevant information (Bunescu and Mooney, 2005; Xu et al., 2015a,b). Unlike sentence-level extraction, where each sentence only has two entities, each sentence here may involve multiple mentions.",positive
4,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Unlike existing models that use co-reference links (Sahu et al., 2019) or heuristics (Christopoulou et al., 2019) to construct a document-level graph
for reasoning, our model treats the graph as a latent variable and induces it in an end-to-end fashion. The structure induction module is built based on the structured attention (Kim et al., 2017; Liu and Lapata, 2018). Inspired by Liu and Lapata (2018), we use a variant of KirchhoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Matrix-Tree Theorem (Tutte, 1984; Koo et al., 2007) to induce the latent dependency structure.
Let ui denote the contextual representation of the i-th node, where ui ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd, we first calculate the pair-wise unnormalized attention score sij between the i-th and the j-th node with the node represen-
tations ui and uj . The score sij is calculated by two feed-forward neural networks and a bilinear transformation:
sij = (tanh(Wpui)) TWb(tanh(Wcuj)) (3)
where Wp ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂd and Wc ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂd are weights for two feed-forward neural networks, d is the dimension of the node representations, and tanh is applied as the activation function. Wb ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂd are the weights for the bilinear transformation. Next we compute the root score sri which represents the unnormalized probability of the i-th node to be selected as the root node of the structure:
sri = Wrui (4)
where Wr ÃÂÃÂ¢ÃÂÃÂÃÂÃÂR1ÃÂÃÂÃÂÃÂd is the weight for the linear transformation. Following Koo et al. (2007), we calculate the marginal probability of each dependency edge of the document-level graph. For a graph G with n nodes, we first assign non-negative weights P ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnÃÂÃÂÃÂÃÂn to the edges of the graph:
Pij = { 0 if i = j exp (sij) otherwise
(5)
where Pij is the weight of the edge between the i-th and the j-th node. We then define the Laplacian matrix L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnÃÂÃÂÃÂÃÂn of G in Equation (6), and its variant LÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnÃÂÃÂÃÂÃÂn in Equation (7) for further computations (Koo et al., 2007).
Lij =
{ÃÂÃÂ¢ÃÂÃÂÃÂÃÂn iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²=1PiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²j if i = j
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPij otherwise (6)
LÃÂÃÂÃÂÃÂij = { exp(sri ) if i = 1 Lij if i > 1
(7)
We use Aij to denote the marginal probability of the dependency edge between the i-th and the j-th node. Then, Aij can be derived based on Equation (8), where ÃÂÃÂÃÂÃÂ´ is the Kronecker delta (Koo et al., 2007).
Aij = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ´1,j)Pij [LÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1]ij ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ´i,1)Pij [LÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1]ji
(8)
Here, A ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnÃÂÃÂÃÂÃÂn can be interpreted as a weighted adjacency matrix of the document-level entity graph. Finally, we can feed A ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnÃÂÃÂÃÂÃÂn into the multi-hop reasoning module to update the representations of nodes in the latent structure.",positive
5,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Though structured attention (Kim et al., 2017; Liu and Lapata, 2018) is able to automatically induce a latent structure, recent research efforts show that the induced structure is relatively shallow and may not be able to model the complex dependencies for document-level input (Liu et al., 2019b; Ferracane et al., 2019). Unlike previous work (Liu and Lapata, 2018) that only induces the latent structure once, we repeatedly refine the document-level graph based on the updated representations, allowing the model to infer a more informative structure that goes beyond simple parent-child relations.
As shown in Figure 3, we stack N blocks of the dynamic reasoner in order to induce the documentlevel structure N times. Intuitively, the reasoner
induces a shallow structure at early iterations since the information propagates mostly between neighboring nodes. As the structure gets more refined by interactions with richer non-local information, the induction module is able to generate a more informative structure.",positive
6,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","We compare our proposed LSR with the following three types of competitive models on the DocRED dataset, and show the main results in Table 2. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Sequence-based Models. These models lever-
age different neural architectures to encode sentences in the document, including convolutional neural networks (CNN) (Zeng et al., 2014), LSTM, bidirectional LSTM (BiLSTM) (Cai et al., 2016) and attention-based LSTM (ContextAware) (Sorokin and Gurevych, 2017). ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Graph-based Models. These models construct
task-specific graphs for inference. GCNN (Sahu et al., 2019) constructs a document-level graph by co-reference links, and then applies relational GCNs for reasoning. EoG (Christopoulou et al., 2019) is the state-of-the-art document-level relation extraction model in biomedical domain. EoG first uses heuristics to construct the graph, then leverages an edge-oriented model to perform inference. GCNN and EoG are based on static structures. GAT (VelicÃÂÃÂÃÂÃÂkovicÃÂÃÂÃÂÃÂ et al., 2018) is able to learn the weighted graph structure based on a local attention mechanism. AGGCN (Guo 2https://spacy.io/ 3https://competitions.codalab.org/
competitions/20717
et al., 2019a) is the state-of-the-art sentencelevel relation extraction model, which constructs the latent structure by self-attention. These two models are able to dynamically construct taskspecific structures. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ BERT-based Models. These models fine-tune
BERT (Devlin et al., 2019) for DocRED. Specifically, Two-Phase BERT (Wang et al., 2019) is the best reported model. It is a pipeline model, which predicts if the relation exists between entity pairs in the first phase and predicts the type of the relation in the second phase.
As shown in Table 2, LSR with GloVe achieves 54.18 F1 on the test set, which is the new state-ofthe-art result for models with GloVe. In particular, our model consistently outperforms sequencebased models by a significant margin. For example, LSR improves upon the best sequence-based model BiLSTM by 3.1 points in terms of F1. This suggests that models which directly encode the entire document are unable to capture the inter-sentence relations present in documents.
Under the same setting, our model consistently outperforms graph-based models based on static graphs or attention mechanisms. Compared with EoG, our LSR model achieves 3.0 and 2.4 higher F1 on development and test set, respectively. We also have similar observations for the GCNN model, which shows that a static document-level graph may not be able to capture the complex interactions in a document. The dynamic latent structure induced by LSR captures richer non-local dependencies. Moreover, LSR also outperforms GAT and AGGCN. This empirically shows that
compared to the models that use local attention and self-attention (VelicÃÂÃÂÃÂÃÂkovicÃÂÃÂÃÂÃÂ et al., 2018; Guo et al., 2019a), LSR can induce more informative document-level structures for better reasoning. Our LSR model also shows its superiority under the setting of Ign F1.
In addition, LSR with GloVe obtains better results than two BERT-based models. This empirically shows that our model is able to capture longrange dependencies even without using powerful context encoders. Following Wang et al. (2019), we leverage BERT as the context encoder. As shown in Table 2, our LSR model with BERT achieves a 59.05 F1 score on DocRED, which is a new stateof-the-art result. As of the ACL deadline on the 9th of December 2019, we held the first position on the CodaLab scoreboard under the alias diskorak.",positive
7,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","In this subsection, we analyze intra- and intersentence performance on the development set. An entity pair requires inter-sentence reasoning if the two entities from the same document have no mentions in the same sentence. In DocREDÃÂÃÂ¢ÃÂÃÂÃÂÃÂs development set, about 45% of entity pairs require information aggregation over multiple sentences.
Under the same setting, our LSR model outperforms all other models in both intra- and intersentence setting. The differences in F1 scores between LSR and other models in the inter-sentence setting tend to be larger than the differences in the intra-sentence setting. These results demonstrate that the majority of LSRÃÂÃÂ¢ÃÂÃÂÃÂÃÂs superiority comes from the inter-sentence relational facts, suggesting that
the latent structure induced by our model is indeed capable of synthesizing the information across multiple sentences of a document.
Furthermore, LSR with GloVe also proves better in the inter-sentence setting compared with two BERT-based (Wang et al., 2019) models, indicating latent structureÃÂÃÂ¢ÃÂÃÂÃÂÃÂs superiority in resolving longrange dependencies across the whole document compared with the BERT encoder.",positive
8,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Table 3 depicts the comparisons with state-ofthe-art models on the CDR dataset. Gu et al. (2017); Nguyen and Verspoor (2018); Verga et al. (2018) leverage sequence-based models. Convolutional neural networks and self-attention networks are used as the encoders. Sahu et al. (2019); Christopoulou et al. (2019) use graph-based models. As shown in Table 3, our LSR performs worse than the state-of-the-art models. It is challenging for an off-the-shelf parser to get high quality dependency trees in the biomedical domain, as we observe that the MDP nodes extracted by the spaCy parser from the CDR dataset contains much less informative context compared with the nodes from DocRED. Here we introduce a simplified LSR model indicated as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLSR w/o MDP NodesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ , which removes the MDP nodes and builds a fullyconnected graph using all tokens of a document. It shows that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLSR w/o MDP NodesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ consistently outperforms sequence-based and graph-based models, indicating the effectiveness the latent structure. Moreover, the simplified LSR outperforms most of the models with external resources, except for Li et al. (2016b), which leverages co-training with additional unlabeled training data. We believe such a setting also benefits our LSR model.
Table 4 shows the results on the distantly supervised GDA dataset. Here ÃÂÃÂ¢ÃÂÃÂÃÂÃÂFullÃÂÃÂ¢ÃÂÃÂÃÂÃÂ indicates EoG model with a fully connected graph as the inputs, while ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNoInfÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is a variant of EoG model without inference component (Christopoulou et al., 2018). The simplified LSR model achieves the new state-of-the-art result on GDA. The ÃÂÃÂ¢ÃÂÃÂÃÂÃÂFullÃÂÃÂ¢ÃÂÃÂÃÂÃÂ model (Christopoulou et al., 2019) yields a higher F1 score on the inter-sentence setting while having a relatively low score on the intra-sentence. It is likely because that this model neglects the differences between relations expressed within the sentence and across sentences.",positive
9,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","We investigate the extent to which the latent structures, that are induced and iteratively refined by the proposed dynamic reasoner, help to improve the overall performance. We experiment with the three different structures defined below. For fair comparisons, we use the same GCN model to perform multi-hop reasoning for all these structures.
Rule-based Structure: We use the rule-based structure in EoG (Christopoulou et al., 2019). Also,
We adapt rules from De Cao et al. (2019) for multihop question answering, i.e., each mention node is connected to its entity node and to the same mention nodes across sentences, while mention nodes and MDP nodes which reside in the same sentence are fully connected. The model is termed QAGCN.
Attention-based Structure: This structure is induced by AGGCN (Guo et al., 2019a) with multihead attention (Vaswani et al., 2017). We extend the model from sentence-level to document-level.
We explore multiple settings of these models with different block numbers ranging from 1 to 4, where a block is composed of a graph construction component and a densely connected GCN component. As shown in Figure 4, LSR outperforms QAGCN, EoG and AGGCN in terms of overall F1. This empirically confirms our hypothesis that the latent structure induced by LSR is able to capture a more informative context for the entire document.",positive
10,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","In Figure 5, we present a case study to analyze why the latent structure induced by our proposed LSR performs better than the structures learned by AGGCN. We use the entity World War II to illustrate the reasoning process and our goal here is to predict the relation of the entity pair ÃÂÃÂ£ÃÂÃÂÃÂÃÂJapan, World War IIÃÂÃÂ£ÃÂÃÂÃÂÃÂ. As shown in Figure 5, in the first refinement of LSR, Word War II interacts with several local mentions with higher attention scores, e.g., 0.43 for the mention Lake Force, which will be used as a bridge between the mention Japan and World War II. In the second refinement, the attention scores of several non-local mentions, such as Japan and Imperial Japanese Army, significantly increase from
0.09 to 0.41, and 0.17 to 0.37, respectively, indicating that information is propagated globally at this step. With such intra- and inter-sentence structures, the relation of the entity pair ÃÂÃÂ£ÃÂÃÂÃÂÃÂJapan, World War IIÃÂÃÂ£ÃÂÃÂÃÂÃÂ can be predicted as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂparticipant ofÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, which is denoted by P1344. Compared with LSR, the attention scores learned by AGGCN are much more balanced, indicating that the model may not be able to construct an informative structure for inference, e.g., the highest score is 0.27 in the second head, and most of the scores are near 0.11.
We also depict the predicted relations of ContextAware, AGGCN and LSR on the graph on the right side of the Figure 5. Interested reader could refer to (Yao et al., 2019) for the definition of a relation, such as P607, P17, etc. The LSR model proves capable of filling out the missing relation for ÃÂÃÂ£ÃÂÃÂÃÂÃÂJapan, World War IIÃÂÃÂ£ÃÂÃÂÃÂÃÂ that requires reasoning across sentences. However, LSR also attends to the mention New Ireland with a high score, thus failing to predict that the entity pair ÃÂÃÂ£ÃÂÃÂÃÂÃÂNew Ireland, World War IIÃÂÃÂ£ÃÂÃÂÃÂÃÂ actually has no relation (NIL type).",positive
11,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Document-level relation extraction. Early efforts focus on predicting relations between entities within a single sentence by modeling interactions in the input sequence (Zeng et al., 2014; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017; Guo et al., 2020) or the corresponding dependency tree (Xu et al., 2015a,b; Liu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2018). These approaches do not consider interactions across mentions and ignore relations expressed across sentence boundaries. Recent work begins to explore cross-sentence extraction (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2018; Song et al., 2018c, 2019). Instead of using discourse structure understanding techniques (Liu et al., 2019a; Lei et al., 2017, 2018), these approaches leverage the dependency graph to capture inter-sentence interactions, and their scope is still limited to several sentences. More recently, the extraction scope has been expanded to the entire document (Verga et al., 2018; Jia et al., 2019; Sahu et al., 2019; Christopoulou et al., 2019) in the biomedical domain by only considering a few relations among chemicals. Unlike previous work, we focus on document-level relation extraction datasets (Yao et al., 2019; Li et al., 2016a; Wu et al., 2019) from different domains with a large number of relations
and entities, which require understanding a document and performing multi-hop reasoning.
Structure-based relational reasoning. Structural information has been widely used for relational reasoning in various NLP applications including question answering (Dhingra et al., 2018; De Cao et al., 2019; Song et al., 2018a) and relation extraction (Sahu et al., 2019; Christopoulou et al., 2019). Song et al. (2018a) and (De Cao et al., 2019) leverage co-reference information and set of rules to construct document-level entity graph. GCNs (Kipf and Welling, 2017) or GRNs (Song et al., 2018b) are applied to perform reasoning for multi-hop question answering (Welbl et al., 2018). Sahu et al. (2019) also utilize co-reference links to construct the dependency graph and use labelled edge GCNs (Marcheggiani and Titov, 2017) for document-level relation extraction. Instead of using GNNs, Christopoulou et al. (2019) use the edgeoriented model (Christopoulou et al., 2018) for logical inference based on a heterogeneous graph constructed by heuristics. Unlike previous approaches that use syntactic trees, co-references or heuristics, LSR model treats the document-level structure as a latent variable and induces it in an iteratively refined fashion, allowing the model to dynamically construct the graph for better relational reasoning.",positive
12,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","A well-written document is rich not only in content but also in structure. One type of structure is the grouping of content into topically coherent segments. These segmented documents have many uses across various domains and downstream tasks. Segmentation can, for example, be used to convert unstructured medical dictations into clinical reports (Sadoughi et al., 2018), which in turn can help with medical coding (since a diagnosis mentioned in a ""Medical History"" might be different from a diagnosis mentioned in an ""Intake"" section (Ganesan and Subotin, 2014)). Segmentation can also be used downstream for retrieval (Hearst and Plaunt, 2002; Edinger et al., 2017; Allan et al., 1998), where it can be particularly useful when applied to informal text or speech that lacks explicit segment markup. Topically segmented documents are also useful for pre-reading (the process of skimming or surveying a text prior to careful reading), thus serving as an aid for reading comprehension (Swaffar et al., 1991; Ajideh, 2003).
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ? Work done while interning at Adobe.
Uncovering latent, topically coherent segments of text is a difficult problem because it requires solving a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; GlavaÃÂÃÂÃÂÃÂ¡ et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation.
Our work is motivated by the observation that the segment bounds and topicality are tightly interwoven, and should ideally be considered jointly rather than sequentially. We start by examining three properties about text segmentation: (1) segment bounds and segment labels contain complementary supervisory signals, (2) segment labels are a product of lower level (e.g. sentence) labels which must be composed, and (3) the model should not only learn to label from ground-truth segmentations at training time, but instead the labeler should learn to be robust to segmentation errors. These properties build on previous work discussed in Section 2. We
experimentally evaluate and verify each of these properties in Section 5 with respect to a document segmentation and segment labeling task.
Taking advantage of these properties, we propose a neural model that jointly segments and labels without committing to a priori segmentations, Segment Pooling LSTM (S-LSTM). It consists of three components: a segment proposal LSTM (discussed in Section 3.2), a segment pooling layer (Section 3.3), and a segment aligner for training and evaluation (Section 3.4).
Our main contribution is a model that performs segmentation and labeling jointly rather than separately. By virtue of joint inference, our model takes advantage of the complementary supervisory signals for segmentation and topic inference, considers the contribution of all sentences to the segment label, and avoids committing to early errors in low-level inference.
Our approach improves over neural and nonneural baselines of a document segmentation task. We use a dataset of Wikipedia articles described in Section 5 for training and evaluation. We show that S-LSTM is capable of reducing segmentation error by, on average, 30% while also improving segment classification. We also show that these improvements hold on out-of-domain datasets.",positive
13,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","Coherence-based Segmentation. Much work on text segmentation uses measures of coherence to find topic shifts in documents. Hearst (1997) introduced the TextTiling algorithm, which uses term co-occurrences to find coherent segments in a document. Eisenstein and Barzilay (2008) introduced BayesSeg, a Bayesian method that can incorporate other features such as cue phrases. Riedl and Biemann (2012) later introduced TopicTiling, which uses coherence shifts in topic vectors to find segment bounds. GlavaÃÂÃÂÃÂÃÂ¡ et al. (2016) proposed GraphSeg, which constructs a semantic relatedness graph over the document using lexical features and word embeddings, and segments using cliques. Nguyen et al. (2012) proposed SITS, a model for topic segmentation in dialogues that incorporates a per-speaker likelihood to change topics.
While the above models are unsupservised, Arnold et al. introduced a supervised method to compute sentence-level topic vectors using Wikipedia articles. The authors created the WikiSection dataset and proposed the SECTOR neural
model. The SECTOR model predicts a label for each sentence, and then performs post hoc segmentation looking at the coherence of the latent sentence representations, addressing segmentation and labeling separately. We propose a model capable of jointly learning segmentation boundaries and segment-level labels at training time. Our segmentation does not rely on measures of coherence, and can instead learn from signals in the data, such as cue phrases, to predict segment bounds, while still performing well at the segment labeling task.
Supervised Segmentation. An alternative to using measures of topical coherence to segment text is to learn to directly predict segment bounds from labeled data. This was the approach taken in Koshorek et al. (2018), where the authors used Wikipedia as a source of training data to learn text segmentation as a supervised task. However, learning only to predict segment bounds does not necessarily capture the topicality of a segment that is useful for informative labeling.
The task of document segmentation and labeling is well-studied in the clinical domain, where both segmenting and learning segment labels are important tasks. Pomares-Quimbaya et al. (2019) provide a current overview of work on clinical segmentation. Ganesan and Subotin (2014) trained a logistic regression model on a clinical segmentation task, though they did not consider the task of segment labeling. Tepper et al. (2012) considered both tasks of segmentation and segment labeling, and proposed a two-step pipelined method that first segments and then classifies the segments. Our proposed model is trained jointly on both the segmentation and segment labeling tasks.
Concurrent work considers the task of document outline generation (Zhang et al., 2019). The goal of outline generation is to segment and generate (potentially hierarchical) headings for each segment. The authors propose the HiStGen model, a hierarchical LSTM model with a sequence decoder. The work offers an alternative view of the joint segmentation and labeling problem, and is evaluated using exact match for segmentation and ROUGE (Lin, 2004) for heading generation if the segment is predicted correctly. In contrast, we evaluate our models using a commonly-used probabilistic segmentation measure, Pk, which assigns partial credit to incorrect segmentations (Beeferman et al., 1999). We also use an alignment technique to assign partial credit to labels of incorrect segmentations, both for
training and evaluation. In addition, we explicitly consider the problem of model transferability, evaluating the pretrained models on additional datasets.
IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach.
CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ |L| (where |L| is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a single-label labeling task.",positive
14,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The second step of our model is a Segment Predictor LSTM, which predicts segment boundaries within the document. For this step we use a bidirectional LSTM that consumes each sentence vector and predicts an indicator variable, (B)eginning or (I)nside a segment. It is trained from presegmented documents using a binary cross entropy loss. This indicator variable determines if the sentence is the start of a new segment or not. This is similar to the approach taken by TextSeg in Koshorek et al. (2018), though we do not estimate a threshold, ÃÂÃÂÃÂÃÂ , and instead learn to to predict two classes: (B)eginning and (I)nside.",positive
15,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","After segmenting the document, the third stage of the model pools within the predicted segments to predict a label for each segment. The sentence vectors for the predicted segments are all grouped, and a pooling function is run over them. There are several possible sequence-to-vector pooling functions that could be used, such as averaging, and more complex learned pooling functions, such as LSTMs. The full S-LSTM model uses a concat pooling LSTM, and our experimental results show that this yields a better segment label than just averaging. We then use a classifier following the output of the segment pooler, which can provide a distribution over labels for each segment.
The combination of segment prediction and pooling is one way that S-LSTM is different from previous hierarchical LSTM models. The model can predict and label segments dynamically, generating a single vector for predicted segments.",positive
16,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","Because segments can be considered dynamically at training time, we propose a method of assigning labels to potentially incorrect segments by aligning the predicted segments with ground truth segments. This label assignment allows segment-labeling loss to be propagated through the end-to-end model.
Teacher Forcing. Teacher forcing, or feeding ground truth inputs into a recurrent network as
opposed to model predictions, was first developed in Williams and Zipser (1989). The idea is to use ground truth predictions for inputs that would normally come from model predictions for the first stages of training, to help with convergence. For S-LSTM, it is the simplest approach to segment pooling and alignment: at training time feed the ground truth segments (as opposed to the predicted segments) the segment pooler (step 3 in Figure 1). This gives us a one-to-one alignment of ""predicted"" (forced) segments and ground truth segments. This is opposed to only using the predicted segments as the bounds for segment pooler.
Exploration. Employing only teacher forcing does not allow the segment labeler to learn how to recover from errors in segmentation. The mechanism for allowing the model to explore incorrect segmentations is to align the predicted segments with overlapping ground truth segments at training time, and treat the all aligned ground truth labels as correct. While many alignments are possible, we use the one presented in Figure 2. This manyto-many alignment ensures that every ground-truth segment is mapped to at least one predicted segment and every predicted segment is mapped to at least one ground truth segment.
We can additionally schedule teacher forcing. At the beginning, when the segmentation prediction network performs poorly, the model pools over only ground truth segment bounds, allowing it to learn the cleanest topic representations. However, as training progresses and the segmentation accuracy begins to converge, we switch from pooling over ground truth segments to aligning predicted and ground truth segment. In this way, the segment
pooler learns to be robust to segmentation errors.",positive
17,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","In this paper we introduce the Segment Pooling LSTM (S-LSTM) model for joint segmentation and segment labeling tasks. We find that the model dramatically reduces segmentation error (by 30% on average across four datasets) while improving segment labeling accuracy compared to previous neural and non-neural baselines for both singlelabel and multi-label tasks. Experiments demonstrate that jointly modeling the segmentation and segment labeling, segmentation alignment and exploration, and segment pooling each contribute to S-LSTMÃÂÃÂ¢ÃÂÃÂÃÂÃÂs improved performance.
S-LSTM is agnostic as to the sentence encoder used, so we would like to investigate the potential
usefulness of transformer-based language models as sentence encoders. There are additional engineering challenges associated with using models such as BERT as sentence encoders, since encoding entire documents can be too expensive to fit on a GPU without model parallelism. We would also like to investigate the usefulness of an unconsidered source of document structure: the hierarchical nature of sections and subsections. Like segment bounds and headers, this structure is naturally available in Wikipedia. Having shown that segment bounds contain useful supervisory signal, it would be interesting to examine if segment hierarchies might also contain useful signal.",positive
18,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","Attention mechanism (Bahdanau et al., 2015) has been used as an important component across a wide range of NLP models. Typically, an attention layer produces a distribution over input representations to be attended to. Such a distribution is then used for constructing a weighted combination of the inputs, which will then be employed by certain downstream modules.
Recently, several research efforts on investigating the interpretability of attention on tasks such as text classification, question answering, and natural language inference (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Arras et al., 2019) have been conducted. One of their important arguments was whether the attention distribution could adequately reflect the significance of inputs. To answer this question, they designed a series of metrics and
1Supplementary material and code at https:// github.com/richardsun-voyager/UAFTC
conducted corresponding experiments. In their approaches, they were mainly observing how the attention may impact the outputs on the pre-trained models by changing some elements in the inputs. While such approaches have resulted in interesting findings, the attention mechanism itself remains a black box to us ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ it is still largely unclear what are the underlying factors that may have an impact on the attention mechanism.
When analyzing the results of a typical model with attention on the text classification tasks, we noticed that in some instances, many of the word tokens with large attention weights were adjectives or adverbs which conveyed explicit signals on the underlying class label. On the other hand, in some other instances, we also noticed that such useful words may not always be able to receive significant attention weights, especially under certain configurations of hyperparameters, making the attention mechanism less interpretable.
Such observations lead to several important questions. First, the attention weight for a word token appears to be the relative measurement to its significance, and is largely local and instance specific. Would there be an instance-independent quantity to assess the corpus-level importance of a word token? And if so, what role would such a quantity play in terms of interpreting the overall attention mechanism? Second, when the attention mechanism appears to be less interpretable, how would the underlying model be affected in terms of performance?
In this work, we focus on answering the above questions. We argue that the attention scores (rather than attention weights) are able to capture the global, absolute importance of word tokens within a corpus. We present a study to figure out the underlying factors that may influence such attention scores under a simple neural classification model. Inspired by Qian (1999), we analyzed the gradients as well as the updates of intermediate variables in the process of gradient descent, and
found that there exist some implicit trends on the intermediate variables related to attention: the degree of association between a word token and the class label may impact their attention scores. We argue that when certain hyperparameters are properly set, tokens with strong polarity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ high degree of association with specific labels, would likely end up with large attention scores, making them more likely to receive large attention weights in a particular sentence. While in such scenarios, the attention mechanism would appear to be more interpretable, we also discuss scenarios where the attention weights may become less interpretable, and show how the polarity scores, another important token-level quantity, will play their roles in the overall model in terms of contributing towards the model performance.",positive
19,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","Research on interpretability of neural models has received significant attention recently. One approach was using visualization to explore patterns that exist in the intermediate representations of neural networks. Simonyan et al. (2013) visualized the image-specific class saliency on image classification tasks using learnt ConvNets, and displayed the features captured by the neural networks. Li et al. (2016a,b) proposed visualization methods to look into the neural representations of the embeddings from the local composition, concessive sentences, clause composition, as well as the saliency of phrases and sentences, and illustrated patterns based on the visualizations. An erasure method was also adopted to validate the importance of different dimensions and words. Vig and Belinkov (2019) analyzed the attention structure on the Transformer (Vaswani et al., 2017) language model as well as GPT-2 (Radford et al., 2019) pre-trained model.
Another approach to understanding neural approaches is to conduct theoretical analysis to investigate the underlying explanations of neural models. One example is the work of Levy and Goldberg (2014), which regarded the word embedding learning task as an optimization problem, and found that the training process of the skip-gram model (Mikolov et al., 2013a,b) can be explained as implicit factorization of a shifted positive PMI (pointwise mutual information) matrix.
Recently, several research efforts have focused on the interpretability of the attention mechanism. Jain and Wallace (2019) raised the question on the explainability of feature importance as captured by the attention mechanism. They found the attention weights may not always be consistent with
the feature importance from the human perspective in tasks such as text classification and question answering. Serrano and Smith (2019) also carried out an analysis on the interpretability of the attention mechanism, with a focus on the text classification task. They conducted their study in a cautious way with respect to defining interpretability and the research scope. The paper concluded that the attention weights are noisy predictors of importance, but should not be regarded as justification for decisions. Wiegreffe and Pinter (2019) suggested that the notion of explanation needs to be clearly defined, and the study of the explanation requires taking all components of a model into account. Their results indicated that prior work could not disprove the usefulness of attention mechanisms with respect to explainability. Moreover, Michel et al. (2019) and Voita et al. (2019) examined the multi-head self-attention mechanism on Transformer-based models, particularly the roles played by the heads.
Our work and findings are largely consistent with such findings reported in the literature. We believe there are many factors involved when understanding the attention mechanism. Inspired by Qian (1999), which investigated the internal mechanism of gradient descent, in this work we focus on understanding attentionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs internal mechanism.",positive
20,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","We consider the task of text classification, with a specific focus on binary classification.2 The architecture of the model is depicted in Figure 1.
There are various attention mechanisms introduced in the field (Luong et al., 2015). Two commonly used mechanisms are the additive attention (Bahdanau et al., 2015) and scaled dot-product attention (Vaswani et al., 2017). In this work, we will largely focus our analysis on the latter approach (but we will also touch the former approach later).
2Extending to multi-class classification is possible. See the supplementary material for detailed analysis and discussion.
Consider an input token sequence of length n: x = e1, e2, . . . , en, where ej is the j-th input token whose representation before the attention layer is hj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd. The attention score for the j-th token is:
aj = h>j V
ÃÂÃÂÃÂÃÂ» , (1)
where the hyperparameter ÃÂÃÂÃÂÃÂ» is the scaling factor (typically set to a large value, e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ d is often used in the literature (Vaswani et al., 2017)), and V ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd is the context vector that can be viewed as a fixed query asking for the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmost informative wordÃÂÃÂ¢ÃÂÃÂÃÂÃÂ from the input sequence (Yang et al., 2016). The token representation hj can be the word embedding, or the output of an encoder.
The corresponding attention weight would be:
ÃÂÃÂÃÂÃÂ±j = exp(aj)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp ( ajÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² ) . (2)
The complete input sequence is represented as:
h = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j ÃÂÃÂÃÂÃÂ±jhj , (3)
and the output of the linear layer is:
s = h>W , (4)
which we call instance-level polarity score of the input sequence. Here, W ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd is the weight vector for the linear layer.
When we make predictions, if the resulting polarity score s is positive, the corresponding input sequence will be classified as positive (i.e., y = +1, where y is the output label). Otherwise, it will be classified as negative (i.e., y = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1).
During training, assume we have a training set D = {(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))} with m labeled instances. Our overall loss is:
` = 1
m mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 `(t) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 m mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 log ( ÃÂÃÂÃÂÃÂ(y(t)s(t)) ) .
(5)
where y(t) and s(t) are the gold output label and the instance-level polarity score for the t-th instance respectively, and ÃÂÃÂÃÂÃÂ is the sigmoid function.
The instance-level polarity score s can also be written as:
s = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j ÃÂÃÂÃÂÃÂ±jh > j W = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j ÃÂÃÂÃÂÃÂ±jsj . (6)
Here, we have introduced the token-level polarity score sj for the input token representation hj :
sj = h > j W . (7)
From here we can observe that the instance-level polarity score of the input sequence can be interpreted as the weighted sum of the token-level polarity scores, where the weights are given by the attention weights (ÃÂÃÂÃÂÃÂ±j for hj). Such attention weights measure the relative importance of the token within a specific input sequence.
On the other hand, the attention score aj captures the absolute importance of the token. We believe such absolute measurements to the significance of words may be playing a more crucial role (than attention weights) when understanding the attention mechanism. Thus, unlike many previous research efforts, we will instead focus on the understanding of attention scores in this work.
In this paper, we will mainly investigate a simple neural model where hj = ej . Here ej is the word embedding for the j-th input token. In other words, we assume the word embeddings are used as the inputs to the attention layer. Detailed discussions on other assumptions on hj can be found in the supplementary material.",positive
21,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","We conduct some analysis in this section to understand how the attention mechanism works for the task of text classification. First, let us consider the following 3 different types of tokens: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ positive tokens: tokens that frequently appear
in positive training instances only, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ negative tokens: tokens that frequently appear
in negative training instances only, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ neutral tokens: tokens that appear evenly across
both positive and negative training instances. We also call the first two types of tokens polarity tokens. For the ease of analysis and discussion, we assume each token belongs to either of these 3 types, and we assume the dataset is balanced and symmetric3. While some of these assumptions may seem strong, having them would significantly simplify our analysis. As we will see later in experiments, even though some of the above assumptions do not hold in some real datasets, our findings are still valid in practice.
The gradient descent algorithm that minimizes a loss ` could be interpreted as the integration of
3In other words, if we flip the signs of the y labels for all documents in the training set, we arrive at exactly the same training set (under a particular mapping between tokens).
the gradient flow equation using EulerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Method (Scieur et al., 2017; Qian, 1999), written as:
dz(ÃÂÃÂÃÂÃÂ)
dÃÂÃÂÃÂÃÂ = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ`(z(ÃÂÃÂÃÂÃÂ)), z(0) = z0, (8)
where z is the parameter vector, and z0 is its initialization, and ÃÂÃÂÃÂÃÂ is the time step. We assume that all parameters have initializations, and will omit such initializations in the subsequent differential equations. We will not seek to solve the differential equations directly but to find out whether there exist some trends and patterns for certain variables during training.",positive
22,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","In order to explain the predictions produced by accurate yet black-box neural models, a growing number of works propose extending these models with natural language explanation generation modules, thus obtaining models that explain themselves in human language (Hendricks et al., 2016; Camburu et al., 2018; Park et al., 2018; Kim et al., 2018; Ling et al., 2017).
In this work, we first draw attention to the fact that such models, while appealing, are nonetheless prone to generating inconsistent explanations. We define two explanations to be inconsistent if
they provide contradictory arguments about the instances and predictions that they aim to explain. For example, consider a visual question answering (VQA) task (Park et al., 2018) and two instances where the image is the same but the questions are different, say ÃÂÃÂ¢ÃÂÃÂÃÂÃÂIs there an animal in the image?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂCan you see a Husky in the image?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. If for the first instance a model predicts ÃÂÃÂ¢ÃÂÃÂÃÂÃÂYes.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and generates the explanation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, while for the second instance the same model predicts ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNo.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and generates the explanation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, then the model is producing inconsistent explanations.
Inconsistent explanations reveal at least one of the following undesired behaviors: (i) at least one of the explanations is not faithfully describing the decision mechanism of the model, or (ii) the model relied on a faulty decision mechanism for at least one of the instances. Note that, for a pair of inconsistent explanations, further investigation would be needed to conclude which of these two behaviors is the actual one (and might vary for each instance). Indeed, a pair of inconsistent explanations does not necessarily imply at least one unfaithful explanation. In our previous example, if the image contains a dog, it is possible that the model identifies the dog when it processes the image together with the first question, and that the model does not identify the dog when it processes the image together with the second question, hence both explanations would faithfully reflect the decision mechanism of the model even if they are inconsistent. Similarly, a pair of inconsistent explanations does not necessarily imply that the model relies on a faulty decision mechanism, because the explanations may not faithfully describe the decision mechanism of the model. We here will not investigate the problem of identifying which of the two undesired behaviors is true for a pair of inconsistent explanations.
In this work, we introduce a framework for
checking if models are robust against generating inconsistent natural language explanations. Given a model m that produces natural language explanations for its predictions, and an instance x, our framework aims to generate inputs xÃÂÃÂÃÂÃÂ that cause the model to produce explanations that are inconsistent with the explanation produced for x. Thus, our framework falls under the category of adversarial methods, i.e., searching for inputs that cause a model to produce undesired answers (Biggio et al., 2013; Szegedy et al., 2014).
As part of our framework, we address the problem of adversarial attacks with full target sequences, a scenario that has not been previously addressed in sequence-to-sequence attacks, and which can be useful for other areas, such as dialog systems. Finally, we apply our framework on a state-of-theart neural natural language inference model that generates natural language explanations for its decisions (Camburu et al., 2018). We show that this model can generate a significant number of inconsistent explanations.",positive
23,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","Given a model m that can jointly produce predictions and natural language explanations, we propose a framework that, for any given instance x, attempts to generate new instances for which the model produces explanations that are inconsistent with the explanation produced for x; we refer to the latter as em(x).
We approach the problem in two high-level steps. Given an instance x, (A) we create a list of explanations that are inconsistent with the explanation generated by the model on x, and (B) given an inconsistent explanation from the list created in A, we find an input that causes the model to generate
this precise inconsistent explanation.
Setup. Our setup has three desired properties that make it different from commonly researched adversarial settings in natural language processing:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ At step (B), the model has to generate a full target sequence: the goal is to generate the exact explanation that was identified at step (A) as inconsistent with the explanation em(x).
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Adversarial inputs do not have to be a paraphrase or a small perturbation of the original input, since our objective is to generate inconsistent explanations rather than incorrect predictions ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ these can eventually happen as a byproduct.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Adversarial inputs have to be realistic to the task at hand.
To our knowledge, this work is the first to tackle this problem setting, especially due to the challenging requirement of generating a full target sequence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ see Section 4 for comparison with existing works.
Context-dependent inconsistencies. In certain tasks, instances consist of a context (such as an image or a paragraph), and some assessment to be made about the context (such as a question or a hypothesis). Since explanations may refer (sometimes implicitly) to the context, the assessment of whether two explanations are inconsistent may also depend on it. For example, in VQA, the inconsistency of the two explanations ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ depends on the image. However, if the image is the same, the two explanations are inconsistent regardless of which questions were asked on that image.
For such a reason, given an instance x, we differentiate between parts of the instance that will remain fixed in our method (referred to as context parts and denoted as xc) and parts of the instance that our method will vary in order to obtain inconsistencies (referred to as variable parts and denoted as xv). Hence, x = (xc,xv). In our VQA example, xc is the image, and xv is the question.
Stand-alone inconsistencies. Furthermore, we note that there are cases for which explanations are inconsistent regardless of the input. For example, explanations formed purely of background knowledge such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA woman is a person.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA woman is not a person.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 are always inconsistent (and sometimes outrageous), regardless of the instances that lead to them. For these cases, our method can treat the whole input as variable, i.e., xc = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and xÃÂÃÂÃÂÃÂv = x.
Steps. Our adversarial framework consists of the following steps:
1. Reverse the explanation generator module of model m by training a REVEXPL model to map from the generated explanation and the context part of the input to the variable part of the input, i.e., REVEXPL(xc, em(x)) = xv.
2. For each explanation e = em(x):
(a) Create a list of statements that are inconsistent with e, we call it Ie. (b) Query REVEXPL on each eÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Ie and the context xc. Get the new variable part xÃÂÃÂÃÂÃÂv = REVEXPL(xc, eÃÂÃÂÃÂÃÂ) of a reverse input xÃÂÃÂÃÂÃÂ = (xc, xÃÂÃÂÃÂÃÂv), which may cause the m to produce inconsistent explanations.
(c) Query m on each reverse input to get a reverse explanation em(xÃÂÃÂÃÂÃÂ).
(d) Check if each reverse explanation em(xÃÂÃÂÃÂÃÂ) is indeed inconsistent with e by checking if em(xÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Ie.
To execute step (2a), note that explanations are by nature logical sentences. Hence, for any task, one may define a set of logical rules to transform an explanation into an inconsistent counterpart, such as negation or replacement of task-essential tokens with task-specific antonyms. For example, in explanations for self-driving cars (Kim et al., 2018), one can replace ÃÂÃÂ¢ÃÂÃÂÃÂÃÂgreen lightÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂred lightÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe
1Which was generated by the model in our experiments.
road is emptyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe road is crowdedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (which are task-specific antonyms), to get inconsistent (and hazardous) explanations such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂThe car accelerates because there is a red light.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Another strategy to obtain inconsistent explanations consists of swapping explanations from mutually exclusive labels. For example, assume a recommender system predicts that movie X is a bad recommendation for user Y ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbecause X is a horror movie.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, implying that user Y does not like horror movies. If it also predicts that movie Z is a good recommendation to the same user Y ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbecause Z is a horror movie.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, then we have an inconsistency, as the latter would imply that user Y likes horror movies.
While this step requires a degree of specific adjustment to the task at hand, we consider it a small price to pay to ensure that the deployed system is coherent. Also, note that this step can eventually be automated, for example, by training a neural network to generate task-specific inconsistencies after crowd-sourcing a dataset of inconsistent explanations for a task at hand ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ we leave this as future work.
Finally, to execute step (2d), our framework currently checks for an exact string match between a reverse explanation and any of the inconsistent explanations created at step (2a). Alternatively, one can train a model to identify if a pair of explanations forms an inconsistency, which we also leave as future work.",positive
24,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","Large-scale pre-trained language models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have brought significant improvements to natural language processing (NLP) applications. Despite their power, they are notorious for being enormous in size and slow in both training and inference. Their long inference latencies present challenges to deployment in real-time applications and hardwareconstrained edge devices such as mobile phones and smart watches.
To accelerate inference for BERT, we propose DeeBERT: Dynamic early exiting for BERT. The inspiration comes from a well-known observation in the computer vision community: in deep convolutional neural networks, higher layers typically produce more detailed and finer-grained features (Zeiler and Fergus, 2014). Therefore, we
hypothesize that, for BERT, features provided by the intermediate transformer layers may suffice to classify some input samples.
DeeBERT accelerates BERT inference by inserting extra classification layers (which we refer to as off-ramps) between each transformer layer of BERT (Figure 1). All transformer layers and offramps are jointly fine-tuned on a given downstream dataset. At inference time, after a sample goes through a transformer layer, it is passed to the following off-ramp. If the off-ramp is confident of the prediction, the result is returned; otherwise, the sample is sent to the next transformer layer.
In this paper, we conduct experiments on BERT and RoBERTa with six GLUE datasets, showing that DeeBERT is capable of accelerating model inference by up toÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% with minimal model quality degradation on downstream tasks. Further analyses reveal interesting patterns in the modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ transformer layers, as well as redundancy in both BERT and RoBERTa.",positive
25,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","We start with a pre-trained BERT model with n transformer layers and add n off-ramps to it. For fine-tuning on a downstream task, the loss function of the ith off-ramp is
Li(D; ÃÂÃÂÃÂÃÂ¸) = 1 |D| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(x,y)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂD
H(y, fi(x; ÃÂÃÂÃÂÃÂ¸)), (1)
where D is the fine-tuning training set, ÃÂÃÂÃÂÃÂ¸ is the collection of all parameters, (x, y) is the featureÃÂÃÂ¢ÃÂÃÂÃÂÃÂ label pair of a sample, H is the cross-entropy loss function, and fi is the output of the ith off-ramp.
The network is fine-tuned in two stages:
1. Update the embedding layer, all transformer layers, and the last off-ramp with the loss function Ln. This stage is identical to BERT fine-tuning in the original paper (Devlin et al., 2019).
2. Freeze all parameters fine-tuned in the first stage, and then update all but the last offramp with the loss function ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 i=1 Li. The rea-
son for freezing parameters of transformer layers is to keep the optimal output quality for the last off-ramp; otherwise, transformer layers are no longer optimized solely for the last off-ramp, generally worsening its quality.
Algorithm 1 DeeBERT Inference (Input: x) for i = 1 to n do zi = fi(x; ÃÂÃÂÃÂÃÂ¸) if entropy(zi) < S then
return zi end if
end for return zn",positive
26,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","Discourse parsing is one of the fundamental tasks in natural language processing (NLP). Typical types of discourse parsing include hierarchical discourse parsing and shallow discourse parsing. The former is aimed at finding the relationships among a series of neighboring elementary discourse units (EDUs) and further building up a hierarchical tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied.
This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017)
propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information.
In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhetorical relation recognition. Given a paragraph, our parser extracts all EDUs, builds the tree structure, identifies the nucleuses, and recognizes the rhetorical relations of all internal nodes. We propose a revised dynamic-oracle procedure (Yu et al., 2018) for training the shift-reduce parser. Because of the limited training instances in CDTB-14, we also address the data sparsity issue by introducing unsupervised data augmentation (Xie et al., 2019). Experimental results show that our methodology is effective, and our model outperforms all the previous models. The contributions of this work are three-fold shown as follows.
1. We explore the task of Chinese discourse parsing with a variety of strategies, and our parser achieves the state-of-the-art performance. Our robust dynamic-oracle procedure can be applied to other shift-reduce parsers.
2. Our complete Chinese discourse parser handles a raw paragraph/document directly and performs all the subtasks in hierarchical discourse parsing. No pre-processing procedures such as Chinese word segmentation, POStagging, and syntactic parsing are required.
3. We release the pre-trained, standalone, readyto-use parser as a resource for the research community.1",positive
27,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","We propose a shift-reduce parser for building the structure of the discourse parse tree. A shift-reduce parser maintains a stack and a queue for representing a state during parsing, and an action classifier is trained to predict the action (i.e., shift or reduce) for making a transition from the given state to the next state. In the initial state, the stack is empty, and the queue contains all the EDUs in a raw document. In the final state, the queue is empty, and the stack contains only one element, i.e., the discourse parse tree of the whole paragraph.
To decide whether to shift or to reduce, we propose an action classifier by considering the information of the top two elements of the stack s1 and s2 (i.e., the two most recent discourse units) and the first element of the queue q (i.e., the next
1https://github.com/jeffrey9977/ Chinese-Discourse-Parser-ACL2020
EDU). The textual form of each of these three discourse units will be fed into the BERT encoder for representing as Enc(s1), Enc(s2), and Enc(q). Next, we concatenate the max pooling of Enc(s1), Enc(s2), and Enc(q) and feed the resulting vector into a dense layer to predict the next action.
Since shift-reduce is a greedy algorithm, it can hardly recover from an error state. The shift-reduce parser is typically trained with the teacher mode, where only correct states are given, and the resulting parser may perform poor when it reaches unfamiliar states. For this reason, we propose a revised dynamic-oracle procedure (Yu et al., 2018) for training our discourse parser. One drawback of the original dynamic oracle is that some golden training examples may be neglected. Because CDTB-14 has relatively few action steps to build a tree, the probability of falling into a wrong state is much small compared to that of RST-DT. In our revision, we want to guarantee all correct states have been trained. As shown in Algorithm 1, the document will be gone through twice when training a document example. We first follow the golden actions, and choose action predicted by the model with a probability ÃÂÃÂÃÂÃÂ± at the second time. We refer to them as teacher mode and student mode, respectively. Note that we follow the suggestion of Yu et al. (2018) to set ÃÂÃÂÃÂÃÂ± to 0.7.
Algorithm 1 Training Procedure for Our Shift-Reduce Discourse Parser. 1: S,QÃÂÃÂ¢ÃÂÃÂÃÂÃÂ empty stack, elementary discourse units 2: while Q is not empty ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨ S has more than 1 unit do . Teacher mode 3: predicted, goldenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ACTIONCLASSIFIER(S.top1(), S.top2(), Q.front()) , GOLDENACTION 4: COMPUTELOSSANDUPDATE(predicted, golden) 5: PERFORMACTION(golden) 6: S,QÃÂÃÂ¢ÃÂÃÂÃÂÃÂ empty stack, elementary discourse units 7: while Q is not empty ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨ S has more than 1 unit do . Student mode 8: predicted, goldenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ACTIONCLASSIFIER(S.top1(), S.top2(), Q.front()) , GOLDENACTION 9: COMPUTELOSSANDUPDATE(predicted, golden)
10: if rand() > ÃÂÃÂÃÂÃÂ± then PERFORMACTION(golden) else PERFORMACTION(predicted)",positive
28,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","If two discourse units are decided to be merged during the tree construction stage, a new internal node will be generated and the relationship of the two discourse units will be determined. Predicting the relation between two textual arguments is a typical classification task in NLP. We propose a BERT-based classifier, which predicts the relation of two arguments separated by the symbol [SEP], with additional dense layers as the output.
In CDTB-14, the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcoordinationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ relation accounts for 59.6% of the training data, while minor relations suffer from data sparseness. To address this issue, we introduce unsupervised data augmentation (UDA) (Xie et al., 2019) to enhance the performance. We adopt the discourse pairs in CDTB-12 as the material for UDA. Note that other unlabeled text pairs can also be used for UDA. We chose those from CDTB-12 simply because the format is convenient for us to use.
The original loss is shown as Eq. 1. Given a span of text x, our main model P (ÃÂÃÂÃÂÃÂ·) predicts the rhetorical relation yc. Eq. 2 shows the additional consistency loss to enforce the smoothness of our main model, and xÃÂÃÂÃÂÃÂ stands for the augmented unlabeled sentence pair. L and U stand for labeled data and unlabeled data, respectively. As shown in Eq. 3, we train both objectives at the same time with a weight ÃÂÃÂÃÂÃÂ» to adjust the effect of UDA.
H = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 N NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xÃÂÃÂ¢ÃÂÃÂÃÂÃÂL MÃÂÃÂ¢ÃÂÃÂÃÂÃÂ c=1 yc log (P (yc|x)) (1)
DKL = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1
N NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xÃÂÃÂ¢ÃÂÃÂÃÂÃÂU P (y|x) log ( P (y|x) P (y|xÃÂÃÂÃÂÃÂ) ) (2)
L (ÃÂÃÂÃÂÃÂ¸) = H + ÃÂÃÂÃÂÃÂ»DKL (3) The UDA procedure first generates the augmented unlabeled sentence pairs. Various ap-
proaches to paraphrasing can be employed. In this work, we utilize the back-translation strategy (Sennrich et al., 2016), where we translate the Chinese sentence pair to English and then translate back to Chinese. This is equivalent to add noises to the original inputs. As the original and the backtranslated sentence pairs express the same meaning, our model is expected to predict the same label for both pairs. By minimizing the consistency loss, our model can behave consistently no matter whether an original instance or its paraphrases are given. In this way, the model can be more generalized and robust. Besides, when our model is able to predict the same label for both sentence pairs, it means that our model has also learned their label.",positive
29,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","Table 1 shows the performances of our parser in micro-averaged F-score, compared with previous work Zhou (Kong and Zhou, 2017) and Lin (Lin
et al., 2018). We also implement BERT-CKY, a CKY parser by using BERT for representation, as an additional baseline model. The evaluation is based on multiway trees.
Both the performances with and without golden EDUs are measured. The results show that BERT is highly competitive and has the ability to catch the potential relations between discourse units since Lin and BERT-CKY basically use the same approach while the latter model uses BERT as the text encoder. Our parser outperforms all the baseline models and achieves a significant improvement without the golden EDUs given. Note that BERT-CKY is based on Lin et al. (2018), which has its own EDU segmentation module different from ours, hence the EDU score is different.
We examine the performance of three different training techniques for shift-reduce parsing. As mentioned in Section 2.2, Normal stands for action classifier trained with gold standard actions, Dynamic stands for Dynamic Oracle introduced by Yu et al. (2018), and Ours stands for our revised dynamic-oracle procedure where the model is trained with both gold standard actions and dynamic oracle actions.
Compared to Normal, experimental results show no improvement made by the original dynamic oracle, while our revised dynamic oracle outperforms the other two strategies. Our strategy does not ignore the golden action in every correct state and also has the chance to explore error states.
In order to compare with SUN (Sun and Kong, 2018), we convert the golden standard trees into binary trees and measure the performances on bi-
nary trees in macro-averaged F-score. The results are shown in Table 2. Sun and Kong (2018) do not address all subtasks in Chinese discourse parsing, and our model outperforms SUN in every subtask.",positive
30,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","To examine the effectiveness of UDA, Table 3 shows the performances of rhetorical relation recognition with and without UDA. Experimental results show that application of UDA successfully enhances the recall scores of the three minor classes with a little trade-off in the recall score of the dominant class, Coordination. In addition, the F-scores of all the four relations are increased. In other words, applying UDA deals with the data imbalance issue and improves the overall performance. Applying UDA to nuclearity classification also has a similar improvement as Table 3.
Theoretically, beam search with a larger beam width helps find a better solution. As shown in
Table 4, however, our parser is worse when a larger beam width is used, which means the sequence having higher overall score does not ensure the better decoding result. Our experiment only shows the beam widths up to five because the scores of worse sequences are already higher than that of the correct sequence in some cases. That is, the larger beam widths seem to be unnecessary.
The reason may be that beam search is not really suitable for the shift-reduce paradigm. For example, a sequence might fall into a seriously bad stage but the rest of actions can be easily determined so that the sequence will get a high overall probability. This assumption also implies that unlike beam search applied on sequence to sequence model, we cannot judge a transition sequence is good or bad by solely considering its overall score. In addition, for longer textual units such as paragraph, human readers and writers may not follow the assumption of overall optimization. Instead, human beings read and write sequentially, similar to the greedy nature.
We also evaluate our approach in English discourse parsing. The famous dataset, RST-DT, is used. Our model achieves F-scores of 85.0%, 58.8%, 69.9%, and 56.7% in tree construction, rhetorical relation recognition, nuclearity labeling, and all subtasks, respectively. The overall performance is similar to that of the state-of-the-art model (Yu et al., 2018).",positive
31,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","Factoid question answering (QA) has recently enjoyed rapid progress due to the increased availability of large crowdsourced datasets (e.g., SQuAD (Rajpurkar et al., 2016), MS MARCO (Bajaj et al., 2016), Natural Questions (Kwiatkowski et al., 2019)) for training neural models and the significant advances in pre-training contextualized representations using massive text corpora (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019)). Building on these successes, recent work examines conversational QA (ConvQA) systems capable of interacting with users over multiple turns.
1The code and data are available at https://github.com/abaheti95/QADialogSystem.
Large crowdsourced ConvQA datasets (e.g., CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018)) consist of dialogues between crowd workers who are prompted to ask and answer a sequence of questions regarding a source document. Although these ConvQA datasets support multi-turn QA interactions, the responses have mostly been limited to extracting text spans from the source document and do not readily support abstractive answers (Yatskar, 2019a). While responses copied directly from a Wikipedia article can provide a correct answer to a user question, they do not sound natural in a conversational setting. To address this challenge, we develop SEQ2SEQ models that generate fluent and informative answer responses to conversational questions.
To obtain data needed to train these models, rather than constructing yet-another crowdsourced QA dataset, we transform the answers from an existing QA dataset into fluent responses via data augmentation. Specifically, we synthetically generate supervised training data by converting questions and associated extractive answers from a SQuADlike QA dataset into fluent responses via Syntactic Transformations (STs). These STs over-generate a large set of candidate responses from which a BERT-based classifier selects the best response as shown in the top half of Figure 1.
While over-generation and selection generates fluent responses in many cases, the brittleness of the off-the-shelf parsers and the syntatic transformation rules prevent direct use in cases that are not well-covered. To mitigate this limitation, we generate a new augmented training dataset using the best response classifier that is used to train end-toend response generation models based on PointerGenerator Networks (PGN) (See et al., 2017) and pre-trained Transformers using large amounts of dialogue data, DialoGPT (D-GPT) (Zhang et al., 2019). In ÃÂÃÂÃÂÃÂ§3.2 and ÃÂÃÂÃÂÃÂ§3.3, we empirically demon-
strate that our proposed NLG models are capable of generating fluent, abstractive answers on both SQuAD 2.0 and CoQA.",positive
32,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","The first step is to apply the Syntactic Transformations (STs) to the questionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs parse tree along with the expert answer phrase to produce multiple candidate responses. For the STs to work effectively accurate question parses are essential. We use the Stanford English lexparser2(Klein and Manning, 2003), which is trained on WSJ sections 1-21, QuestionBank (Judge et al., 2006), amongst other corpora. However, this parser still fails to recognize ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 20% of the questions (neither SBARQ nor SQ tag is assigned). For such erroneous parse trees, we simply output the expert answer phrase as a single
2https://nlp.stanford.edu/software/parser-faq.html#z
response. The remaining questions are processed via the following transformations to over-generate a list of candidate answers: (1) Verb modification: change the tense of the main verb based on the auxiliary verb using SimpleNLG (Gatt and Reiter, 2009); (2) Pronoun replacement: substitute the noun phrase with pronouns from a fixed list; (3) Fixing Preposition and Determiner: find the preposition and determiner in the questionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs parse tree that connects to the answer phrase and add all possible prepositions and determiners if missing. (4) Response Generation: Using Tregex and Tsurgeon (Levy and Andrew, 2006), compile responses by combining components of all previous steps and the answer phrase. In cases where there are multiple options in steps (2) and (3), the number of options can explode and we use the best response classifier (described below) to winnow. An example ST process is shown in Figure 2.",positive
33,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","Using the resulting SS and SS+ datasets, we train Pointer generator networks (PGN) (See et al., 2017), DialoGPT (D-GPT) (Zhang et al., 2019) and their variants to produce a fluent answer response
7at most three responses per question 8HarvestingQA is a QA dataset containing 1M QA pairs generated over 10,000 top-ranking Wikipedia articles. This dataset is noisy as the questions are automatically generated using an LSTM based encoder-decoder model (which makes use of coreference information) and the answers are extracted using a candidate answer extraction module.
generator. The input to the generation model is the question and the answer phrase ÃÂÃÂ£ÃÂÃÂÃÂÃÂq, aÃÂÃÂ£ÃÂÃÂÃÂÃÂ and the response r is the corresponding generation target. PGN: PGNs are widely used SEQ2SEQ models equipped with a copy-attention mechanism capable of copying any word from the input directly into the generated output, making them well equipped to handle rare words and named entities present in questions and answer phrases. We train a 2-layer stacked bi-LSTM PGN using the OpenNMT toolkit (Klein et al., 2017) on the SS and SS+ data. We additionally explore PGNs with pre-training information by initializing the embedding layer with GloVe vectors (Pennington et al., 2014) and pretraining it with ÃÂÃÂ£ÃÂÃÂÃÂÃÂq, rÃÂÃÂ£ÃÂÃÂÃÂÃÂ pairs from the questions-only subset of the OpenSubtitles corpus9 (Tiedemann, 2009). This corpus contains about 14M questionresponse pairs in the training set and 10K pairs in the validation set. We name the pre-trained PGN model as PGN-Pre. We also fine-tune PGN-Pre on the SS and SS+ data to generate two additional variants. D-GPT: DialoGPT (i.e. dialogue generative pretrained transformer) (Zhang et al., 2019) is a recently released large tunable automatic conversation model trained on 147M Reddit conversationlike exchanges using the GPT-2 model architecture (Radford et al., 2019). We fine-tune D-GPT on our task using the SS and SS+ datasets. For comparison we also train GPT-2 on our datasets from scratch (i.e. without any pre-training). Finally, to assess the impact of pre-training datasets, we pre-train the GPT-2 on the 14M questions from questions-only subset of the OpenSubtitles data (similar to the PGN-Pre model) to get GPT-2-Pre model. The GPT-2-Pre is later fine-tuned on the SS and SS+ datasets to get two corresponding variants. CoQA Baseline: Conversational Question Answering (CoQA) (Reddy et al., 2019) is a large-scale ConvQA dataset aimed at creating models which can answer the questions posed in a conversational setting. Since we are generating conversational responses for QA systems, it is sensible to compare against such ConvQA systems. We pick one of the best performing BERT-based CoQA model from the SMRCToolkit (Wu et al., 2019) as a baseline.10 We refer to this model as the CoQA baseline. QuAC Baseline: Question Answering in Context
9http://forum.opennmt.net/t/english-chatbot-model-withopennmt/184
10one of the top performing model with available code.
is another ConvQA dataset. We use the modified version of BiDAF model presented in (Choi et al., 2018) as a second baseline. Instead of a SEQ2SEQ generation, it selects spans from passage which acts as responses. We use the version of this model implemented in AllenNLP (Gardner et al., 2017) and refer to this model as the QuAC baseline. STs+BERT Baseline: We also compare our generation models with the technique that created the SS and SS+ training datasets (i.e. the responses generated by STs ranked with the BERT response classifier).
We validate all the SEQ2SEQ models on the human annotated SG data (Table 1).",positive
34,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","To have a fair and unbiased comparison, we create a new 500 question sample from the SQuAD 2.0 dev set (SQuAD-dev-test) which is unseen for all the models and baselines. This sample contains ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 20% of the questions that cannot be handled by the STs (parser errors). For such questions, we default to outputting the answer-phrase as the response for the STs+BERT baseline. For the CoQA baseline and the QuAC baseline, we run their models on passages (corresponding to the questions) from SQuAD-dev-test to get their responses.
To demonstrate that our models too can operate in a fully automated setting like the CoQA baseline and the QuAC baseline, we generate their responses using the answer spans selected by a BERTbased SQuAD model (instead of the gold answer span from the SQuAD-dev-test).
For automatic evaluation we compute validation perplexity of all SEQ2SEQ generation models on SG data (3rd column in Table 3). However, validation perplexity is a weak evaluator of generation models. Also, due to the lack of human-generated references in SQuAD-dev-test, we cannot use other typical generation based automatic metrics. Therefore, we use Amazon Mechanical Turk to do human evaluation. Each response is judged by 5 annotators. We ask the annotators to identify if the response is conversational and answers the question correctly. While outputting answer-phrase to all questions is trivially correct, this style of response generation seems robotic and unnatural in a prolonged conversation. Therefore, we also ask the annotators to judge if the response is a completesentence (e.g. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂit is in IndianaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and not a sentencefragment (e.g. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂIndianaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). For each question and response pair, we show the annotators five options
based on the three properties (correctness, grammaticality, and complete-sentence). These five options (a to e) are shown in the Table 3 header. The best response is a complete-sentence which is grammatical and answers the question correctly (i.e. option e). Other options give us more insights into different modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ behavior. For each response, we assign the majority option selected by the annotators and aggregate their judgments into buckets. We present this evaluation in Table 3.
We compute the inter-annotator agreement by calculating CohenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs kappa (Cohen, 1960) between individual annotatorÃÂÃÂ¢ÃÂÃÂÃÂÃÂs assignments and the aggregated majority options. The average CohenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs kappa (weighted by the number of annotations for every annotator) is 0.736 (i.e. substantial agreement).
The results reveal that CoQA baseline does the worst in terms of option e. The main reason for that is because most of the responses generated from this baseline are exact answer spans. Therefore, we observe that it does very well in option b (i.e. correct answer but not a complete-sentence). The QuAC baseline can correctly select span-based informative responseÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 42% of the time. Other times, however, it often selects a span from the passage which is related to the topic but doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt contain the correct answer i.e. (option c). Another problem with this baseline is that it is restricted by the input passage and many not always be able to find a valid span that answers the questions. Our STs+BERT baseline does better in terms of option e compared to the other baselines but it is limited by the STs
and the parser errors. As mentioned earlier, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 20% of the time this baseline directly copies the answerphrase in the response which explains the high percentage of option b.
Almost all models perform better when trained with SS+ data showing that the additional data from Natural Questions and HarvestingQA is helping. Except for the PGN model trained on SS data, all other variants perform better than STs+BERT baseline in terms of option e. The GPT-2 model trained on SS data from scratch does not perform very well because of the small size of training data. The pretraining with OpenSubtitiles questions boosts its performance (option e% for GPT-2Pre model variants > option e % for GPT-2 model variants). The best model however is D-GPT when finetuned with SS+ dataset. While retaining the correct answer, it makes less grammatical errors (lower % in option c and d compared to other models). Furthermore with oracle answers it performs even better (last row in Table 3). This shows that D-GPT can generate better quality responses with accurate answers. We provide some sample responses from different models in Appendix A.",positive
35,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","In this section, we test our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to generate conversational answers on the CoQA dev set, using CoQA baselineÃÂÃÂ¢ÃÂÃÂÃÂÃÂs predicted answers. The CoQA dataset consists of passages from seven different domains (out of which one is Wikipedia) and conversational questions and answers on those
passages. Due to the conversational nature of this dataset, some of the questions are one word (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 3.1%), like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhat?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhy?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ etc. Such questions are out-of-domain for our models as they require the entire context over multiple turns of the conversation to develop their response. Other out-of-domain questions include unanswerable (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 0.8%) and yes/no (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 18.4%) questions. We also donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt consider questions with answers > 5 words (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 11.6%) as they are typically non-factoid questions. We take a random sample of 100 from the remaining questions. This sample contains questions from a diverse set of domains outside of the Wikipedia (on which our models are trained). This includes questions taken from the middle of a conversation (for example, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂwho did they meet ?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) which are unfamiliar for our models. We perform a human evaluation similar to ÃÂÃÂÃÂÃÂ§3.2 on this sample. We compare CoQA against D-GPT trained on the SS+ dataset (with CoQAÃÂÃÂ¢ÃÂÃÂÃÂÃÂs predictions input as answer-phrases). The results are shown in Table 4.
This evaluation reveals that the D-GPT model is able to successfully convert the CoQA answer spans into conversational responses 57% of the time (option e). D-GPT gets the wrong answer 18% of the time (option a and c), because the input answer predicted by the CoQA baseline is also incorrect 17% of the time. However with oracle answers, it is able to generate correct responses 77% of the times (option e). The weighted average CohenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs kappa (Cohen, 1960) score for all annotators in this evaluation is 0.750 (substantial agreement). This result demonstrates ability of our model to generalize over different domains and generate good conversational responses for questions when provided with correct answer spans.",positive
36,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","On the basis of the existing architecture of MUNMT, we introduce self-knowledge distillation (Hahn and Choi, 2019) (SKD) during backtranslation, to enhance the generalization ability of the MUNMT model, as shown in Figure 2(a). Unlike Hahn and Choi (2019)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs method, using two soft target probabilities that are based on the word embedding space, we make full use of multilingual information via self-knowledge distillation.
During back-translation, only language Lj sentences M j(X1i ) are generated before training the MUNMT model in the Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1 direction. However, other languages, which have substantial multilingual information, are not used during this training. Motivated by this, we propose to introduce another language Lz (randomly chosen but dis-
tinct from L1 and Lj) during this training. We argue that the translation from the source sentences through different paths, L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1 and L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lz ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1, should be similar. The MUNMT model matches not only the ground-truth output of language Lj sentences M j(X1i ), but also the soft probability output of language Lz sentences M z(X1i ). The opposite direction is similar. Therefore, this MUNMT model is optimized by minimizing the objective function:
LMBÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ±)LMB + ÃÂÃÂÃÂÃÂ±T 2LSKD,
LSKD = NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=2 |X1|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 KL(X1(M j(X1i )), X 1(Mz(X1i )))
+ NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=2 |Xj |ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 KL(Xj(M1(Xji )), X j(Mz(Xji ))),
(8)
where KL(ÃÂÃÂÃÂÃÂ·) denotes the KL divergence. It is computed over full output distributions to keep these two probability distributions similar. For any languageLj ,X1(M j(X1i )) andX
1(M z(X1i )) denote the softened L1 sentence probability distribution after encoding M j(X1i ) and M
z(X1i ), respectively. M j(X1i ) and M
z(X1i ) were generated by the previous model in the L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lj and L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lz directions, respectively. Xj(M1(Xji )) andXj(M z(Xji )) denote the softened Lj sentence probability distribution after encoding M1(Xji )
Algorithm 2 The LBKD algorithm Input:
Monolingual training data X1, X2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , XN ; LBUNMT models ÃÂÃÂÃÂÃÂ¸LB1 , ÃÂÃÂÃÂÃÂ¸ LB 2 , ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , ÃÂÃÂÃÂÃÂ¸LBM ;
The pretrained model ÃÂÃÂÃÂÃÂ¸0; Number of steps K 1: Initialize ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ¸0 2: while Step q ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ max step K do 3: for j = 1; j < N ; j ++ do 4: Sample batch {Xji } from Xj 5: Compute denoising loss LMD 6: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMD) 7: end for 8: for j = 2; j < N ; j ++ do 9: Sample batch {X1i }from X1
10: Compute back-translation loss LMB 11: Select LBUNMT language L1 belongs and compute distillation loss LLBKD 12: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMB + LLBKD) 13: Sample batch{Xji } from Xj 14: Compute back-translation loss LMB 15: Select LBUNMT language Lj belongs and compute distillation loss LLBKD 16: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMB + LLBKD) 17: end for 18: end while
and M z(Xji ), respectively. M 1(Xji ) and M z(Xji ) were generated by the previous model in the Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1 and Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lz directions, respectively. Note that zero-shot translation was used to translate language Lj to language Lz . The direction Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lz was not trained during MUNMT training.",positive
37,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","Bridging Anaphora Resolution. Since the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ90s, the empirical corpus studies related to bridging have been carried out on various genres and different languages (Fraurud, 1990; Poesio and Vieira, 1998; Poesio, 2004; Nissim et al., 2004; Gardent
and ManueÃÂÃÂÃÂÃÂlian, 2005; Nedoluzhko et al., 2009; Eckart et al., 2012; Markert et al., 2012; RoÃÂÃÂÃÂÃÂsiger, 2018; Poesio et al., 2018). Among those datasets, ISNotes (Markert et al., 2012), BASHI (RoÃÂÃÂÃÂÃÂsiger, 2018) and ARRAU (Poesio et al., 2018) are recent three public English corpora which contain medium- to large-sized bridging annotations and have been used to evaluate systemsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance on bridging anaphora recognition (Hou et al., 2013a; Hou, 2016; RoÃÂÃÂÃÂÃÂsiger et al., 2018), bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b; Hou, 2018a), as well as full bridging resolution (Hou et al., 2014, 2018; RoÃÂÃÂÃÂÃÂsiger et al., 2018). In this paper, we focus exclusively on the task of antecedent selection.
It is worth noting that the bridging definition in the ARRAU corpus is different from the one used in the other two datasets. RoÃÂÃÂÃÂÃÂsiger et al. (2018) pointed out that ISNotes and BASHI contain ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ where bridging anaphors are truly anaphoric and bridging relations are contextdependent, while in ARRAU, most bridging links are purely lexical bridging pairs which are not context-dependent (e.g., Europe ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Spain or Tokyo ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Japan). In this paper, we focus on resolving referential bridging anaphors.
Regarding the algorithm for bridging anaphora resolution, most previous work uses the pairwise model for the task. The model assumes gold or system mention information (NPs) is given beforehand. It creates (positive/negative) training instances by pairing every anaphor a with its preceding mention m. Usually, m is from a set of antecedent candidates which is formed using a fixed window size. Poesio et al. (2004) and Lassalle and Denis (2011) trained such pairwise models to resolve mereological bridging anaphors in the English GNOME corpus2 and the French DEDE corpus (Gardent and ManueÃÂÃÂÃÂÃÂlian, 2005), respectively. One exception is Hou et al. (2013b), which proposed a joint inference framework to resolve bridging anaphors in ISNotes. The framework is built upon the pairwise model and predicts all semantically related bridging anaphors in one document together.
Recently, Hou (2018a) generated a word representation resource for bridging (i.e., embeddings bridging) and proposed a simple deterministic algorithm to find antecedents for bridging anaphors in ISNotes and BASHI. The word representation resource is learned from a large corpus
2The GNOME corpus is not publicly available.
and it captures the common-sense knowledge (i.e., semantic relatedness) between NPs.
Different from the algorithms mentioned above, our QA model does not require the extracted or gold mentions (NPs) as the input, and it predicts the span of the antecedent for a bridging anaphor directly.
Question Answering. Reading comprehension or question answering based on context has attacted much attention within the NLP community, in particular since Rajpurkar et al. (2016) released a large-scale dataset (SQuAD) consisting of 100,000+ questions on a set of paragraphs extracted from Wikipedia articles. Previous work has cast a few traditional NLP tasks as question answering, such as textual entailment (McCann et al., 2018), entityÃÂÃÂ¢ÃÂÃÂÃÂÃÂrelation extraction (Li et al., 2019), and coreference resolution (Wu et al., 2020). However, unlike these tasks, we do not have large scale training datasets for bridging. As a result, we form the questions for our task in a more natural way in order to leverage the existing QA datasets (e.g., SQuAD) that require common-sense reasoning. In addition, we generate a large-scale training dataset of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and demonstrate that it is a good pre-training corpus for bridging anaphora resolution.
Recently, Gardner et al. (2019) argue that we should consider question answering as a format instead of a task in itself. From this perspective, our work can be seen as a specific probing task to test a QA modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to understand bridging anaphora based on context.
Winograd Schema Challenge. Bridging anaphora resolution shares some similarities with Winograd Schema Challenge (WSC). Specifically, in both tasks, one has to understand the context to find the antecedents for anaphors. However, the antecedent search space in bridging anaphora resolution is much bigger than the one in WSC. This is because an anaphor (pronoun) and its antecedent in WSC are usually from the same sentence, while bridging pairs usually require cross-sentence inference. For instance, in ISNotes, only around 26% of anaphors have antecedents occurring in the same sentence, and 23% of anaphors have antecedents that are more than two sentences away.
Recently, Kocijan et al. (2019) use some heuristics to generate a large-scale WSC-like dataset and report that the model pre-trained on this dataset achieves the best results on several WSC datasets after being fine-tuned on a small in-domain dataset. We find similar patterns of results for bridging anaphora resolution (see Section 5.3).",positive
38,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","We formulate bridging anaphora resolution as a context-based QA problem. More specifically, given a bridging anaphor a and its surrounding
context ca, we rephrase a as a question qa. The goal is to predict a text span sa from ca that is the antecedent of a. We propose to use the spanbased QA framework to extract sa. In general, our BARQA system is built on top of the vanilla BERT QA framework (Devlin et al., 2019). We further modify the inference algorithm to guarantee that the answer span sa should always appear before the bridging anaphor a (see Section 3.4 for more details).
Following Devlin et al. (2019), we present the input question qa and the context ca as a single packed sequence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[cls] qa [sep] caÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and calculate the probabilities of every word in ca being the start and end of the answer span. The training objective is the log-likelihood of the correct start and end positions.",positive
39,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","Different from the SQuAD-style question answering where there is no specific requirement for the position of the predicted span, in bridging anaphora resolution, an anaphor must appear after its antecedent. Therefore in the inference stage, for each bridging anaphor a, we first identify the position of a in its context ca, then we only predict text spans which appear before a. We further prune the list of predicted text spans by only keeping the top k span candidates that contain at most l words (k and l are empirically set to 20 and 5, respectively). We also prune span predictions that are function words (e.g., a, an, the, this, that).",positive
40,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","Bridging anaphora is a complex phenomenon, and there are no large-scale corpora available for referential bridging. In this section, we describe how we generate a large scale ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ dataset.
Hou (2018b) explores the syntactic prepositional and possessive structures of NPs to train word embeddings for bridging. Inspired by this work, we first use these structures to identify ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbridging anaphorsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and the corresponding ÃÂÃÂ¢ÃÂÃÂÃÂÃÂantecedentsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Next, we map them back to the discourse to create bridging-like examples.
More specifically, given a text, we first extract NPs containing the prepositional structure (e.g., X preposition Y) or the possessive structure (e.g., Y
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs X). In order to have a high-quality set of automatically generated bridging annotations, we apply an additional constraint to the above NPs, i.e., X and Y should not contain any other NP nodes in the constituent tree. For instance, we do not consider NPs such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe political value of imposing sanctions against South AfricaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe cost of repairing the regionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs transportation systemÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Figure 2 illustrates how we generate a bridging annotation with a sentence pair {sy, sx} from a raw text5: we first extract the NP ÃÂÃÂ¢ÃÂÃÂÃÂÃÂobstruction of justiceÃÂÃÂ¢ÃÂÃÂÃÂÃÂ from the sentence si and identify X/Y in this extracted NP (i.e., X = obstruction, Y = justice). Next, we collect a list of sentences S from the
parameters for various training configurations on a small set (10 documents) of the ISNotes corpus and the BASHI corpus, respectively. On both corpora, we observed that a learning rate of 3e-5, 4e-5, or 5e-5 has minimal impact on results; and for each learning rate, the result continues improving at the beginning (epochs = 1,2,3,4,5), but the performances stays more or less the same after epochs>5.
5The raw text is from the Gigaword corpus (Parker et al., 2011; Napoles et al., 2012).
whole text. Every sentence in S contains Y but does not contain X. If S contains more than one sentence, we choose the one which is the closest to si as sy. This is because close sentences are more likely semantically related. Finally, we generate the sentence sx by replacing ÃÂÃÂ¢ÃÂÃÂÃÂÃÂobstruction of justiceÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in the original sentence si with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe obstructionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. This gives us a quasi-bridging example with two adjacent sentences (i.e., sy and sx) and a bridging link (i.e., justice - the obstruction).
As a result, we obtain a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasibridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data (i.e., around 2.8 million bridging pairs) by applying the method described above to the NYT19 section of the automatically parsed Gigaword corpus.
In order to understand the quality of our ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasibridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training dataset, we randomly sample 100 quasi-bridging sentence pairs and manually check bridging annotations in these instances. We score each bridging annotation using a scale of 0-2: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ means that the bridging annotation is correct and the sentence pair sounds natural; ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ indicates that the example makes sense, but it does not sound natural in English; and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ denotes that the annotation is unacceptable. Overall, we find that 25% of instances and 37% of instances have a score of 2 and 1, respectively. And the remaining 38% of instances are scored as zero. In general, our noisy ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training dataset does contain a large number of diverse bridging pairs.",positive
41,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","We use four datasets for experiments. The first dataset is ISNotes6 released by Markert et al.
6http://www.h-its.org/en/research/nlp/ isnotes-corpus
(2012). This dataset contains 50 texts with 663 referential bridging NPs from the World Street Journal (WSJ) portion of the OntoNotes corpus (Weischedel et al., 2011). The second dataset is called BASHI from RoÃÂÃÂÃÂÃÂsiger (2018). It contains 459 bridging NPs7 with 344 referential anaphors from 50 WSJ texts8. Note that bridging anaphors in these two corpora are not limited to definite NPs as in previous work (Poesio et al., 1997, 2004; Lassalle and Denis, 2011) and bridging relations are not limited to the prototypical whole ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ part relation or set ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ element relation. We consider these two corpora as expert-annotated in-domain datasets.
We assume that some reasoning skills (e.g., world knowledge, word relatedness) required to answer questions in SQuAD can also be applied for bridging anaphora resolution. Therefore we include the SQuAD 1.1 training data (Rajpurkar et al., 2016) as one training dataset. Another training dataset is the large scale quasi-bridging corpus (QuasiBridging) described in Section 4.
Table 1 summarizes the four datasets mentioned above. Note that in ISNotes and BASHI, the number of QA pairs is more than the number of bridging anaphors. This is because an anaphor can have multiple antecedents (e.g., coreferent mentions of the same antecedent entity).",positive
42,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","Following Hou (2018a), we use accuracy on the number of bridging anaphors to measure systemsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance for resolving bridging anaphors on ISNotes and BASHI. It is calculated as the number of the correctly resolved bridging anaphors divided by the total number of bridging anaphors.
We measure two types of accuracy: lenient accuracy and strict accuracy. In strict accuracy, only the original gold antecedent annotations are counted as the correct answers. For lenient accuracy, we add the additional variations of the original antecedent annotations (described in Section 3.3) into the correct answer list. For instance, suppose that the gold antecedent annotation is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe Four Seasons restaurantÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and the predicted span is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂFour Seasons restaurantÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, we count this prediction as an incorrect prediction in strict accuracy evaluation. However, it is a correct prediction in lenient accuracy evaluation.
7BASHI considers comparative anaphora as bridging anaphora. We exclude them from this study.
8Note that these WSJ articles are different from the ones in ISNotes.
It is worth noting that our lenient accuracy corresponds to the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂexact matchÃÂÃÂ¢ÃÂÃÂÃÂÃÂ metric in SQuAD (Rajpurkar et al., 2016). The correct answer lists that are generated as described in Section 3.3 can partially address the evaluation problem of imperfect system mention predictions. We do not report F1 score because it will give partial credit for a prediction that does not capture the main semantics of the original gold annotation, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe Four SeasonsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
During evaluation, for every bridging anaphor a, let sa be the sentence containing a, we use the first sentence of the text, the previous two sentences of sa, as well as sa to form aÃÂÃÂ¢ÃÂÃÂÃÂÃÂs surrounding context ca. This is in line with Hou (2018a)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs antecedent candidate selection strategy.",positive
43,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","Previous work for bridging anaphora resolution on ISNotes and BASHI use gold/system mentions as antecedent candidates and report results using strict accuracy (Hou et al., 2013b; Hou, 2018a).
In order to fairly compare against these systems, for every bridging anaphor a, we first map all top 20 span predictions of our system BARQA to the gold/system mentions, then we choose the gold/system mention with the highest confidence score as the predicted antecedent. Specifically, we map a predicted span s to a mention m if they share the same head and s is part of mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² (mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² is created by removing all postmodifiers from m). For instance, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtotal potential claimsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is mapped to the mention ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe total potential claims from the disasterÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. If a predicted span can not be mapped to any gold/system mentions, we filter it out. Following
Hou (2018a), we only keep the predictions whose semantic types are ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtimeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ if a is a time expression. The above process is equal to using gold/system mentions and their semantic information to further prune BARQAÃÂÃÂ¢ÃÂÃÂÃÂÃÂs span predictions.
Table 3 and Table 4 compare the results of our system BARQA against previous studies for bridging anaphora resolution on ISNotes and BASHI, respectively. For both datasets, the BARQA model is trained using the best strategy reported in Table 2 (pre-training with QuasiBridging + fine-tuning with small in-domain data).
On ISNotes, previously Hou (2018a) reported the best result by adding the prediction from a deterministic algorithm (embeddings bridging (NP head + modifiers)) as an additional feature into the global inference model (MLN II) proposed by Hou et al. (2013b). The deterministic algorithm is based on word embeddings for bridging and models the meaning of an NP based on its head noun and modifications.
Our system BARQA, when using the gold mentions together with their semantic information to further prune the span predictions, achieves the new state-of-the-art result on ISNotes, with a strict accuracy of 50.08% (see BARQA with gold mentions/semantics, strict accuracy in Table 3). How-
ever, we argue that using gold mention information to construct the set of antecedent candidates is a controlled experiment condition, and our experiment setup BARQA without mention information, lenient accuracy is a more realistic scenario in practice.
On BASHI, Hou (2018a) reported an accuracy of 29.94% (strict accuracy) using automatically extracted mentions from the gold syntactic tree annotations. Our system BARQA without any mention/semantic information achieves an accuracy of 32.27% using the same strict accuracy evaluation. The result of BARQA is further improved with an accuracy of 38.66% when we integrate mention/semantic information into the model.
Note that Hou (2018a) also adapted their deterministic algorithm to resolve lexical bridging anaphors on ARRAU (Poesio et al., 2018) and reported an accuracy of 32.39% on the RST Test dataset. Although in this paper we do not focus on lexical bridging, our model BARQA can also be applied to resolve lexical bridging anaphors. We found that BARQA trained on the RST Train dataset alone with around 2,000 QA pairs achieves an accuracy of 34.59% on the RST Test dataset.",positive
44,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","In this paper, we model bridging anaphora resolution as a question answering problem and propose a QA system (BARQA) to solve the task.
We also propose a new method to automatically generate a large scale of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our QA system, when
trained on this ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training dataset and fine-tuned on a small amount of in-domain dataset, achieves the new state-of-the-art results on two bridging corpora.
Compared to previous systems, our model is simple and more realistic in practice: it does not require any gold annotations to construct the list of antecedent candidates. Moreover, under the proposed QA formulation, our model can be easily strengthened by adding other span-based text understanding QA corpora as pre-training datasets.
Finally, we will release our experimental QA datasets (in the SQuAD json format) for bridging anaphora resolution on ISNotes and BASHI. They can be used to test a QA modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to understand a text in terms of bridging inference.",positive
45,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","Cross-sentence relation extraction, which aims to identify relations between two arguments that are not mentioned in the same sentence or relations that cannot be supported by any single sentence, is an essential step in building knowledge bases from large-scale corpora automatically (Ji et al., 2010; Swampillai and Stevenson, 2010; Surdeanu, 2013). It has yet to receive extensive study in natural language processing, however. In particular, although dialogues readily exhibit cross-sentence relations, most existing relation extraction tasks focus on texts from formal genres such as professionally written and edited news reports or well-edited websites (Elsahar et al., 2018; Yao et al., 2019;
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ  Equal contribution.
Mesquita et al., 2019; Grishman, 2019), while dialogues have been under-studied.
In this paper, we take an initial step towards studying relation extraction in dialogues by constructing the first human-annotated dialogue-based relation extraction dataset, DialogRE. Specifically, we annotate all occurrences of 36 possible relation types that exist between pairs of arguments in the 1,788 dialogues originating from the complete transcripts of Friends, a corpus that has been widely employed in dialogue research in recent years (Catizone et al., 2010; Chen and Choi, 2016; Chen et al., 2017; Zhou and Choi, 2018; Rashid and Blanco, 2018; Yang and Choi, 2019). Altogether, we annotate 10,168 relational triples. For each (subject, relation type, object) triple, we also annotate the minimal contiguous text span that most clearly expresses the relation; this may enable researchers to explore relation extraction methods that provide fine-grained explanations along with evidence sentences. For example, the bolded text span ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbrotherÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in Table 1 indicates the PER:SIBLINGS relation (R1 and R2) between speaker 2 (S2) and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂFrankÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Our analysis of DialogRE indicates that the supporting text for most (approximately 96.0%) an-
notated relational triples includes content from multiple sentences, making the dataset ideal for studying cross-sentence relation extraction. This is perhaps because of the higher person pronoun frequency (Biber, 1991) and lower information density (Wang and Liu, 2011) in conversational texts than those in formal written texts. In addition, 65.9% of relational triples involve arguments that never appear in the same turn, suggesting that multi-turn information may play an important role in dialogue-based relation extraction. For example, to justify that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPheebsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is an alternate name of S2 in Table 1, the response of S2 in the second turn is required as well as the first turn.
We next conduct a thorough investigation of the similarities and differences between dialoguebased and traditional relation extraction tasks by comparing DialogRE and the Slot Filling dataset (McNamee and Dang, 2009; Ji et al., 2010, 2011; Surdeanu, 2013; Surdeanu and Ji, 2014), and we argue that a relation extraction system should be aware of speakers in dialogues. In particular, most relational triples in DialogRE (89.9%) signify either an attribute of a speaker or a relation between two speakers. The same phenomenon occurs in an existing knowledge base constructed by encyclopedia collaborators, relevant to the same dialogue corpus we use for annotation (Section 3.2). Unfortunately, most previous work directly applies existing relation extraction systems to dialogues without explicitly considering the speakers involved (Yoshino et al., 2011; Wang and Cardie, 2012).
Moreover, traditional relation extraction methods typically output a set of relations only after they have read the entire document and are free to rely on the existence of multiple mentions of a relation throughout the text to confirm its existence. However, these methods may be insufficient for powering a number of practical real-time dialoguebased applications such as chatbots, which would likely require recognition of a relation at its first mention in an interactive conversation. To encourage automated methods to identify the relationship between two arguments in a dialogue as early as possible, we further design a new performance evaluation metric for the conversational setting, which can be used as a supplement to the standard F1 measure (Section 4.1).
In addition to dataset creation and metric design, we adapt a number of strong, representative learning-based relation extraction methods (Zeng
et al., 2014; Cai et al., 2016; Yao et al., 2019; Devlin et al., 2019) and evaluate them on DialogRE to establish baseline results on the dataset going forward. We also extend the best-performing method (Devlin et al., 2019) among them by letting the model be aware of the existence of arguments that are dialogue participants (Section 4.2). Experiments on DialogRE demonstrate that this simple extension nevertheless yields substantial gains on both standard and conversational RE evaluation metrics, supporting our assumption regarding the critical role of tracking speakers in dialogue-based relation extraction (Section 5).
The primary contributions of this work are as follows: (i) we construct the first human-annotated dialogue-based relation extraction dataset and thoroughly investigate the similarities and differences between dialogue-based and traditional relation extraction tasks, (ii) we design a new conversational evaluation metric that features the timeliness aspect of interactive communications in dialogue, and (iii) we establish a set of baseline relation extraction results on DialogRE using standard learning-based techniques and further demonstrate the importance of explicit recognition of speaker arguments in dialogue-based relation extraction.",positive
46,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","We focus on the annotation of relational triples (i.e., (subject, relation type, object)) in which at least one of the arguments is a named entity. We regard an uninterrupted stream of speech from one speaker and the name of this speaker as a turn.
As we follow the TAC-KBP guideline to annotate relation types and design new types, we use internal annotators (two authors of this paper) who are familiar with this task. For a pilot annotation, annotator A annotates relational triples in each scene in all transcripts and form a dialogue
by extracting the shortest snippet of contiguous turns that covers all annotated relational triples and sufficient supportive contexts in this scene. The guidelines are adjusted during the annotation.2 We prefer to use speaker name (i.e., the first word or phrase of a turn, followed by a colon) as one argument of a speaker-related triple if the corresponding full names or alternate names of the speaker name also appear in the same dialogue, except for relation PER:ALTERNATE NAMES in which both mentions should be regarded as arguments. For an argument pair (i.e., (subject, object)), there may exist multiple relations between them, and we annotate all instances of all of them. For each
2As the pilot annotation only involves one annotator, we admit there may exist a certain degree of bias in defining new relation types and labeling argument pairs.
triple, we also annotate its trigger: the smallest extent (i.e., span) of contiguous text in the dialogue that most clearly indicates the existence of the relation between two arguments. If there exist multiple spans that can serve as triggers, we only keep one for each triple. For relation types such as PER:TITLE and PER:ALTERNATE NAMES, it is difficult to identify such supportive contexts, and therefore we leave their triggers empty. For each relational triple, we annotate its inverse triple if its corresponding inverse relation type exists in the schema (e.g., PER:CHILDREN and PER:PARENTS) while the trigger remains unchanged.
In the second process, annotator B annotates the possible relations between candidate pairs annotated by annotator A (previous relation labels are hidden). CohenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs kappa among annotators is around 0.87. We remove the cases when annotators cannot reach a consensus. On average, each dialogue in DialogRE contains 4.5 relational triples and 12.9 turns, as shown in Table 3. See Table 1 for relational triple examples (R1, R2, and R3).",positive
47,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","As a pilot study, we examine the similarities and differences between dialogue-based and traditional relation extraction datasets that are manually annotated. We compare DialogRE with the official SF (2013-2014) dataset (Surdeanu, 2013; Surdeanu and Ji, 2014) as 47.2% of relation types in DialogRE originate from the SF relation types (Section 2.1), and 92.2% of the source documents in it that contain ground truth relational triples are formally written newswire reports (72.8%) or well-edited web documents (19.4%) compared to the remaining documents from discussion fora. We show the relation distributions in DialogRE and SF in Figure 1 and Figure 2 (Appendix A.2), respectively. Half of the top ten relation types in DialogRE are newly defined (PER:GIRL/BOYFRIEND, PER:POSITIVE(NEGATIVE) IMPRESSION, PER:FRIENDS, and PER:ROOMMATE), partially justifying the need for new relation types.
Argument Type: Based on the predefined SF and DialogRE relation types, a subject is expected to be an entity of type PER, ORG, or geo-political entity (GPE). Notably, subjects of most relational triples (96.8% vs. 69.7% in the SF dataset) in DialogRE are person names. The coarse-grained object type is entity, string, or value (i.e., a numerical value or a date). As shown in Table 4, we observe that a higher proportion (80.1%) of objects are entities in DialogRE compared to that in SF (65.3%).
In particular, the subjects of 77.3% of relational triples are speaker names, and more than 90.0% of relational triples contain at least one speaker argument. The high percentage of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂspeaker-centricÃÂÃÂ¢ÃÂÃÂÃÂÃÂ relational triples and the low percentage of ORG and GPE arguments in DialogRE is perhaps because the transcripts for annotation are from a single situation comedy that involves a small group of characters in a very limited number of scenes (see more discussions in Section 5.3).
Distance Between Argument Pairs: It has been shown that there is a longer distance between two arguments in the SF dataset (Surdeanu, 2013; Huang et al., 2017) compared to that in many widely used human-annotated relation extraction datasets such as ACE (Doddington et al., 2004) and SemEval (Hendrickx et al., 2010). However, it is not trivial to compute an accurate distance between two arguments in a dialogue, especially for cases containing arguments that are speaker names. We instead consider different types of distances (e.g., average and minimum) between two argument mentions in a dialogue. We argue that DialogRE exhibits a similar level of difficulty as SF from the perspective of the distance between two arguments. 41.3% of arguments are separated by at least seven words even considering the minimum distance, and the percentage can reach as high as 96.5% considering the average distance, contrast with 46.0% in SF (Huang et al., 2017) and 59.8% in a recently released cross-sentence relation extraction dataset DocRED, in which Wikipedia articles serve as documents (Yao et al., 2019). Note that the provenance/evidence sentences in SF and DocRED are provided by automated systems or annotators. Also, 95.6% of relational triples from an annotated subset of DialogRE (Section 5.2) require reasoning over multiple sentences in a dialogue, compared with 40.7% in DocRED (Table 7). See Figure 3 in Appendix A.3 for more details.",positive
48,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","As annotated triggers are rarely available in existing relation extraction datasets (Aguilar et al., 2014), the connections between different relation types and trigger existence are under-investigated.
Relation Type: In DialogRE, 49.6% of all relational triples are annotated with triggers. We find that argument pairs are frequently accompanied by triggers when (1) arguments have the same type such as PER:FRIENDS, (2) strong emotions are involved (e.g., PER:POSITIVE(NEGATIVE) IMPRESSION), or (3) the relation type is related to death or birth (e.g., GPE:BIRTHS IN PLACE). In comparison, a relation between two arguments of different types (e.g., PER:ORIGIN and PER:AGE) is more likely to be implicitly expressed instead of relying on triggers. This is perhaps because there exist fewer possible relations between such an argument pair compared to arguments of the same type, and a relatively short distance between such an argument pair might be sufficient to help the listeners understand the message correctly. For each relation type, we report the percentage of relational triples with triggers in Table 2.
Argument Distance: We assume the existence of triggers may allow a longer distance between argument pairs in a text as they help to decrease ambiguity. This assumption may be empirically
validated by the longer average distance (68.3 tokens) between argument pairs with triggers in a dialogue, compared to the distance (61.2 tokens) between argument pairs without any triggers.",positive
49,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","Given a dialogue D = s1 : t1, s2 : t2, . . . , sm : tm and an argument pair (a1, a2), where si and ti denote the speaker ID and text of the ith turn, respectively, and m is the total number of turns, we evaluate the performance of approaches in extracting relations between a1 and a2 that appear in D in the following two settings.
Standard Setting: As the standard setting of relation extraction tasks, we regard dialogue D as document d. The input is a1, a2, and d, and the expected output is the relation type(s) between a1 and a2 based on d. We adopt F1, which is the harmonic mean of precision (P) and recall (R), for evaluation.
Conversational Setting: Instead of only considering the entire dialogue, here we can regard the first i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ m turns of the dialogue as d. Accordingly, we propose a new metric F1c, the harmonic mean of conversational precision (Pc) and recall (Rc), as a supplement to the standard F1. We start by introducing some notation that will be used in the definition of F1c. LetOi denote the set of predicted relation types when the input is a1, a2, and the first i turns (i.e., d = s1 : t1, s2 : t2, . . . , si : ti). For an argument pair (a1, a2), let L denote its corresponding set of relation types that are manually annotated based on the full dialogue. R represents the set of 36 relation types. By definition, Oi, L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R. We define that auxiliary function (x) returns m if x does not appear in D. Otherwise, it returns the index of the turn where x first appears.
We define auxiliary function ÃÂÃÂÃÂÃÂ±(r) as: (i) For each relation type r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L, if there exists an annotated trigger for r, ÃÂÃÂÃÂÃÂ±(r) = (ÃÂÃÂÃÂÃÂ»r) where ÃÂÃÂÃÂÃÂ»r denotes the trigger. Otherwise, ÃÂÃÂÃÂÃÂ±(r) = m. (ii) For each r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R\L, ÃÂÃÂÃÂÃÂ±(r) = 1. We define the set of relation types that are evaluable based on the first i turns by Ei:
Ei = {r | i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ max{(a1), (a2), ÃÂÃÂÃÂÃÂ±(r)}} (1)
The interpretation of Equation 1 is that given d containing the first i turns in a dialogue, relation type r associated with a1 and a2 is evaluable if a1, a2, and the trigger for r have all been mentioned in d. The definition is based on our assumption
that we can roughly estimate how many turns we require to predict the relations between two arguments based on the positions of the arguments and triggers, which most clearly express relations. See Section 5.2 for more discussions.
The conversational precision and recall for an input instance D, a1, and a2 are defined as:
Pc(D, a1, a2) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm
i=1 |Oi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© Ei|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm i=1 |Oi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© Ei|
(2)
Rc(D, a1, a2) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm
i=1 |Oi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© Ei|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂm i=1 |L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© Ei|
(3)
We average the conversational precision/recall scores of all instances to obtain the final conversational precision/recall.
Pc =
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1,a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2 Pc(DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1, a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1,a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2 1
(4)
Rc =
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1,a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2 Rc(DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1, a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
DÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²1,a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² 2 1
(5)
and F1c = 2 ÃÂÃÂÃÂÃÂ· Pc ÃÂÃÂÃÂÃÂ· Rc/(Pc + Rc).",positive
50,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","We report the performance of all baselines in both the standard and conversational settings in Table 5. We run each experiment five times and report the average F1 and F1c along with standard deviation (ÃÂÃÂÃÂÃÂ). The fine-tuned BERT method already outperform other baselines (e.g., BiLSTM that achieves 51.1% in F1 on DocRED (Yao et al., 2019)), and our speaker-aware extension to the BERT baseline further leads to 2.7% and 2.2% improvements in F1 and F1c, respectively, on the test set of DialogRE, demonstrating the importance of tracking speakers in dialogue-based relation extraction.
Conversational Metric: We randomly select 269 and 256 instances, which are associated with 50 dialogues from each of the dev and test sets, respectively. For each of relational instances (188 in total) that are previously labeled with triggers in the subsets, annotator A labels the smallest turn iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ such that the first iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ turns contain sufficient information to justify a relation. The average distance between iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and our estimation max{(a1), (a2), ÃÂÃÂÃÂÃÂ±(r)} in Equation (1) (Section 4.1) is only 0.9 turn, supporting our hypothesis that the positions of arguments and triggers may be good indicators for estimating the minimum turns for humans to make predictions.
For convenience, we use BERT for the following discussions and comparisons.
Ground Truth Argument Types: Methods in Table 5 are not provided with ground truth argument types considering the unavailability of this kind of annotation in practical use. To study the impacts of argument types on DialogRE, we report the performance of four methods, each of which additionally takes as input the ground truth argument types as previous work (Zhang et al., 2017; Yao et al., 2019). We adopt the same baseline for a direct comparison
except that the input sequence is changed.
In Method 1, we simply extend the original input sequence of BERT (Section 4.2) with newly-introduced special tokens that represent argument types. The input sequence is [CLS]d[SEP]ÃÂÃÂÃÂÃÂ1a1[SEP]ÃÂÃÂÃÂÃÂ2a2[SEP], where ÃÂÃÂÃÂÃÂi is a special token representing the argument type of ai (i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1, 2}). For example, given a1 of type PER and a2 of type STRING, ÃÂÃÂÃÂÃÂ1 is [PER] and ÃÂÃÂÃÂÃÂ2 is [STRING]. In Method 2, we extend the input sequence of BERTS with ÃÂÃÂÃÂÃÂi defined in Method 1 (i.e., [CLS]dÃÂÃÂÃÂÃÂ[SEP]ÃÂÃÂÃÂÃÂ1aÃÂÃÂÃÂÃÂ1[SEP]ÃÂÃÂÃÂÃÂ1aÃÂÃÂÃÂÃÂ2[SEP]). We also follow the input sequence of previous single-sentence relation extraction methods (Shi and Lin, 2019; Joshi et al., 2020) and refer them as Method 3 and 4, respectively. We provide the implementation details in Appendix A.5. As shown in Table 6, the best performance achieved by Method 2 is not superior to that of BERTS, which does not leverage ground truth argument types. Therefore, we guess that ground truth argument types may only provide a limited, if at all positive, contribution to the performance on DialogRE.
Ground Truth Triggers: We investigate what performance would be ideally attainable if the model could identify all triggers correctly. We append the ground truth triggers to the input sequence on the baseline, and the F1 of this model is 74.9%, a 16.4% absolute improvement compared to the BERT baseline. In particular, through the introduction of triggers, we observe a 22.9% absolute improvement in F1 on relation types whose inverse relation types are themselves (e.g., PER:ROOMMATE and PER:SPOUSE). These experimental results show the critical role of triggers in dialogue-based relation extraction. However, trigger identification is perhaps as difficult as relation extraction, and it is labor-intensive to annotate large-scale datasets with triggers. Future research may explore how to identify triggers based on a small amount of human-annotated triggers as seeds (Bronstein et al., 2015; Yu and Ji, 2016).",positive
51,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","We analyze the outputs on the dev set and find that BERT tends to make more mistakes when there exists an asymmetric inverse relation of the relation to be predicted compared to those that have symmetric inverse relations. For example, the baseline mistakenly predicts S2 as the subordinate of S1 based on the following dialogue: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. . . S2: Oh. Well, I wish I could say no, but you canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt stay my assistant forever. Neither can you Sophie, but for different reasons. S1: God, I am so glad you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have a problem with this, because if you did, I wouldnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt even consider applying. . . ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Introducing triggers into the input sequence leads to a relatively small gain (11.0% in F1 on all types with an asymmetric inverse relation) perhaps because inverse relation types share the same triggers (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmy assistantÃÂÃÂ¢ÃÂÃÂÃÂÃÂ serves as the trigger for both PER:BOSS and PER:SUBORDINATE). One possible solution may be the use of directed syntactic graphs constructed from the given dialogue, though the performance of coreference resolution and dependency parsing in dialogues may be relatively unsatisfying.
A major limitation in DialogRE is that all transcripts for annotation are from Friends, which may limit the diversity of scenarios and generality of the relation distributions. It may be useful to leverage existing triples in knowledge bases (e.g., Fandom) for thousands of movies or TV shows using distant supervision (Mintz et al., 2009), considering the time-consuming manual annotation process. In addition, dialogues in Friends presents less variation based on linguistic features (Biber, 1991) than natural conversations; nonetheless, compared to other registers such as personal letters and prepared speeches, there are noticeable linguistic similarities between natural conversations and television dialogues in Friends (Quaglio, 2009).",positive
52,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","Cross-Sentence Relation Extraction Datasets Different from the sentence-level relation extraction (RE) datasets (Roth and Yih, 2004; Hendrickx et al., 2010; Riedel et al., 2010; Zhang and Wang, 2015; Zhang et al., 2017; Han et al., 2018), in which relations are between two arguments in the same sentence, we focus on cross-sentence RE tasks (Ji et al., 2011; Surdeanu, 2013; Surdeanu and Ji, 2014) and present the first dialogue-based RE dataset, in which dialogues serve as input contexts instead of formally written sentences or documents.
We compare DialogRE and existing cross-sentence RE datasets (Li et al., 2016; Quirk and Poon, 2017; Yao et al., 2019; Mesquita et al., 2019) in Table 7. In this paper, we do not consider relations that take relations or events as arguments and are also likely to span multiple sentences (Pustejovsky and Verhagen, 2009; Do et al., 2012; Moschitti et al., 2013).
Relation Extraction Approaches Over the past few years, neural models have achieved remarkable success in RE (Nguyen and Grishman, 2015b,a; Adel et al., 2016; Yin et al., 2017; Levy et al., 2017; Su et al., 2018; Song et al., 2018; Luo et al., 2019), in which the input representation usually comes from shallow neural networks over pre-trained word and character embeddings (Xu et al., 2015; Zeng et al., 2015; Lin et al., 2016). Deep contextualized word representations such as the ELMo (Peters et al., 2018) are also applied as additional input features to boost the performance (Luan et al., 2018). A recent thread is to fine-tune pre-trained deep language models on downstream tasks (Radford et al., 2018; Devlin et al., 2019), leading to further performance gains on many RE tasks (Alt et al., 2019; Shi and Lin, 2019; Baldini Soares et al., 2019; Peters et al., 2019; Wadden et al., 2019). We propose an improved method that explicitly considers speaker arguments, which are seldom investigated in previous RE methods.
Dialogue-Based Natural Language Understanding To advance progress in spoken language understanding, researchers have studied dialoguebased tasks such as argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), coreference resolution (Chen et al., 2017; Zhou and Choi, 2018), emotion detection (Zahiri and Choi, 2018), and machine reading comprehen-
sion (Ma et al., 2018; Sun et al., 2019; Yang and Choi, 2019). Besides, some pioneer studies focus on participating in dialogues (Yoshino et al., 2011; Hixon et al., 2015) by asking users relation-related questions or using outputs of existing RE methods as inputs of other tasks (KluÃÂÃÂÃÂÃÂwer et al., 2010; Wang and Cardie, 2012). In comparison, we focus on extracting relation triples from human-human dialogues, which is still under investigation.",positive
53,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","Relation Extraction (RE) is the task of extracting relationships between concepts and entities from text, where relations correspond to semantic categories such as per:spouse, org:founded by or org:subsidiaries (Figure 1). This makes RE a key part of many information extraction systems, and its performance determines the quality of extracted facts for knowledge base population (Ji and Grishman, 2011), or the quality of answers in question answering systems (Xu et al., 2016). Standard benchmarks such as SemEval 2010 Task 8 (Hendrickx et al., 2010) and the more recent TACRED
(Zhang et al., 2017) are essential to evaluate new RE methods and their limitations, and to establish open challenges.
TACRED is one of the largest and most widely used RE datasets. It contains more than 106k examples annotated by crowd workers. The methods best performing on the dataset use some form of pre-training to improve RE performance: finetuning pre-trained language representations (Alt et al., 2019; Shi and Lin, 2019; Joshi et al., 2019) or integrating external knowledge during pre-training, e.g. via joint language modelling and linking on entity-linked text (Zhang et al., 2019; Peters et al., 2019; Baldini Soares et al., 2019); with the last two methods achieving a state-of-the-art performance of 71.5 F1. While this performance is impressive, the error rate of almost 30% is still high. The question we ask in this work is: Is there still room for improvement, and can we identify the underlying factors that contribute to this error rate? We analyse this question from two separate viewpoints: (1) to what extent does the quality of crowd based annotations contribute to the error rate, and (2) what can be attributed to dataset and models? Answers to these questions can provide insights for improving crowdsourced annotation in RE, and suggest directions for future research.
To answer the first question, we propose the following approach: We first rank examples in the development and test sets according to the misclas-
sifications of 49 RE models and select the top 5k instances for evaluation by our linguists. This procedure limits the manual effort to only the most challenging examples. We find that a large fraction of the examples are mislabeled by the crowd. Our first contribution is therefore a extensively relabeled TACRED development and test set.
To answer the second question, we carry out two analyses: (1) we conduct a manual explorative analysis of model misclassifications on the most challenging test instances and categorize them into several linguistically motivated error categories; (2) we formulate these categories into testable hypotheses, which we can automatically validate on the full test set by adversarial rewriting ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ removing the suspected cause of error and observing the change in model prediction (Wu et al., 2019). We find that two groups of ambiguous relations are responsible for most of the remaining errors. The dataset also contains clues that are exploited by models without entity masking, e.g. to correctly classify relations even with limited access to the sentential context.
We limit our analysis to TACRED, but want to point out that our approach is applicable to other RE datasets as well. We make the code of our analyses publicly available.1 In summary, our main contributions in this paper are:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We validate the 5k most challenging examples in the TACRED development and test sets, and provide a revised dataset2 that will improve the accuracy and reliability of future RE method evaluations.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We evaluate the most challenging, incorrectly predicted examples of the revised test set, and develop a set of 9 categories for common RE errors, that will also aid evaluation on other datasets.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We verify our error hypotheses on three stateof-the-art RE models and show that two groups of ambiguous relations are responsible for most of the remaining errors and that models exploit cues in the dataset when entities are unmasked.
1https://github.com/DFKI-NLP/tacrev 2Due to licensing restrictions, we can not publish the
dataset but instead provide a patch to the original TACRED.",positive
54,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","In order to identify the impact of potentially noisy, crowd-generated labels on the observed model performance, we start with an analysis of TACREDÃÂÃÂ¢ÃÂÃÂÃÂÃÂs label quality. We hypothesize that while comparatively untrained crowd workers may on average produce relatively good labels for easy relation mentions, e.g. those with obvious syntactic and/or
3https://catalog.ldc.upenn.edu/ LDC2018T24
4https://tac.nist.gov/2017/KBP/index. html
5See the supplemental material provided by Zhang et al. (2017) for details of the dataset creation and annotation process.
lexical triggers, or unambiguous entity type signatures such as per:title, they may frequently err on challenging examples, e.g. highly ambiguous ones or relation types whose scope is not clearly defined.
An analysis of the complete dataset using trained annotators would be prohibitively expensive. We therefore utilize a principled approach to selecting examples for manual analysis (Section 3.1). Based on the TAC-KBP annotation guidelines, we then validate these examples (Section 3.2), creating new Dev and Test splits where incorrect annotations made by crowd workers are revised (Section 3.3).",positive
55,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","We validate the selected examples on the basis of the TAC KBP guidelines.8 We follow the approach of Zhang et al. (2017), and present each example by showing the exampleÃÂÃÂ¢ÃÂÃÂÃÂÃÂs text with highlighted head and tail spans, and a set of relation label suggestions. We differ from their setup by showing
6A similar approach was used e.g. by Barnes et al. (2019). 7See the supplemental material for details on the models,
training procedure, hyperparameters, and task performance. 8https://tac.nist.gov/2014/KBP/ ColdStart/guidelines/TAC_KBP_2014_Slot_ Descriptions_V1.4.pdf
more label suggestions to make the label choice less restrictive: (a) the original, crowd-generated ground truth label, (b) the set of labels predicted by the models, (c) any other relation labels matching the head and tail entity types, and (d) no relation. The suggested positive labels are presented in an alphabetical order and are followed by no relation, with no indication of a labelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs origin. Annotators are asked to assign no relation or up to two positive labels from this set. A second label was allowed only if the sentence expressed two relations, according to the guidelines, e.g. per:city of birth and per:city of residence. Any disagreements are subsequently resolved by a third annotator, who is also allowed to consider the original ground truth label. All annotators are educated in general linguistics, have extensive prior experience in annotating data for information extraction tasks, and are trained in applying the task guidelines in a trial annotation of 500 sentences selected from the development set.",positive
56,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","Relation Extraction on TACRED Recent RE approaches include PA-LSTM (Zhang et al., 2017) and GCN (Zhang et al., 2018), with the former combining recurrence and attention, and the latter leveraging graph convolutional neural networks.
Many current approaches use unsupervised or semi-supervised pre-training: fine-tuning of language representations pre-trained on token-level (Alt et al., 2019; Shi and Lin, 2019) or span-level (Joshi et al., 2019), fine-tuning of knowledge enhanced word representations that are pre-trained on entity-linked text (Zhang et al., 2019; Peters et al., 2019), and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmatching the blanksÃÂÃÂ¢ÃÂÃÂÃÂÃÂ pre-training (Baldini Soares et al., 2019).
Dataset Evaluation Chen et al. (2016) and Barnes et al. (2019) also use model results to assess dataset difficulty for reading comprehension and sentiment analysis. Other work also explores bias in datasets and the adoption of shallow heuristics on biased datasets in natural language inference (Niven and Kao, 2019) and argument reasoning comprehension (McCoy et al., 2019).
Analyzing trained Models Explanation methods include occlusion or gradient-based methods, measuring the relevance of input features to the output (Zintgraf et al., 2017; Harbecke et al., 2018), and probing tasks (Conneau et al., 2018; Kim et al., 2019) that probe the presence of specific features e.g. in intermediate layers. More similar to our approach is rewriting of instances (Jia and Liang, 2017; Ribeiro et al., 2018) but instead of evaluating model robustness we use rewriting to test explicit error hypotheses, similar to Wu et al. (2019).",positive
57,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","Aggressive and abusive behaviour online can lead to severe psychological consequences for its victims (Munro, 2011). This stresses the need for automated techniques for abusive language detection, a problem that has recently gained a great deal of interest in the natural language processing community. The term abuse refers collectively to all forms of expression that vilify or offend an individual or a group, including racism, sexism, personal attacks, harassment, cyber-bullying, and many others. Much of the recent research has focused on detecting explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of am-
biguous terms and figurative language, which has proved more challenging to identify.
The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to model these two phenomena jointly and present the first abusive language detection method that incorporates affective features via a multitask learning (MTL) paradigm.
MTL (Caruana, 1997) allows two or more tasks to be learned jointly, thus sharing information and features between the tasks. In this paper, our main focus is on abuse detection; hence we refer to it as the primary task, while the task that is used to provide additional knowledge ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ emotion detection ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is referred to as the auxiliary task. We propose an MTL framework where a single model can be trained to perform emotion detection and identify abuse at the same time. We expect that affective features, which result from a joint learning setup through shared parameters, will encompass the emotional content of a comment that is likely to be predictive of potential abuse.
We propose and evaluate different MTL architectures. We first experiment with hard parameter sharing, where the same encoder is shared between the tasks. We then introduce two variants of the
MTL model to relax the hard sharing constraint and further facilitate positive transfer. Our results demonstrate that the MTL models significantly outperform single-task learning (STL) in two different abuse detection datasets. This confirms our hypothesis of the importance of affective features for abuse detection. Furthermore, we compare the performance of MTL to a transfer learning baseline and demonstrate that MTL provides significant improvements over transfer learning.",positive
58,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","Techniques for abuse detection have gone through several stages of development, starting with extensive manual feature engineering and then turning to deep learning. Early approaches experimented with lexicon-based features (Gitari et al., 2015), bagof-words (BOW) or n-gram features (Sood et al., 2012; Dinakar et al., 2011), and user-specific features, such as age (Dadvar et al., 2013) and gender (Waseem and Hovy, 2016).
With the advent of deep learning, the trend shifted, with abundant work focusing on neural architectures for abuse detection. In particular, the use of convolutional neural networks (CNNs) for detecting abuse has shown promising results (Park and Fung, 2017; Wang, 2018). This can be attributed to the fact that CNNs are well suited to extract local and position-invariant features (Yin et al., 2017). Character-level features have also been shown to be beneficial in tackling the issue of Out-of-Vocabulary (OOV) words (Mishra et al., 2018b), since abusive comments tend to contain obfuscated words. Recently, approaches to abuse detection have moved towards more complex models that utilize auxiliary knowledge in addition to the abuse-annotated data. For instance, Mishra et al. (2018a, 2019a) used community-based author information as features in their classifiers with promising results. Founta et al. (2019) used transfer learning to fine-tune features from the author metadata network to improve abuse detection.
MTL, introduced by Caruana (1997), has proven successful in many NLP problems, as illustrated in the MTL survey of Zhang and Yang (2017). It is interesting to note that many of these problems are domain-independent tasks, such as part-of-speech tagging, chunking, named entity recognition, etc. (Collobert and Weston, 2008). These tasks are not restricted to a particular dataset or domain, i.e., any text data can be annotated for the phenomena
involved. On the contrary, tasks such as abuse detection are domain-specific and restricted to a handful of datasets (typically focusing on online communication), therefore presenting a different challenge to MTL.
Much research on emotion detection cast the problem in a categorical framework, identifying specific classes of emotions and using e.g., EkmanÃÂÃÂ¢ÃÂÃÂÃÂÃÂs model of six emotions (Ekman, 1992), namely anger, disgust, fear, happiness, sadness, surprise. Other approaches adopt the Valence-ArousalDominance (VAD) model of emotion (Mehrabian, 1996), which represents polarity, degree of excitement, and degree of control, each taking a value from a range. The community has experimented with a variety of computational techniques for emotion detection, including vector space modelling (Danisman and Alpkocak, 2008), machine learning classifiers (Perikos and Hatzilygeroudis, 2016) and deep learning methods (Zhang et al., 2018). In their work, Zhang et al. (2018) take an MTL approach to emotion detection. However, all the tasks they consider are emotion-related (annotated for either classification or emotion distribution prediction), and the results show improvements over single-task baselines. Akhtar et al. (2018) use a multitask ensemble architecture to learn emotion, sentiment, and intensity prediction jointly and show that these tasks benefit each other, leading to improvements in performance. To the best of our knowledge, there has not yet been an approach investigating emotion in the context of abuse detection.",positive
59,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997). Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person.2 A new form
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEqual contribution. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Work was done while at the University of Sheffield.
1Data is available here: https://archive.org/de tails/parody data acl20
2The ÃÂÃÂ¢ÃÂÃÂÃÂÃÂKapou OpaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ column by K. Maniatis parodying Greek popular persons was a source of inspiration for this work - https://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/
of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes. There should be no risk of mistaking their tweets for their subjectÃÂÃÂ¢ÃÂÃÂÃÂÃÂs actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona.
A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008). Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.
While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017), parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to au-
tomatically classify real and parody tweets. For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature- and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-
ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).
We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet;4 and many users were fooled by a Donald Trump parody tweet about ÃÂÃÂ¢ÃÂÃÂÃÂÃÂDow JoansÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019), sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018).
Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016); (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997); (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018).",positive
60,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Parody in Linguistics Parody is an artistic form and literary genre that dates back to Aristophanes in ancient Greece who parodied argumentation styles in Frogs. Verbal parody was studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) and researchers long debated its definition and theoretic distinctions to other types of humor (Grice et al., 1975; Sperber, 1984; Wilson, 2006; Dynel, 2014). In general, verbal parody
3Both the profile description and account name need to mention this ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ https://help.twitter.com/en/ru les-and-policies/parody-account-policy
4https://tinyurl.com/ybbrh74g 5https://tinyurl.com/s34dwgm
involves a highly situated, intentional, and conventional speech act (Rossen-Knill and Henry, 1997) composed of both a negative evaluation and a form of pretense or echoic mention (Sperber, 1984; Wilson, 2006; Dynel, 2014) through which an entity is mimicked or imitated with the goal of criticizing it to a comedic effect. Thus, imitative composition for amusing purpose is an an inherent characteristic of parody (Franke, 1971). The parodist intentionally re-presents the object of the parody and flaunts this re-presentation (Rossen-Knill and Henry, 1997).
Parody on Social Media Parody is considered an integral part of Twitter (Vis, 2013) and previous studies on parody in social media focused on analysing how these accounts contribute to topical discussions (Highfield, 2016) and the relationship between identity, impersonation and authenticity (Page, 2014). Public relation studies showed that parody accounts impact organisations during crises while they can become a threat to their reputation (Wan et al., 2015).
Satire Most related to parody, satire has been tangentially studied as one of several prediction targets in NLP in the context of identifying disinformation (McHardy et al., 2019; de Morais et al., 2019). (Rashkin et al., 2017) compare the language of real news with that of satire, hoaxes, and propaganda to identify linguistic features of unreliable text. They demonstrate how stylistic characteristics can help to decide the textÃÂÃÂ¢ÃÂÃÂÃÂÃÂs veracity. The study of parody is therefore relevant to this topic, as satire and parodies are classified by some as a type of disinformation with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂno intention to cause harm but has potential to foolÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (Wardle and Derakhshan, 2018).
Irony and Sarcasm There is a rich body of work in NLP on identifying irony and sarcasm as a classification task (Wallace, 2015; Joshi et al., 2017). Van Hee et al. (2018) organized two open shared tasks. The first aims to automatically classify tweets as ironic or not, and the second is on identifying the type of irony expressed in tweets. However, the definition of irony is usually ÃÂÃÂ¢ÃÂÃÂÃÂÃÂa trope whose actual meaning differs from what is literally enunciatedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (Van Hee et al., 2018), following the Gricean belief that the hallmark of irony is to communicate the opposite of the literal meaning (Wilson, 2006), violating the first maxim of Quality (Grice et al., 1975). In this
sense, irony is treated in NLP in a similar way as sarcasm (GonzaÃÂÃÂÃÂÃÂlez-IbaÃÂÃÂÃÂÃÂnÃÂÃÂÃÂÃÂez et al., 2011; Khattri et al., 2015; Joshi et al., 2017). In addition to the words in the utterance, further using the user and pragmatic context is known to be informative for irony or sarcasm detection in NLP (Bamman and Smith, 2015; Wallace, 2015). For instance, Oprea and Magdy (2019) make use of user embeddings for textual sarcasm detection. In the design of our data splits, we aim to limit the contribution of this aspects from the results.
Relation to other NLP Tasks The pretense aspect of parody relates our task to a few other NLP tasks. In authorship attribution, the goal is to predict the author of a given text (Stamatatos, 2009; Juola et al., 2008; Koppel et al., 2009). However, there is no intent for the authors to imitate the style of others and most differences between authors are in the topics they write about, which we aim to limit by focusing on political parody. Further, in our setups, no tweets from an author are in both training and testing to limit the impact of terms specific to a particular person.
Pastiche detection (Dinu et al., 2012) aims to distinguish between an original text and a text written by someone aiming to imitate the style of the original author with the goal of impersonating. Most similar in experimental setup to our task, PreotÃÂÃÂÃÂÃÂ§iuc-Pietro and Devlin Marier (2019) aim to distinguish between tweets published from the same account by different types of users: politicians or their staff. While both pastiches and staff writers aim to present similar content with similar style to the original authors, the texts lack the humorous component specific of parodies.
A large body of related NLP work has ex-
plored the inference of user characteristics. Past research studied predicting the type of a Twitter account, most frequently between individual or organizational, using linguistic features (De Choudhury et al., 2012; McCorriston et al., 2015; Mac Kim et al., 2017). A broad literature has been devoted to predicting personal traits from language use on Twitter, such as gender (Burger et al., 2011), age (Nguyen et al., 2011), geolocation (Cheng et al., 2010), political preference (Volkova et al., 2014; PreotÃÂÃÂÃÂÃÂ§iuc-Pietro et al., 2017), income (PreotÃÂÃÂÃÂÃÂ§iuc-Pietro et al., 2015; Aletras and Chamberlain, 2018), impact (Lampos et al., 2014), socio-economic status (Lampos et al., 2016), race (PreotÃÂÃÂÃÂÃÂ§iuc-Pietro and Ungar, 2018) or personality (Schwartz et al., 2013; PreotÃÂÃÂÃÂÃÂ§iuc-Pietro et al., 2016).",positive
61,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We first query the public Twitter API using the following terms: {parody, #parody, parody account, fake, #fake, fake account, not real} to retrieve candidate parody accounts according to TwitterÃÂÃÂ¢ÃÂÃÂÃÂÃÂs policy. From that set, we exclude any accounts matching fan or commentary in their bio or account name since these are likely to be not posting parodical content. We also exclude private and deactivated accounts and accounts with a majority of non-English tweets.
After collecting this initial set of parody candidates, the authors of the paper manually inspected up to the first ten original tweets from each candidate to identify whether an account is a parody or not following the definition of a public figure parody account from Highfield (2016) (see Section 1), further filtering out non-parody accounts. We keep a single parody account in case of multiple parody accounts about the same person. Finally, for each remaining account, the authors manually identified the corresponding real politician account to collect pairs of real and parody.
Following the process above, we were able to identify parody accounts of 103 unique people, with 81 having a corresponding real account. The authors also identified the binary gender and location (country) of the accounts using publicly available records. This resulted in 21.6% female accounts (women parliamentarians percentages as of 2017: 19% US, 30% UK, 28.8% OECD average).8
6https://www.mpsontwitter.co.uk/ 7https://help.twitter.com/en/rules-an d-policies/parody-account-policy 8https://data.oecd.org/inequality/wom en-in-politics.htm
The majority of the politicians are located in the US (44.5%) followed by the UK (26.7%) while 28.8% are from the rest of the world (e.g. Germany, Canada, India, Russia).",positive
62,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We first analyse the linguistic features specific of real and parody tweets. For this purpose, we use the method introduced in (Schwartz et al., 2013) and used in several other analyses of user traits (PreotÃÂÃÂÃÂÃÂ§iuc-Pietro et al., 2017) or speech acts (PreotÃÂÃÂÃÂÃÂ§iuc-Pietro et al., 2019). We thus rank the feature sets described in Section 4 using univariate Pearson correlation (note that for the analysis we use POS tags instead of POS n-grams). Features are normalized to sum up to unit for each tweet. Then, for each feature, we compute correlations independently between its distribution across posts and the label of the post (parody or not).
Table 8 presents the top unigrams and part-ofspeech features correlated with real and parody tweets. We first note that the top features related to either parody or genuine tweets are function words or related to style, as opposed to the topic. This enforces that the make-up of the data set or any of its categories are not impacted by topic choice and parody detection is mostly a stylistic difference. The only exception are a few hashtags related to parody accounts (e.g. #imwithme), but on a closer inspection, all of these are related to tweets from a single parody account and are thus not useful in prediction by any setup, as tweets containing these",positive
63,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","will only appear in either the train or test set. The top features related to either category of tweets are pronouns (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂourÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for genuine tweets, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for parody tweets). In general, we observe that parody tweets are much more personal and include possessives (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂiÃÂÃÂ¢ÃÂÃÂÃÂÃÂmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, PRP) or second person pronouns (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂyouÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). This indicates that parodies are more personal and direct, which is
also supported by use of more @-mentions and quotation marks. The real politician tweets are more impersonal and the use of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂourÃÂÃÂ¢ÃÂÃÂÃÂÃÂ indicates a desire to include the reader in the conversation.
The real politician tweets include more stopwords (e.g. prepositions, conjunctions, determiners), which indicate that these tweets are more well formed. Conversely, the parody tweets include more contractions (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdonÃÂÃÂ¢ÃÂÃÂÃÂÃÂtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂiÃÂÃÂ¢ÃÂÃÂÃÂÃÂmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), hinting to a less formal style (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdudeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). Politician tweets frequently use their account to promote events they participate in or are relevant to the day-today schedule of a politician, as hinted by several prepositions (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂatÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂonÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and words (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmeetingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtodayÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) (PreotÃÂÃÂÃÂÃÂ§iuc-Pietro and Devlin Marier, 2019). For example, this is a tweet of the U.S. Senator from Connecticut, Chris Murphy:
Rudy Giuliani is in Ukraine today, meeting with Ukranian leaders on behalf of the President of the United States, representing the PresidentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs re-election campaign.[...]
Through part-of-speech patterns, we observe that parody accounts are more likely to use verbs in the present singular (VBZ, VBP). This hints that parody tweets explicitly try to mimic direct quotes from the parodied politician in first person and using present tense verbs, while actual politician tweets are more impersonal. Adverbs (RB) are used predominantly in parodies and a common sequence in parody tweets is adverbs followed by verbs (RB VB) which can be used to emphasize actions or relevant events. For example, the following is a tweet of a parody account (@Queen Europe) of Angela Merkel:
I mean, the Brexit Express literally appears to be going backwards but OK <url>",positive
64,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Finally, we perform an error analysis to examine the behavior of our best performing model (RoBERTa) and identify potential limitations of the current approaches. The first example is a tweet by the former US president Barack Obama which was classified as parody while it is in fact a real tweet:
SummerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs almost over, Senate Leaders. #doyourjob <url>
Similarly, the next tweet was posted by the real account of the Virginia governor, Ralph Northam:
At this point, the list of Virginians Ed Gillespie *hasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt* sold out is shorter than the folks he has. <url>
Both of the tweets above contain humoristic elements and come off as confrontational, aimed at someone else which is more prevalent in parody. We hypothesize that the model picked up this information to classify these tweets as parody. From the previous analyses, we noticed that tweets by real politicians often convey information in a more neutral or impersonal way. On the other hand, the following tweet was posted by a Mitt Romney parody account and was classified as real:
ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs up to you, America: do you want a repeat of the last four years, or four years staggeringly worse than the last four years?
This parody tweet, even though it is more opinionated, is more similar in style to a slogan or campaign speech and is therefore missclassified. Lastly, the following is a tweet from former President Obama that was misclassified as parody:
ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs the #GimmeFive challenge, presidential style. <url>
The reason behind is that there are politicians, such as Barack Obama, who often write in an informal manner and this may cause the models to misclassify this kind of tweets.",positive
65,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","A functioning society is impossible without trust. In online text interactions, users are typically trusting (Shneiderman, 2000), but this trust can be betrayed through false identities on dating sites (Toma and Hancock, 2012), spearphishing attacks (Dhamija et al., 2006), sockpuppetry (Kumar et al., 2017) and, more broadly, disinformation campaigns (Kumar and Shah, 2018). Beyond such one-off antisocial acts directed at strangers, deception can also occur in sustained relationships, where it can be strategically combined with truthfulness to advance a long-term objective (Cornwell and Lundgren, 2001; Kaplar and Gordon, 2004).
We introduce a dataset to study the strategic use of deception in long-lasting relationships. To collect reliable ground truth in this complex scenario, we design an interface for players to naturally generate and annotate conversational data while playing a negotiation-based game called Diplomacy.
Message SenderÃÂÃÂ¢ÃÂÃÂÃÂÃÂs intention ReceiverÃÂÃÂ¢ÃÂÃÂÃÂÃÂs percep.
If I were lying to you, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd smile and say ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthat sounds great.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm honest with you because I sincerely thought of us as partners. Lie Truth
These annotations are done in real-time as the players send and receive messages. While this game setup might not directly translate to real-world situations, it enables computational frameworks for studying deception in a complex social context while avoiding privacy issues.
After providing background on the game of Diplomacy and our intended deception annotations (Section 2), we discuss our study (Section 3). To probe the value of the resulting dataset, we develop lie prediction models (Section 4) and analyze their results (Section 5).",positive
66,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","The Diplomacy board game places a player in the role of one of seven European powers on the eve of World War I. The goal is to conquer a simplified map of Europe by ordering armies in the field against rivals. Victory points determine the success of a player and allow them to build additional armies; the player who can gain and maintain the highest number of points wins.1 The mechanics of the game are simple and deterministic: armies, represented as figures on a given territory, can only move to adjacent spots and the side with the most armies always wins in a disputed move. The game movements become publicly available to all players after the end of a turn.
Because the game is deterministic and everyone begins with an equal amount of armies, a player cannot win the game without forming alliances with other playersÃÂÃÂ¢ÃÂÃÂÃÂÃÂhence the name of the game: Diplomacy. Conquering neighboring territories depends on support from another playerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs armies. After an alliance has outlived its usefulness, a player often dramatically breaks it to take advantage of their erstwhile allyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs vulnerability. Table 1 shows the end of one such relationship. As in real life, to succeed a betrayal must be a surprise to the victim. Thus, players pride themselves on being able to lie and detect lies. Our study uses their skill and passion to build a dataset of deception created by battle-hardened diplomats. Senders annotate whether each message they write is an ACTUAL LIE and recipients annotate whether each message received is a SUSPECTED LIE. Further details on the annotation process are in Section 3.1.",positive
67,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","We need two technical components for our study: a game engine and a chat system. We choose Backstabbr3 as an accessible game engine on desktop and mobile platforms: players input their moves and the site adjudicates game mechanics (Chiodini, 2020). Our communication framework is atypical. Thus, we create a server on Discord,4 the group messaging platform most used for online gaming and by the online Diplomacy community (Coberly, 2019). The app is reliable on both desktop and mobile devices, free, and does not limit access to messages. Instead of direct communication, players communicate with a bot; the bot does not forward messages to the recipient until the player annotates the messages (Figure 2). In addition, the bot scrapes the game state from Backstabbr to sync game and language data.
Annotation of lies is a forced binary choice in our experiment. Explicitly calling a statement a lie is difficult, and people would prefer degrees of deception (Bavelas et al., 1990; Bell and DePaulo, 1996). Thus, we follow previous work that views linguistic deception as binary (Buller et al., 1996; Braun and Van Swol, 2016). Some studies make a more fine-grained distinction; for example, Swol et al. (2012) separate strategic omissions from blatant lies (we consider both deception). However, because we are asking the speakers themselves (and not trained annotators) to make the decision, we follow the advice from crowdsourcing to simplify the task as much as possible (Snow et al., 2008; Sabou et al., 2014). Long messages can contain both truths and lies, and we ask players to categorize these as lies since the truth can be a shroud for their aims.",positive
68,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","Logistic regression models have interpretable coefficients which show linguistic phenomena that correlate with lies. A word that occurs infrequently overall but often in lies, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhonestÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcandidlyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, helps identify which messages are lies.
Niculae et al. (2015) propose linguistic Harbingers that can predict deception. These are word lists that cover topics often used in interpersonal communicationÃÂÃÂ¢ÃÂÃÂÃÂÃÂclaims, subjectivity, premises, contingency, comparisons, expansion,
temporal language associated with the future, and all other temporal language (complete word list in Appendix, Table 8). The Harbingers word lists do not provide full coverage, as they focus on specific rhetorical areas. A logistic regression model with all word types as features further improves F1.
Power dynamics influence the language and flow of conversation (Danescu-Niculescu-Mizil et al., 2012, 2013; Prabhakaran et al., 2013). These dynamics may influence the likeliness of lying; a stronger player may feel empowered to lie to their neighbor. Recall that victory points (Section 2) encode how well a player is doing (more is better). We represent the power differential as the difference between the two players. Peers will have a zero differential, while more powerful players will have a positive differential with their interlocutor. The differential changes throughout the game, so this feature encodes the difference in the season the message was sent. For example, a message sent by an Italy with seven points to a Germany with two points in a given season would have a value of five.",positive
69,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","While less interpretable, neural models are often more accurate than logistic regression ones (Ribeiro et al., 2016; Belinkov and Glass, 2019). We build a standard long short-term memory network (Hochreiter and Schmidhuber, 1997, LSTM) to investigate if word sequencesÃÂÃÂ¢ÃÂÃÂÃÂÃÂignored by logistic regressionÃÂÃÂ¢ÃÂÃÂÃÂÃÂcan reveal lies.
Integrating message context and power dynamics improves on the neural baseline. A Hierarchical LSTM can help focus attention on specific phrases in long conversational contexts. In the same way it would be difficult for a human to determine prima facie if a statement is a lie without previous context, we posit that methods that operate at the level of a single message are limited in the types of cues they
can extract. The hierarchical LSTM is given the context of previous messages when determining if a given message is a lie, which is akin to the labeling task humans do when annotating the data. The model does this by encoding a single message from the tokens, and then running a forward LSTM over all the messages. For each message, it looks at both the content and previous context to decide if the current message is a lie. Fine-tuning BERT (Devlin et al., 2019) embeddings to this model did not lead to notable improvement in F1, likely due to the relative small size of our training data. Last, we incorporate information about power imbalance into this model. This model approaches human performance in terms of F1 score by combining content with conversational context and power imbalance.",positive
70,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","This section examines specific messages where both players and machines are correctly identifying lies and when they make mistakes on our test set. Most messages are correctly predicted by both the model and players (2055 of 2475 messages); but this is because of the veracity effect. The picture is less rosy if we only look at messages the sender
marks as ACTUAL LIE: both players and models are generally wrong (Table 5).
Both models and players can detect lies when liars get into specifics. In Diplomacy, users must agree to help one another through orders that stipulate ÃÂÃÂ¢ÃÂÃÂÃÂÃÂI will help another player move from X to YÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The in-game term for this is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsupportÃÂÃÂ¢ÃÂÃÂÃÂÃÂ; half the messages where players and computers correctly identify lies contain this word, but it rarely occurs in the other quadrants.
Models seem to be better at not falling for vague excuses or fantastical promises in the future. Players miss lies that promise long-term alliances, involve extensive apologies, or attribute motivation as coming from other countriesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ disinformation (Model Correct). Unlike our models, players have access to conversations with other players and accordingly players can detect lies that can easily be verified through conversations with other players (Player Correct).
However, ultimately most lies are believable and fool both models and players (Both Wrong). For example, all messages that contain the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtrueÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are predicted as truthful by both models and play-
ers. Many of these messages are relatively tame;8 confirming the Pinocchio effect found by Swol et al. (2012). If liars can be detected when they wax prolix, perhaps the best way to avoid detection is to be terse and to the point.
Sometimes additional contextual information helps models improve over player predictions. For example, when France tells Austria ÃÂÃÂ¢ÃÂÃÂÃÂÃÂI am worried about a steamroller Russia Turkey allianceÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, the message is incorrectly perceived as truthful by both the player and the single-message model. However, once the model has contextÃÂÃÂ¢ÃÂÃÂÃÂÃÂa preceding question asking if Austria and Turkey were cooperatingÃÂÃÂ¢ÃÂÃÂÃÂÃÂit can detect the lie.
Finally, we investigate categories from the Harbingers (Niculae et al., 2015) word lists. Lies are more likely to contain subjectivity and premises while true messages include expansion phrases (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlaterÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂadditionallyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). We also use specific words in the bag of words logistic regression model. The coefficient weights of words that express sincerity (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsincerelyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfranklyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and apology (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂaccusationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfalloutÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂalternativesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) skew toward ACTUAL LIE prediction in the logistic regression model. More laid back appella-
8Examples include ÃÂÃÂ¢ÃÂÃÂÃÂÃÂItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trueÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[Budapest] back to [Rumania] and [Serbia] on to [Albania] could position for more forward convoys without needing the rear fleet. . . ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂidk if itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs true just letting u know since were alliesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
tions (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdudeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmanÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) skew towards truthfulness, as do words associated with reconnaissance (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfyiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂusefulÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂinformationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and time (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂweekendÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmorningÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). Contested areas on the Diplomacy map, such as Budapest and Sevastopol, are more likely to be associated with lies, while more secure ones like Berlin, are more likely to be associated with truthful messages.",positive
71,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","Early computational deception work focuses on single utterances (Newman et al., 2003), especially for product reviews (Ott et al., 2012). But deception is intrinsically a discursive phenomenon and thus the context in which it appears is essential. Our platform provides an opportunity to observe deception in the context in which it arises: goaloriented conversations around in-game objectives. Gathering data through an interactive game has a cheaper per-lie cost than hiring workers to write deceptive statements (Jurgens and Navigli, 2014).
Other conversational datasets are mostly based on games that involve deception including Werewolf (Girlea et al., 2016), Box of Lies (Soldner et al., 2019), and tailor-made games (Ho et al., 2017). However, these games assign individuals roles that they maintain throughout the game (i.e., in a role that is supposed to deceive or in a role that is deceived). Thus, deception labels are coarse: an individual always lies or always tells the truth. In contrast, our platform better captures a more multifaceted reality about human nature: everyone can lie or be truthful with everyone else, and they use both strategically. Hence, players must think about every player lying at any moment: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂgiven the evidence, do I think this person is lying to me now?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
Deception data with conversational labels is also available through interviews (PÃÂÃÂÃÂÃÂ©rez-Rosas et al.,
2016), some of which allow for finer-grained deception spans (Levitan et al., 2018). Compared with game-sourced data, however, interviews provide shorter conversational context (often only a single exchange with a few follow-ups) and lack a strategic incentiveÃÂÃÂ¢ÃÂÃÂÃÂÃÂindividuals lie because they are instructed to do so, not to strategically accomplish a larger goal. In Diplomacy, users have an intrinsic motivation to lie; they have entertainmentbased and financial motivations to win the game. This leads to higher-quality, creative lies.
Real-world examples of lying include perjury (Louwerse et al., 2010), calumny (Fornaciari and Poesio, 2013), emails from malicious hackers (Dhamija et al., 2006), and surreptitious user recordings. But real-world data comes with realworld complications and privacy concerns. The artifice of Diplomacy allows us to gather pertinent language data with minimal risk and to access both sides of deception: intention and perception. Other avenues for less secure research include analyzing dating profiles for accuracy in self-presentation (Toma and Hancock, 2012) and classifying deceptive online spam (Ott et al., 2011).",positive
72,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","In DanteÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Inferno, the ninth circle of HellÃÂÃÂ¢ÃÂÃÂÃÂÃÂa fate worse even than that reserved for murderersÃÂÃÂ¢ÃÂÃÂÃÂÃÂis for betrayers. Dante asks Count Ugolino to name his betrayer, which leads him to say:
but if my words can be the seed to bear the fruit of infamy for this betrayer who feeds my hunger, then I shall speakÃÂÃÂ¢ÃÂÃÂÃÂÃÂin tears (Alighieri and Musa, 1995, Canto XXXIII)
Similarly, we ask victims to expose their betrayers in the game of Diplomacy. The seeds of playersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ negotiations and deceit could, we hope, yield fruit to help others: understanding multi-party negotiation and protecting Internet users.
While we ignore nuances of the game board to keep our work general, Diplomacy is also a rich, multi-agent strategic environment; Paquette et al. (2019) ignore DiplomacyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs rich language to build bots that only move pieces around the board. An exciting synthesis would incorporate deception and language generation into an agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs policy; our data would help train such agents. Beyond playing against humans, playing with a human in the loop (HITL) resembles designs for cybersecurity threats (Cranor, 2008), annotation (Branson et al., 2010), and language alteration (Wallace et al.,
2019). Likewise, our lie-detection models can help a user in the moment better decide whether they are being deceived (Lai et al., 2020). Computers can meld their attention to detail and nigh infinite memory to humansÃÂÃÂ¢ÃÂÃÂÃÂÃÂ grasp of social interactions and nuance to forge a more discerning player.
Beyond a silly board game, humans often need help verifying claims are true when evaluating health information (Xie and Bugg, 2009), knowing when to take an e-mail at face value (Jagatic et al., 2007), or evaluating breaking news (Hassan et al., 2017). Building systems to help information consumers become more discerning and suspicious in low-stakes settings like online Diplomacy are the seeds that will bear the fruits of interfaces and machine learning tools necessary for a safer and more robust Internet ecosystem.",positive
73,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2713ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2722 July 5 - 10, 2020. cÃÂÃÂÃÂÃÂ©2020 Association for Computational Linguistics
2713
Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/ KGEmbedding-OTE.",positive
74,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","We consider knowledge graph as a collection of triples D = {(h, r, t)} with V as the graph node set, and R as the graph edge set. Each triple has a head entity h and tail entity t, where h, t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ V . Relation r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R connects two entities with direction from head to tail. As discussed in the introduction section, 1-to-N, N-to-1 and N-to-N relation prediction (Bordes et al., 2013; Wang et al., 2014) are difficult to deal with. They are addressed in our proposed approach by: 1) orthogonal relation transforms that operate on groups of embedding space. Each group is modeled and scored independently, and the final score is the sum of all group scores. Hence, each group could address different aspects of entity-relation pair and alleviate the 1-to-N and N-to-N relation mapping issues; and 2) directed graph context to integrate knowledge graph structure information to reduce the ambiguity.
Next, we first briefly review RotatE that motivates our orthogonal transform embedding (OTE), and then describe the proposed method in details.",positive
75,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","OTE is inspired by RotatE (Sun et al., 2019). In RotatE, the distance scoring is done via Hadamard production (element-wise) defined on the complex domain. Given a triple (h, r, t), the corresponding embedding are eh, ÃÂÃÂÃÂÃÂ¸r, et, where eh and et ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R
2d, ÃÂÃÂÃÂÃÂ¸r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R
d, and d is the embedding dimension. For each dimension i, e[2i] and e[2i + 1] are corresponding real and imaginary components. The projection eÃÂÃÂÃÂÃÂt of t from corresponding relation and head
entities is conducted as an orthogonal transform as below:
[ eÃÂÃÂÃÂÃÂt[2i] eÃÂÃÂÃÂÃÂt[2i+1] ] =Mr(i) [ eh[2i] eh[2i+1] ]
= [cos ÃÂÃÂÃÂÃÂ¸r(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ sin ÃÂÃÂÃÂÃÂ¸r(i) sin ÃÂÃÂÃÂÃÂ¸r(i) cos ÃÂÃÂÃÂÃÂ¸r(i) ] [ eh[2i] eh[2i+1] ]
where Mr(i) is a 2D orthogonal matrix derived from ÃÂÃÂÃÂÃÂ¸r .
Though RotatE is simple and effective for knowledge graph link prediction, it is defined in 2D complex domain and thus has limited modeling capability. A natural extension is to apply similar operation on a higher dimensional space.",positive
76,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","Speech directed to children (CDS) differs from adult-directed speech (ADS) in many aspects. Linguistic differences include the number of words per utterance, with utterances in CDS being considerably shorter than utterances in ADS, and repetition, which is more common in child-directed speech. There are also paralinguistic, acoustic factors that characterize child-directed speech: people speaking to children typically use a higher pitch and exaggerated intonation.
It has been argued that the properties of CDS help perception or comprehension. Kuhl et al. (1997) propose that CDS is optimized for learnability. Optimal learnability may, but does not necessarily align with optimization for perception or comprehension. Although speech with lower variability may be easiest to learn to understand,
higher variability may provide more learning opportunities, leading to more complete language knowledge.
In this paper, we explore how learning to extract meaning from speech differs when learning from CDS and ADS. We discuss task performance on the training register as well as generalization across registers. To tease apart the effect of acoustic and linguistic differences, we also report on models trained on synthesized speech, in which linguistic differences between the registers are retained, but the acoustic properties are similar.",positive
77,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","The characteristics of child-directed speech are a major topic of study in language acquisition research. For a comprehensive overview, see Soderstrom (2007) and Clark (2009, Ch. 2, p. 32-41). With regards to acoustics, CDS is reported to have exaggerated intonation and a slower speech rate (Fernald et al., 1989). Kuhl et al. (1997) show that CDS contains more ÃÂÃÂ¢ÃÂÃÂÃÂÃÂextremeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ realizations of vowels. McMurray et al. (2013) show that these increased means are accompanied by increased variance, and argue that any learning advantage of CDS due to extreme vowel realizations is counteracted by increased variance. However, it has also been argued that increased variance may be beneficial to learning in the long run, as it gives the learner a more complete set of examples for a category, which helps generalization. GuevaraRukoz et al. (2018) show that word forms in childdirected speech are acoustically more diverse. At the utterance level, child-directed language consists of shorter sentences and simpler syntax (Newport et al., 1977; Fernald et al., 1989), and words more often appear in isolation (Ratner and Rooney, 2001).
2 Studies on home recordings show that the availability of CDS input accounts for differences in vocabulary growth between learners, whereas overheard speech is unrelated (Hoff, 2003; Weisleder and Fernald, 2013). This does not necessarily mean that it is easier to learn from CDS. Psycholinguistic research has shown that infants across the world show a CDS preference, paying more attention to it than to ADS (ManyBabies Consortium, 2020). Learning advantages of CDS in children may therefore simply be because they grant it more attention, rather than to properties of CDS that are advantageous for learning. Computational models, however, have no choice in where they allocate attention. Any learning advantages we find of either ADS or CDS in computational studies must be due to properties that make speech in that register more learnable to the model. There has been some computational work comparing learning from ADS and CDS at the level of word learning and phonetic learning. Studies on segmentability use algorithms that learn to identify word units, with some studies reporting higher segmentability for CDS (Batchelder, 2002; Daland and Pierrehumbert, 2011), while Cristia et al. (2019) report mixed results. Kirchhoff and Schimmel (2005) train HMM-based speech recognition systems on CDS and ADS, and test on matched and crossed test sets. They find that both ADS and CDS trained systems perform best on the matching test set, but CDS trained systems perform better on ADS than systems trained on ADS peform on CDS. They show that this is likely caused by phonetic classes have larger overlaps in CDS. To the authorsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ knowledge, the current work is the first to computationally explore learnability differences between ADS and CDS considering the process of speech comprehension as a whole: from audio to semantic information.",positive
78,From Arguments to Key Points: Towards Automatic Argument Summarization,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","Governments, businesses and individuals, all need to make decisions on a daily basis: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂShould cannabis be legalized?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂShould we develop this product?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂShould I become a vegetarian?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. When making an important decision, the process typically comprises several steps: first, we gather as much information as we can about the pros and cons of the proposal under consideration. We may then summarize the collected information as a short list of the main arguments for each side. Lastly, we aim to weigh the pro and con arguments against each other to make the final decision.
Where can we find relevant arguments for a given topic? In recent years, significant progress was made in the field of argument mining, automatic identification and extraction of argumentative structures in text (Lawrence and Reed, 2020). Specifically, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al.,
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAll authors equally contributed to this work.
2017; Stab et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal.
Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as similarity (Reimers et al., 2019), frames (Ajjour et al., 2019), and argument facets (Misra et al., 2016). These works, however, did not attempt to create a concise textual summary from the resulting clusters.
In this work we propose to summarize the arguments supporting each side of the debate by mapping them to a short list of talking points, termed key points. The salience of each key point can be represented by the number of its matching arguments. An example for such summary is shown in Table 1. Key points may be viewed as highlevel arguments. They should be general enough to match a significant portion of the arguments, yet informative enough to make a useful summary.
The proposed method raises a fundamental question: can a small number of key points effectively summarize massive amount of arguments collected from a large population? In this work we give a positive answer to this question, based on extensive analysis over 28 controversial topics and 7,000 crowd-contributed pro and con arguments for these topics. Furthermore, we found that, given a controversial topic, a domain expert can compose a short, comprehensive list of key points even without looking at the arguments themselves.
Motivated by the above findings, we assume in this work that the key points for each topic are given, and focus on the task of automatically map-
ping arguments to these key points. This setting may be viewed as an intermediate step towards fully automatic argument summarization, but also as a valuable setting by itself: argument-to-key point mapping allows measuring the distribution of key points in a massive collection of arguments. It also allows interactive exploration of large argument collections, where key points serve as queries for retrieving matching arguments. In addition, it can be used for novelty detection - identifying unexpected arguments that do not match presupposed key points.
We develop the ArgKP dataset for the argumentto-keypoint mapping task, comprising about 24,000 (argument, key point) pairs labeled as matching/non matching.1 To the best of our knowledge, this is the first dataset for this task. As discussed in the next section in more detail, our dataset is also much larger and far more comprehensive than datasets developed for related tasks such as mapping posts or comments in online debates to reasons or arguments (Hasan and Ng, 2014; BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder, 2014).
We report empirical results for an extensive set of supervised and unsupervised configurations, achieving promising results.
The main contributions of this work are:
1. We demonstrate, through extensive data annotation and analysis over a variety of topics, the feasibility and effectiveness of summarizing a large set of arguments collected from a large audience by mapping them to a small set of key points.
1The dataset is available at https://www.research. ibm.com/haifa/dept/vst/debating_data. shtml
2. We develop the first large-scale dataset for the task of argument-to-key point mapping.
3. We perform empirical evaluation and analysis of a variety of classification methods for the above task.",positive
79,From Arguments to Key Points: Towards Automatic Argument Summarization,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","Several works have focused on identifying pairs of similar arguments, or clustering similar arguments together. Ajjour et al. (2019) addressed the task of splitting a set of arguments into a set of nonoverlapping frames such as Economics, Environment and Politics. Reimers et al. (2019) classified argument pairs as similar/dissimilar. Misra et al. (2016) aimed to detect argument pairs that are assumed to share the same argument facet, which is similar to our notion of key points. However, they did not attempt to explicitly identify or generate these facets, which remained implicit, but rather focused on detecting similarity between argument pairs. In contrast to these works, we directly map arguments to key points.
Egan et al. (2016) proposed to summarize argumentative discussions through the extraction of salient ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpointsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, where each point is a verb and its syntactic arguments. Applying their unsupervised method to online political debates showed significant improvement over a baseline extractive summarizer, according to human evaluation. While the current work also aims to summarize argumentative content via concise points, our goal is not to extract these points but to accurately map arguments to given points. Our main challenge is to identify the various ways in which the meaning of a point is conveyed in different arguments. The method employed by Egan et al. only matches arguments with the same signature - the same verb, subject and object dependency nodes, hence its ability to capture such variability is limited.
The line of work that seems most similar to ours is of Hasan and Ng (2014), BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder (2014) and Naderi (2016). Hasan and Ng classified posts and individual sentences from online debates into a closed set of reasons, composed manually for each topic. BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder mapped comments from one debating website (ProCon.org) to arguments taken from another debating website (iDebate.org). Naderi (2016) addressed a similar task: she used part of the BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder corpus as training data for an SVM classifier, which was then tested on sentences and paragraphs from same-sex marriage debates in the Canadian Parliament, annotated with the same set of arguments.
Our work differs from these works in several respects. First, we deal with crowd-contributed arguments, taken from the dataset of Gretz et al. (2020)
while these works dealt with posts or comments in debate forums, and parliamentary debates. Second, the dataset developed in this work is far more extensive, covering 28 topics and over 6,500 arguments2, as compared to 2-4 topics in the datasets of BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder and Hasan and Ng, respectively. This allows us to perform a comprehensive analysis on the feasibility and effectiveness of argument-to-key point mapping over a variety of topics, which has not been possible with previous datasets. Lastly, while Hasan and Ng only perform within-topic classification, where the classifier is trained and tested on the same topic, we address the far more challenging task of cross-topic classification. BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder experimented with both within-topic and cross-topic classification, however they used a limited amount of data for training and testing: two topics, with less than 200 comments per topic.
Finally, we point out the similarity between the argument/key point relation and the text/hypothesis relation in textual entailment, also known as natural language inference (NLI) (Dagan et al., 2013). Indeed, BoltuzÃÂÃÂÃÂÃÂicÃÂÃÂÃÂÃÂ and SÃÂÃÂÃÂÃÂnajder (2014) used textual entailment as part of their experiments, following the earlier work of Cabrio and Villata (2013), who used textual entailment to detect support/attack relations between arguments.",positive
80,From Arguments to Key Points: Towards Automatic Argument Summarization,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","Next, we consolidate the individual annotations as follows. We say that an argument a is mapped to a key point k if at least 60% of the annotators mapped a to k. Recall that an argument can be mapped to more than one key point. Similarly, we say that a has no key point if at least 60% of the annotators mapped a to None (which is equivalent to not selecting any key point for the argument). Otherwise, we say that a is ambiguous, i.e., the annotations were indecisive. Table 2 shows examples for arguments and their matching key points in our dataset.
The distribution of the arguments in the dataset over the above categories is shown in Table 3. Remarkably, our key points, composed independently of the arguments, were able to cover 72.5% of them, with 5% of the arguments mapped to more than one key point.
We further investigated the differences between arguments in each category, by comparing their average quality score (taken from the IBM-Rank30k dataset), number of tokens and number of sentences. The results are shown as additional columns in Table 3. Interestingly, arguments that have no key point tend to be shorter and have lower quality score, comparing to arguments mapped to a single key point; arguments mapped to more than one key point are the longest and have the highest quality.
Figure 1 examines the impact of the number of key points on argument coverage. For each topic and stance, we order the key points according to the number of their matched arguments, and add them incrementally. The results indicate that arguments are not trivially mapped to only one or two key points, but a combination of several key points is required to achieve high coverage. The marginal contribution decays for the sixth and seventh key points, suggesting that seven key points indeed suffice for this task.
22.8% of the arguments are ambiguous. Annotations for these arguments are split over several possible key points, none reaching the 60% threshold. For instance, the argument ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhomeschooling
enables parents with fringe views to push their agenda on their children without allowing exposure to alternative viewpoints.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, had two key points with annotator votes higher than 40%, but below 60%:
1. Homeschools cannot be regulated / standardized.
2. Parents are not qualified as teachers.
Such cases suggest that many arguments are somewhat covered by the key points, but if the judgment is not clear-cut, the different intuitions of the annotators may result in no label receiving the required majority.",positive
81,From Arguments to Key Points: Towards Automatic Argument Summarization,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","This work addressed the practical problem of summarizing a large collection of arguments on a given topic. We proposed to represent such summaries as a set of key points scored according to their relative salience. Such summary aims to provide both textual and quantitative views of the argument data in a concise form. We demonstrated the feasibility and effectiveness of the proposed approach through extensive data annotation and analysis. We showed that a domain expert can quickly come up with a short list of pro and con key points per topic, that would capture the gist of crowd-contributed arguments, even without being exposed to the arguments themselves. We studied the problem of automatically matching arguments to key points, and developed the first large-scale dataset for this task, which we make publicly available.
Our experimental results demonstrate that the
problem is far from trivial, and cannot be effectively solved using unsupervised methods based on word or sentence-level embedding. However, by using state of the art supervised learning methods for match scoring, together with an appropriate key point selection policy for match classification, we were able to achieve promising results on this task.
The natural next step for this work is the challenging task of automatic key point generation. In addition, we plan to apply the methods presented in this work also to automatically-mined arguments. Finally, detecting the more implicit relations between the argument and the key point, as seen in our error analysis, is another intriguing direction for future work.",positive
82,Injecting Numerical Reasoning Skills into Language Models,"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GENBERT, on this data, dramatically improves performance on DROP (49.3 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GENBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","Numerical reasoning over text (NRoT) is commonly set up as a reading comprehension (RC) task. Given a training set of question-context-answer triples {(qi, ci, ai)}Ni=1, the goal is to learn a function that returns the answer a to a question q given a context c. However, in NRoT the answer generally requires to internally perform some numerical computation using the entities and numbers in the context. Specifically, the answer is either: (a) a span (or list of spans) from the context c or question q, or (b) a number that is the result of some computation (see examples in Table 1).
Two natural, yet opposing, approaches lend themselves to tackling NRoT: (a) A symbolic approach: a model can read the question and context, output a numerical expression and evaluate the answer with an external symbolic calculator. This approach is a particular case of semantic parsing (Kamath and Das, 2019), and was common in early NRoT datasets (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Hosseini et al., 2014). How-
ever, it suffers from several drawbacks. First, because numerical expressions are discrete and their space grows combinatorially, the model must learn to search in this space using non-differentiable operations, which are usually difficult to optimize. Second, numerical expressions are limited to numerical answers, while in DROP often a numerical computation is required but the final answer is a text span. (b) A distributed approach: have a model directly generate the answer given (q, c). When the answer is a text span, the model can extract it from the input, and when the answer is a number that is not in q or c, the model must generate it. While this makes training straightforward, the model must learn to perform numerical computations from the relatively small target dataset. We empirically show in ÃÂÃÂÃÂÃÂ§3 that this leads to low performance in general.
As a compromise, most NRoT models (Dua et al., 2019; Kinley and Lin, 2019; Hu et al., 2019; Efrat et al., 2019) have taken a hybrid approach: they augment standard extractive QA models with specialized modules for handling a limited set of numerical computations. We briefly describe this architecture, as it is the basis for our model in ÃÂÃÂÃÂÃÂ§3.
Given a question with n1 tokens q = (q1, . . . , qn1) and a context with n2 tokens c = (c1, . . . , cn2), the hybrid model first computes contextualized representations for the n1 + n2 + 3 tokens ÃÂÃÂ£ÃÂÃÂÃÂÃÂ[CLS] q [SEP] c[SEP]ÃÂÃÂ£ÃÂÃÂÃÂÃÂ using a pretrained LM, such as BERT (Devlin et al., 2019):
L = LM(q, c).
The representations L are then passed to multiple heads, which are small neural networks that estimate p(a | q, c, h), that is, the probability of the answer given the input and conditioned on a head h, corresponding to a particular answer type: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Context span head: computes a distribution over
all spans in the context using a feed-forward network (FFN) FFc(L). ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Question span head: computes a distribution
over spans in the question using a FFN FFq(L). ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Count head: computes a distribution over the
numbers {0, . . . , 9} using a FFN FFcnt(L). ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Arithmetic head: computes a distribution over
all signed combinations of numbers in the context using a FFN FFcmb(L) (the numbers in the context are identified in a pre-processing step). While the first two heads are standard in extractive QA, the latter two heads are specialized and meant to handle answers that do not appear in the input.
Finally, for deciding which answer head to use for a given input, a type head FFtyp(L) outputs a probability distribution phead(h | q, c) (using a FFN). Thus the model probability for an answer is
p(a | q, c) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
hÃÂÃÂ¢ÃÂÃÂÃÂÃÂheads phead(h | c,q) ÃÂÃÂÃÂÃÂ·p(a | c,q, h).
Training is done by enumerating all of the ways in which the answer can be obtained using all of the heads, and maximizing this marginal probability.
While existing models perform well on DROP, the aforementioned architecture is not flexible. First, the output space is severely constrained ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the model can only count up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ9ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and numerical computations are restricted to signed combinations of a few numbers. Second, expanding the space of supported numerical computations is non-trivial, because training involves marginalizing over all expressions that lead to the correct answer. Since the space of numerical expressions grows exponentially, expanding this space quickly leads to a difficult search problem. Third, delegating numerical computations to an external symbolic calculator leads to modeling challenges, since there could be interactions between text and numerical computation: Consider the DROP question ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHow many total yards did Phil Dawson throw for touchdowns?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Current models handle such questions by computing a sum from numbers in the text and returning the result. However, if the question was ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWho threw 45 total yards for touchdowns?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, the model would have to compute the sum internally, and then find the relevant span in the text. This is impossible when the computation itself is delegated to an external calculator. Thus, training models to handle such numerical questions is desirable.
Motivated by the above arguments, we wish to push the frontier of end-to-end differentiable models for numerical reasoning. Thus, we will automatically generate large amounts of data that endow a pre-trained LM with numerical skills.",positive
83,Injecting Numerical Reasoning Skills into Language Models,"Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GENBERT, on this data, dramatically improves performance on DROP (49.3 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GENBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.","Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limited flexibility. In this work, we propose a general method for injecting additional skills into LMs, assuming automatic data generation is possible. We apply our approach to the task of numerical reasoning over text, using a general-purpose model called GENBERT, and a simple framework for generating large amounts of synthetic examples. Our experiments demonstrate the effectiveness of our method, showing that GENBERT successfully learns the numerical skills, and performs on par with state-ofthe-art NRoT models of the same size.",positive
84,Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences,"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a groundtruth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-byword translation, dictionary-supervised crosslingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 .","Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Corresponding Author. 1Code is available at https://github.com/
mttravel/Dictionary-based-MT
supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a).
The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system (Artetxe et al., 2018c,b; Lample et al., 2018a), we use the ground-truth bilingual dictionary and apply it throughout the training process.
We propose Anchored Training (AT) to tackle this task. Since word representations are learned over monolingual corpora without any parallel sentence supervision, the representation distances between source language and target language are often quite large, leading to significant translation difficulty. As one solution, AT selects words covered by the bilingual dictionary as anchoring points to drive the distance between the source language space and the target language space closer so that translation between the two languages becomes easier. Furthermore, we propose Bi-view AT that places anchors based on either source language view or target language view, and combines both views to enhance the translation quality.
Experiments on various language pairs show that AT performs significantly better than various baselines, including word-by-word translation through looking up the dictionary, unsupervised MT, and dictionary-supervised cross-lingual word embed-
ding transformation to make distances between both languages closer. Bi-view AT further improves AT performance due to mutual strengthening of both views of the monolingual data. When combined with cross-lingual pretraining (Lample and Conneau, 2019), Bi-view AT achieves performances comparable to traditional SMT systems trained on more than 4M parallel sentences. The main contributions of this paper are as follows:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A novel MT task is proposed which can only use the ground-truth bilingual dictionary and monolingual corpora, while is independent on parallel sentences.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ AT is proposed as a solution to the task. AT uses the bilingual dictionary to place anchors that can encourage monolingual spaces of both languages to become closer so that translation becomes easier.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ The detailed evaluation on various language pairs shows that AT, especially Bi-view AT, performs significantly better than various methods, including word-by-word translation, unsupervised MT, and cross-lingual embedding transformation. On distant language pairs that unsupervised MT struggled to be effective, AT and Bi-view AT perform remarkably better.",positive
85,Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences,"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a groundtruth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-byword translation, dictionary-supervised crosslingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 .","The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences.
Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample
et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary.
Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and SÃÂÃÂÃÂÃÂ¸gaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017).",positive
86,Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences,"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a groundtruth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-byword translation, dictionary-supervised crosslingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 .","Since word embeddings are trained on monolingual corpora independently, the embedding spaces of both languages are quite different, leading to significant translation difficulty. AT forces words of a translation pair to share the same word embedding as an anchor. We place multiple anchors by
2https://github.com/facebookresearch/MUSE 3https://en.wiktionary.org/wiki/Wiktionary:Main_Page 4https://panlex.org/
selecting words covered by the bilingual dictionary. With stable anchors, the embedding spaces of both languages become more and more close during the AT process.
As illustrated in Figure 1 (a), given the source sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs1s2s3s4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with words of s2 and s3 being covered by the bilingual dictionary, we replace the two words with their translation words according to the dictionary. This results in the source sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs1 s2.t s3.t s4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, of which s2.t and s3.t serve as the anchors which are actually the target language words obtained by translating s2 and s3 according to the dictionary, respectively. Through the anchors, some words on the source side share the same word embeddings with the corresponding words on the target side. The AT process will strengthen the consistency of embedding spaces of both languages based on these anchors.
The training process illustrated in Figure 1 (a) consists of a mutual back-translation procedure. The anchored source sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs1 s2.t s3.t s4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is translated into target sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂt1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² t2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² t3ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by using source-to-target decoding, then ÃÂÃÂ¢ÃÂÃÂÃÂÃÂt1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² t2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² t3ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs1 s2.t s3.t s4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ constitute a sentence pair for training the target-to-source translation model. In contrast, the target sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂt1t2t3t4t5ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is translated into anchored source sentence ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² s2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² s3.tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² s4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by using target-to-source decoding, then both sen-
tences constitute a sentence pair for training the source-to-target translation model. Note that during training the translation model, the input sentences are always pseudo sentences generated by decoding an MT model, while the output sentences are always true or anchored true sentences. Beside this mutual back-translation procedure, a denoising procedure used in unsupervised MT (Lample et al., 2018b) is also adopted. The deletion and permutation noises are added to the source/target sentence, and the translation model is also trained to denoise them into the original source/target sentence.
During testing, a source sentence is transformed into an anchored sentence at first by looking up the bilingual dictionary. Then we use the source-totarget model trained in the AT process to decode the anchored sentence.
We use Transformer architecture (Vaswani et al., 2017) as our translation model with four stacked layers in both encoder and decoder. In the encoder, we force the last three layers shared by both languages, and leave the first layer not shared. In the decoder, we force the first three layers shared by both languages, and leave the last layer not shared. Such architecture is designed to capture both common and specific characteristics of the two languages in one model for the training.",positive
87,Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences,"In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a groundtruth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-byword translation, dictionary-supervised crosslingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 .","We analyze the cross-lingual property of our approaches in both word level and sentence level. We also compare the performances between the ground-truth dictionary and the artificial dictionary. In the end, we vary the size of the bilingual dictionary and report its impact on the AT training.
9http://www.statmt.org/moses/. We use the default setting of Moses.
Effect on Bilingual Word Embeddings
As shown in Figure 2, we depict the word embeddings of some sampled words in English-Chinese after our Bi-view AT. The dimensions of the embedding vectors are reduced to two by using T-SNE and are visualized by the visualization tool in Tensorflow10.
We sample the English words that are not covered by the dictionary at first, then search their nearest Chinese neighbors in the embedding space. It shows that the words which constitute a new ground-truth translation pair do appear as neighboring points in the 2-dimensional visualization of Figure 2.
Precision of New Word Pairs
We go on with studying bilingual word embedding by quantitative analysis of the new word pairs, which are detected by searching bilingual words that are neighbors in the word embedding space, and evaluate them using the ground-truth bilingual dictionary. In particular, we split the Muse dictionary of Chinese-to-English into standard training set and test set as in BLI (Artetxe et al., 2018a). The training set is used for the dictionary-based systems, including our AT/Biview AT, UNMT+SWET, and Muse, which is a BLI toolkit. The test set is used to evaluate these systems by computing the precision of discovered translation words given the source words in the test set. The neighborhood is computed by CSLS distance (Conneau et al., 2018).
Table 3 shows the precision, where precision@k indicates the accuracy of top-k predicted candidate. Muse induces new word pairs through either the supervised way or the unsupervised way. MuseSupervised is better than MuseUnsupervised since it is supervised by the ground-truth bilingual dictionary. Our AT/Bi-view AT surpasses MuseSupervised by a large margin. UNMT+SWET/UWET also obtains good performance through the word embedding transformation. Bi-view AT significantly surpasses UNMT+SWET/UWET in precision@5 and precision@10, while is worse than them in precision@1. This indicates that Bi-view AT can produce better n-best translation words that are beneficial for NMT beam decoding to find better translations.
Through the word level analysis, we can see that AT/Bi-view AT leads to more consistent word em-
10https://projector.tensorflow.org/
bedding space shared by both languages, making the translation between both languages easier.
Sentence Level Similarity of Parallel Sentences
We check the sentence level representational invariance across languages for the cross-lingual pretraining methods. In detail, following Arivazhagan et al. (2018), we adopt max-pooling operation to collect the sentence representation of each encoder layer for all Chinese-to-English sentence pairs in the test set. Then we calculate the cosine similarity for each sentence pair and average all cosine scores.
Figure 3 shows the sentence level cosine similarity. ACP+Bi-view AT consistently has a higher similarity for parallel sentences than XLM+UNMT on all encoder layers. When compare Bi-view AT and AT, the Bi-view AT is better on more encoder layers.
We can see that in both word level and sentence level analysis, our AT methods achieve better crosslingual invariance, significantly reduce the gap between the source language space and the target language space, leading to decreased translation difficulty between both languages.
Ground-Truth Dictionary Vs Artificial Dictionary
Table 4 presents the comparison in EnglishChinese. The ground-truth dictionary is from the Muse dictionary deposit, and the artificial dictio-
nary is generated by unsupervised BLI (Conneau et al., 2018). We extract top-n word pairs as the artificial dictionary, where n is the same as the number of entries in the ground-truth dictionary.
Both dictionaries use AT methods for translation. As shown in Table 4, the ground-truth dictionary performs significantly better than the artificial dictionary in both methods and both translation directions.
The Effect of The Dictionary Size
We randomly select a portion of the ground-truth bilingual dictionary to study the effect of the dictionary size on the performance. Table 5 reports the performances of ACP+AT using a quarter or a half of the zhÃÂÃÂ¢ÃÂÃÂÃÂÃÂen dictionary.
It shows that, in comparison to the baseline of XLM+UNMT that does not use a dictionary, a quarter of the dictionary consisting of around 3k word pairs is capable of improving the performance significantly. More word pairs in the dictionary lead to better translation results, suggesting that expanding the size of the current Muse dictionary via collecting various dictionaries built by human experts may improve the translation performance further.",positive
88,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","The transformer layer (Vaswani et al., 2017) is currently the primary modeling component in natural language processing, playing a lead role in recent innovations such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). Each transformer layer consists of a self-attention sublayer (s) followed by a feedforward sublayer (f), creating an interleaving pattern of self-attention and feedforward sublayers (sfsfsf ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ) throughout a multilayer transformer model. To the best of our knowledge, there is no reason to expect this particular pattern to be optimal. We conduct a series of explorations to obtain insights about the nature of transformer orderings that work well, and based on this, we
1Our code is available at https://github.com/ ofirpress/sandwich_transformer
design a new transformer ordering pattern that improves upon the baseline.
First, we generate random transformer models, varying the number of each type of sublayer, and their ordering, while keeping the number of parameters constant. We train these models on the standard WikiText-103 word-level language modeling benchmark (Merity et al., 2016), and observe that some of these random models outperform the original interleaved transformer model, even when the number of self-attention and feedforward layers is not equal. Our analysis shows that models with more self-attention toward the bottom and more feedforward sublayers toward the top tend to perform better in general.
Based on this insight, we design a new family of transformer models that follow a distinct sublayer ordering pattern: sandwich transformers (Figure 1). Our experiments demonstrate that a sandwich transformer outperforms the baseline of Baevski and Auli (2019). This result is made more interesting by the fact that our sandwich transformer is simply a reordering of the sublayers in the baseline model, and does not require more parameters, memory, or training time.
Finally, we demonstrate that even though the
attention (s) sublayers and 16 feedforward (f) sublayers, and their perplexity on the WikiText-103 development set. The baselines (the standard transformer trained with different random seeds) are in bold.
sandwich transformer is motivated by random search experiments on WikiText-103, it can improve performance on additional domains and tasks. Sandwich transformers achieve state-of-the-art results on the enwik8 character-level language modeling dataset and on an additional word-level corpus, but have no significant effect on machine translation. We conjecture that tuning transformer reorderings to specific tasks could yield even larger gains, and that further exploration of the ordering space may provide universally beneficial patterns.",positive
89,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","Each transformer layer consists of a self-attention sublayer followed by a feedforward sublayer, modifying a sequence of vectors X0 as follows:2
X1 = self-attention(X0) +X0 X2 = feedforward(X1) +X1
Stacking multiple transformer layers creates an interleaved network of sublayers. We denote these
2We omit dropout (Srivastava et al., 2014) and layer normalization (Ba et al., 2016) to simplify the notation.
models as strings, with s and f representing selfattention and feedforward sublayers, respectively. A three-layer transformer network, for example, would be denoted sfsfsf, with the flow of computation moving from input on the left to output on the right. Thus, any string in the regular language (s|f)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ defines a valid network that uses the same building blocks as the original transformer. For simplicity, we refer to these alternatives as transformers as well.",positive
90,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","We conduct a series of experiments to understand which transformer networks work well and whether particular architectural patterns can improve performance. First, we generate random transformer models while keeping the number of parameters constant. We then train these random models to determine whether the interleaving pattern (sfsfsf ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ) is optimal (Section 3.1), and whether balancing the number of self-attention and feedforward sublayers is desirable (Section 3.2). Finally, we analyze additional properties of these random models, and find that those with more selfattention at the beginning and more feedforward sublayers near the end tend to outperform the standard interleaved model (Section 3.3).
Experimental Setup Our baseline is the strong transformer language model of Baevski and Auli (2019), trained on WikiText-103 (Merity et al., 2016). WikiText-103 contains roughly 103 million tokens from English Wikipedia, split into train, development, and test sets by article. The Baevski
and Auli model contains 16 transformer layers of d = 1024 dimensions, with 16 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4096. In this setting, each self-attention sublayer contains 4d2 parameters, while each feedforward sublayer contains 8d2 parameters (excluding bias terms, which have a marginal contribution). Thus, each f sublayer contains twice the parameters of a s sublayer, following the parameter ratio between self-attention and feedforward sublayers described in Vaswani et al. (2017).
All of our experiments use the same hyperparameters as Baevski and AuliÃÂÃÂ¢ÃÂÃÂÃÂÃÂs original model. To set an accurate baseline, we train the baseline model (the standard interleaved transformer) with five different random seeds, achieving 18.65 ÃÂÃÂÃÂÃÂ± 0.24 perplexity on the development set.",positive
91,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","Our analysis in the previous section motivates designing a transformer model that is heavy on selfattention at the bottom and feedforward sublayers at the top, while at the same time containing a more-or-less balanced amount of both sublayer types. As a first attempt to manually design a better transformer, we take this hypothesis to the extreme, and train a transformer model of 16 self-attention sublayers followed by 16 feedforward sublayers (s16f16). This model achieves 18.82 perplexity, which is comparable to the performance of the baseline with the same number of parameters.
We next generalize this model and the original interleaved transformer, creating the family of sandwich transformers. A sandwichnk transformer consists of 2n sublayers in total (n of each type), conforming to the regular expression sk(sf)nÃÂÃÂ¢ÃÂÃÂÃÂÃÂk fk. The first k sublayers are purely self-attention (s), while the last k are feedforward sublayers (f). In between, we use the original interleaving pattern (sf) to fill the remaining 2(nÃÂÃÂ¢ÃÂÃÂÃÂÃÂk) sublayers. When k = 0, we get the original transformer model, and when k = n ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 (its maximal value) we get the previously mentioned snfn model. We refer to k as the transformerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs sandwich coefficient.
We train sandwich transformers for n = 16 (to remain within the same parameter budget as our baseline language model) and all values of k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {0, . . . , 15}. Figure 5 shows the transformerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance as a function of the sandwich coefficient k. With the exception of k = 14, 15, all sandwich transformers achieve lower perplexities
than the average baseline transformer. Of those, 6 models outperform the best baseline transformer (k = 5, 6, 8, 9, 10, 11). The best performance of 17.84 perplexity is obtained when k = 6. We compare this model to the baseline on WikiText-103ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs test set.
Table 3 shows that, despite its simple design, the sandwich transformer outperforms the original transformer baseline by roughly double the gap between the baseline (Baevski and Auli, 2019) and Transformer XL (Dai et al., 2019). This improvement comes at no extra cost in parameters, data, memory, or computation; we did not even change any of the original hyperparameters, including the number of training epochs.
To check whether this advantage is consistent, we train 4 more sandwich166 models with different random seeds (5 in total) and evaluate them on the development set, to avoid evaluating our model more than once on the test set. This is the only experiment in which we modify our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs random seed. Figure 6 shows that we obtain a mean perplexity value of 17.98 with a standard deviation of 0.10, while the baseline achieves 18.65 mean perplexity, with a larger standard deviation of 0.34 (these values reflect development set performance, not test set performance as in Table 3).
In very recent work, kNN-LM (Khandelwal et al., 2019) set a new state of the art on WikiText103, surpassing other recent models by a wide margin. The model achieves this result by storing the entire training set in an auxiliary memory component. Since this approach appears orthogonal to ours, it is quite possible that kNN-LM could benefit from sublayer reordering as well.",positive
92,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","We first apply sandwich transformers to a different domain, while retaining the other architectural aspects and hyperparameter settings from Baevski and Auli (2019). Specifically, we use the Toronto Books Corpus (Zhu et al., 2015), which has previously been used to train GPT (Radford et al., 2018) and also BERT (Devlin et al., 2019) (combined with Wikipedia). The corpus contains roughly 700M tokens.
We use the same train/validation/test split as Khandelwal et al. (2019), as well as their tokenization, which uses BERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs, we replace the adaptive word embedding and softmax of Baevski and Auli (2019) with a tied word embedding and softmax matrix (Press and Wolf, 2017; Inan et al., 2017). Finally, we tune the sandwich coefficient on the development set for k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {4, . . . , 8}, i.e., a neighborhood of 2 around the best value we found for WikiText-103 (k = 6).
Table 4 shows that the sandwich transformer transfers well to the books domain, improving performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM (Khandelwal et al., 2019), which is the state of the art on WikiText-103 (see Section 4).",positive
93,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","Modeling text as a stream of characters, rather than word or subword tokens, presents a different modeling challenge: long-range dependencies become critical, and the vocabulary takes on a more uniform distribution. We apply our sandwich reordering to the adaptive span model of Sukhbaatar et al. (2019), which is state of the art on the popular English-language benchmark text8 and is currently a close second on enwik8.3 The adaptive span
3Both datasets are taken from http://mattmahoney. net/dc/textdata.html
model learns to control each attention headÃÂÃÂ¢ÃÂÃÂÃÂÃÂs maximal attention span, freeing up memory in the bottom layers (which typically need very short attention spans) and applying it to the top layers, allowing the top-level attention heads to reach significantly longer distances. The adaptive span modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs efficient use of attention also results in a significant speed boost.
We tune the sandwich coefficient on the development set for k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1, . . . , 8} (the baseline model has 24 transformer layers). We do not modify any hyperparameters, including the number of training epochs. Table 5 compares the baseline modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance with the sandwich transformerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs. On text8, the sandwich transformer performs within the baselineÃÂÃÂ¢ÃÂÃÂÃÂÃÂs random seed variance. On enwik8, the sandwich transformer gains an improvement of about 0.007 bits-per-character, matching the state of the art results obtained by the TransformerXL-based Compressive Transformer of Rae et al. (2020).
However, our approach is able to achieve this result without applying the Transformer-XLÃÂÃÂ¢ÃÂÃÂÃÂÃÂs recurrent attention, which is much slower (Sukhbaatar et al., 2019), and without adding additional parameters (the compressive transformer uses 277M parameters, while our baseline and sandwich models use only 209M).",positive
94,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","Sandwich Decoders Tranformer-based translation models (Vaswani et al., 2017) consist of an encoder and decoder, where the encoder has interleaved self-attention and feedforward sublayers (just as in language models), while the decoder includes an additional sublayer, cross-attention (c), between every pair of self-attention and feedforward sublayers. Cross-attention sublayers attend to the encoderÃÂÃÂ¢ÃÂÃÂÃÂÃÂs representations of the input sentenceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs tokens.
Following our notation from Section 2, a transformer decoder layer modifies the sequence of tokens in the target language Y0, using the encoded source tokens X, as follows:
Y1 = self-attention(Y0) +Y0 Y2 = cross-attention(Y1,X) +Y1 Y3 = feedforward(Y2) +Y2
Applying the sandwich pattern to the encoder follows the same methodology as our previous experiments. However, for the decoder, we group the
self-attention (s) and cross-attention (c) sublayers, and treat them as a single unit for reordering purposes (sc). For example, a three layer decoder (scfscfscf) with a sandwiching coefficient of k = 1 would be: scscfscff. We apply the sandwich pattern to either the encoder or decoder separately, while keeping the other stack in its original interleaved pattern.
Experiment Setting As a baseline, we use the large transformer model (6 encoder/decoder layers, embedding size of 1024, feedforward inner dimension of 4096, and 16 attention heads) with the hyperparameters of Ott et al. (2018). We also follow their setup for training and evaluation: we train on the WMT 2014 En-De dataset which contains 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (Sennrich et al., 2016). For inference we use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. (2017) and Ott et al. (2018). As before, we do not modify our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs hyperparameters or training procedure.
Results Table 6 shows that reordering of either the encoder or decoder does not have a significant impact on performance, across the board. We also find that using the most extreme sandwich decoder (sc)6f6 performs almost exactly the same as the average baseline; this result is consistent with our observation from Section 4, where we show that the extreme sandwich language model (s16f16) performs as well as the baseline.
Discussion This experiment indicates that a reordering pattern that benefits one particular task (language modeling) might not carry the same performance gains to another (machine translation). However, it also demonstrates the general robustness of transformer architectures to sublayer reordering, as we did not observe any major perfor-
mance degradation. Since the sandwich pattern naively groups self- and cross-attention sublayers together, it is also possible that a reordering pattern that takes all three sublayer types into account could potentially improve performance.",positive
95,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","At the time of writing, we do not have an explanation for why sublayer reordering improves performance on language modeling. However, we are able to determine that sandwich transformers spread their attention in a different fashion than interleaved models.
We analyze two baseline models and two sandwich166 models trained with different seeds on the WikiText-103 dataset, by first recording the attention values that each tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs heads assign to all other tokens during inference on the validation set. Given the attention outputs of two models, we then compute the modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ attention distance for each token, and for each self-attention sublayer. This metric compares the attention distribution in the ith self-attention sublayer of the first model to that of the ith self-attention sublayer of the second model, for a specific token.
Given a token and a self-attention sublayer,
we use the Hungarian algorithm (Kuhn, 1955) to find a matching of heads in the first model to heads in the second model [a1, b1], . . . , [a8, b8] such that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ8 i=1 EMD(ai, bi) is minimized, where EMD(ai, bi) is the earth moverÃÂÃÂ¢ÃÂÃÂÃÂÃÂs (Wasserstein) distance between the attention distributions of head ai in the first model and head bi in the second model. That minimal value is the attention distance for that token, in that layer. We then average the attention distances across all tokens and layers.
Table 7 shows the average attention distances between every pair of models. We observe that models of the same architecture have significantly lower attention distances than models with different sublayer orderings. This indicates that sublayer reordering has a strong effect on the attention function that the model learns in each head. Future investigations of what this difference is, in a qualitative sense, could potentially provide important insights for designing better reordering patterns.",positive
96,Improving Transformer Models by Reordering their Sublayers,"Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.1","We train random transformer models with reordered sublayers, and find that some perform better than the baseline interleaved transformer in language modeling. We observe that, on average, better models contain more self-attention sublayers at the bottom and more feedforward sublayer at the top. This leads us to design a new transformer stack, the sandwich transformer, which significantly improves performance over the baseline at no cost in parameters, memory, or runtime.
We then show that the sandwich ordering also improves language modeling performance on a different word-level language modeling benchmark, and that the sandwich pattern can be used to achieve state of the art results on character-level language
modeling. Although sandwich ordering does not improve translation models, we show that they are robust to layer order changes, and that even extreme reorderings (all attention sublayers at the bottom, and all the feedforward sublayers at the top) perform as well as the baseline.
Sublayer reordering can improve the performance of transformer models, but an ordering that improves models on one group of tasks (word/character-level language modeling) might not improve the performance on another task. By showing that sublayer ordering can improve models at no extra cost, we hope that future research continues this line of work by looking into optimal sublayer ordering for other tasks, such as translation, question answering, and classification.",positive
97,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","Discourse relation describes how two adjacent text units (e.g., clauses, sentences, and larger sentence groups) are connected logically to one another. A discourse relation instance is usually defined as a connective taking two arguments (as Arg1 and Arg2, respectively). Implicit discourse relation recognition without explicit connectives (Pitler et al., 2009) is still a challenging problem of discourse analysis, which needs to infer the discourse relation from a specific context. It is beneficial to many downstream natural language processing (NLP) applications, such as machine translation (Meyer and Popescu-Belis, 2012) and text summarization (Gerani et al., 2014).
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂCorresponding author.
The existing neural network-based models have shown great success in recognizing implicit discourse relations. It mainly includes 1) Basic neural networks (Braud and Denis, 2015; Zhang et al., 2015; Liu et al., 2016) can learn the dense vector representations of discourse arguments, which can capture the semantic information to some extent. Further studies exploit different attention or memory mechanisms (Liu and Li, 2016; Zhang et al., 2016) to capture the critical information of argument pairs. 2) Complex neural models (Chen et al., 2016; Lei et al., 2017; Guo et al., 2018) utilize gated relevance networks or neural tensor networks to capture the deeper interactions between two discourse arguments. 3) Joint learning architectures (Qin et al., 2017; Bai and Zhao, 2018; Xu et al., 2019) exploit implicit connective cues, different granularity of text, or topic-level relevant information to improve the discourse relation prediction. However, these approaches still have the following drawbacks: 1) do not make full use of the annotated discourse relation signal to explore the argumentrelation features; 2) neglect the extra information in the low-dimensional continuous embedding space, i.e., the direction or structure information of the vectors.
Notice that Translating Embeddings (TransE) is a method for the prediction of entitiesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ missing relations in knowledge graphs. Bordes et al. (2013) model relations by interpreting them as translating operation not on the graph structure directly but in a learned low-dimensional embedding of the knowledge graph entities: if (he, le, te) holds, then the embedding of the tail entity te should be close to the embedding of the head entity he plus some vector that depends on the relation le. Similar to the entity relation extraction, our task aims to identify the semantic relations between two arguments (i.e., sentences).
Inspired by TransE, we design a new method
(TransS), which translates discourse relations in sentence embedding spaces to mine the argumentrelation features. Intuitively, these features reflect the latent geometric structure among the arguments and their discourse relation by performing the algebraic operation, and the argument-relation instances with the same discourse relation may have similar direction and position information in the embedding space. Therefore, we propose a novel TransS-driven joint learning neural network framework that leverages the latent geometric structure information of argument-relation instances, in addition to using the semantic features to improve the comprehension of discourse argument. Among them, we adopt a multi-level encoder to further enrich the argument representations, which could obtain the deeper semantics of discourse.
In summary, the main contributions of this paper are as follows:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Propose a novel TransS-driven joint learning architecture, including the latent geometric structure information learning (GSL) and semantic feature learning (SFL);
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Design TransS approach to translate discourse relations in low-dimensional embedding space from the sentence-level perspective, which could induce the geometric structure of argument-relation instances to some extent;
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Employ the mutual reinforcing between the GSL and SFL to optimize the argument representations: 1) the GSL adopts its geometric structure clues to facilitate the SFL; 2) the SFL utilizes its semantic cues to improve the learning capability of GSL;
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ The experimental results on the PDTB demonstrate the effectiveness of our model.",positive
98,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","TransE, as a model for learning low-dimensional embeddings of entities, is to enforce the structure of embedding space in which different relations between entities of different types may be represented by translation (Bordes et al., 2013). Discourse relation recognition and entity relation extraction are
similar to some extent. Intuitively, the argumentrelation instances with the same discourse relation may also have similar direction and position information in embedding space. However, discourse argument embedding is a sentence-level representation, which is different from the reuse of entities in other sentences, and more diverse and complex than entity representation. Therefore, we design TransS, a method which models discourse relations by interpreting them as translations operating in the low-dimensional embedding space from the sentence perspective. Moreover, it could mine the latent geometric structure of argument-relation instances. Specifically, to define two arguments as head vector hs and tail vector ts respectively, their annotated relation signal as relation vector rs, the latent geometric structure is reflected by hs + rs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ts, their score function is defined as follows:
ds(hs, ts) = ||hs + rs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ts||22. (6)
where hs, ts denote the representations of Arg1 and Arg2 respectively; rs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd is the embedding of discourse relation and d is the dimension of word embedding.
GSL Loss. Under the framework of TransS, given a training set T of triplets (hs, rs, ts) composed of two arguments hs, ts ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ V (the set of sentence vectors) and a relation rs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R (the set of relation), our model would learn the embeddings of the words in arguments and the discourse relation. The GSL
loss function is defined as: LGSL = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(hs,rs,ts)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂT ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s,rs,t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² s)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂT ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²(hs,rs,ts) [ÃÂÃÂÃÂÃÂ³ + ds(hs
+ rs, ts)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ds(hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s + rs, tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s)]+ + ÃÂÃÂÃÂÃÂ»GSLÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¸ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ22. (7)
where [ÃÂÃÂÃÂÃÂ·]+ denotes the positive instances, ÃÂÃÂÃÂÃÂ³ > 0 is a margin hyper-parameter, and the set of negative triplets, constructed according to Eq.(8), in which the head or tail is replaced by a random argument vector (but not simultaneously). ÃÂÃÂÃÂÃÂ¸ denotes the other parameters of the network. L2 regularization is used to penalize the size of all parameters for preventing overfitting, weighted by ÃÂÃÂÃÂÃÂ»GSL.
T ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²(hs,rs,ts) ={(h ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² s, rs, ts)|hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ V }ÃÂÃÂ¢ÃÂÃÂÃÂÃÂª
{(hs, rs, tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s)|tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ V )}. (8)
By optimizing the GSL loss, we could obtain the latent geometric structure information about argument-relation instances. Different from TransE, we could not directly utilize TransS to recognize discourse relations, for that each argument could not be reused in discourse. Therefore, we exploit TransS to mine the latent geometric structure information and further guide the semantic feature learning.",positive
99,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","To validate the effectiveness of our model, we select some state-of-the-art systems from the following three aspects to compare with our model: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Discourse Argument Representation 1) Ji2015: Ji and Eisenstein (2015) computed distributed representations for each discourse argument by composition up the syntactic parse tree. 2) Zhang2015: Zhang et al. (2015) proposed pure neural networks with three different pooling operations to learn shallow representations in tasks. 3) Liu2016a: Liu and Li (2016) combined attention mechanism and external memory to focus on specific words that helps determine discourse relations. 4) Lan2017: Lan et al. (2017) designed an attention-based neural network for learning discourse argument representations and a multi-task framework for learning knowledge from annotated and unannotated corpora. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Complex Neural Models 5) Chen2016: Chen et al. (2016) adopted a gated relevance network to capture interaction information between two arguments to enhance relation recognition. 6) Qin2016: Qin et al. (2016a) adopted contextaware character-enhanced embeddings to address implicit discourse relation recognition task. 7) Lei2017: Lei et al. (2017) devised the Simple Word Interaction Model (SWIM) to learn the interactions between word pairs. 8) Dai2018: Dai and Huang (2018) modeled interdependencies between discourse units as well as discourse relation continuity and patterns, and pre-
dict a sequence of discourse relations in a paragraph. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Joint Learning 9) Liu2016b: Liu et al. (2016) designed related discourse classification tasks specific to a corpus, and proposed a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. 10) Bai2018: Bai and Zhao (2018) employed different grained text representations, including character, subword, word, sentence, and sentence pair levels, and transfered the knowledge from the implicit connectives to support discourse relation prediction.",positive
100,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","For the ablation models, we can make the observations from Table 3:
Overall:1) Our model gains state-of-the-art performance than that of the other ablation models. This demonstrates that the geometric structure information could enrich the argument representation and promote implicit discourse relation recognition. 2) All models have a higher F1 values on the Expansion relation than those of the other relations. The unbalanced data may cause that.
GSL: The F1 score of our model using the GSL module is 48.91%, higher than the performance of Baseline. In addition, compared with ELMo,
although the performance of GSL does not exceed ELMoÃÂÃÂ¢ÃÂÃÂÃÂÃÂs, GSL obtain comparable results. This manifests that the two modules (GSL and SFL) could reinforce with each other, which utilizes the geometric structure information by the algebraic operation. Moreover, we exploit the geometric structure clues to augment the semantic understanding of discourse from a new aspect, which is different from the ELMo only focusing on the semantic information of the text itself.
ELMo: The third row of Table 3 is the result of our model, which only uses the pre-trained ELMo vector to enhance argument representations. The F1 score and accuracy are 50.07% and 58.89%, respectively, which achieve 3.61% and 4.87% improvements than those of the Baseline. It verifies that ELMo, as pre-trained contextualized word embeddings, could contain more contextual information.
GSL & ELMo: Compared with ELMo, GSL & ELMo gains better performance, which demonstrates that inducing spatial geometry structure information based on argument enhancement could understand the semantics of discourse better.",positive
101,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","To illustrate the effectiveness of the latent geometric structure information of argument-relation instances gotten by TransS, we visualize the heat maps of the interaction information of argument representations shown in Figure3. Every word comes with various background colors. The darker patches denote the correlations of word pairs are higher. The example of Comparison relation is listed below:
Arg1: I was prepared to be in a very bad mood tonight.
Arg2: Now, I feel maybe thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a little bit of euphoria.
From the semantics of perspective, this example could be identified as Comparison or Temporal relation. Since argument pairs may have distinct distinguishing features in geometric space, we could consider the geometric structure of argument pairs to help identify the discourse relation. We can obtain the following observations:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Seen from Figure3(a), without introducing geometric structure information, the model has a high correlation around the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNowÃÂÃÂ¢ÃÂÃÂÃÂÃÂ which might indicate the Temporal relation directly. This demonstrates that only considering the semantic information of arguments may suffer from issues such as polysemy, ambiguity, as well as fuzziness.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Figure3(b) shows the result of the interaction information of argument representations, which introduces the GSL. From the results, we can see that the model has a high correlation around the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlittleÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂveryÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with the comparative information. The possible reason is that our model utilizing GSL shifts the higher attention from the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNowÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with Temporal information to the word pairs (little, very), (euphoria, bad) and (euphoria, mood) with Comparison relation. Our model with GSL introduces the geometric structure information and jointly utilizes these features and semantic information to help identify the discourse relation.",positive
102,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","Recently, some researches adopt joint learning framework to capture more discourse clues for the task. Bai and Zhao (2018) jointly predict connectives and relations, assuming the shared parameters of the deep learning models. Xu et al. (2019) propose a topic tensor network (TTN) to model the sentence-level interactions and topic-level relevance among arguments for this task. However, few studies model discourse relations by translating them in the low-dimensional embedding space as we do in this work.
TransE effectively maps the relation to the embedding space of entities by performing the algebraic operation. Bordes et al. (2013) model entity relations by interpreting them as translating operation in the low-dimensional embedding of the entities. Inspired by TransE, we design a TransS method to mine the latent geometric structure information, which could enhance the argument representations for promoting discourse relation recognition. To our knowledge, this is the first attempt to mine the latent geometric structure of argumentrelation. Meanwhile, the embeddings of argument and relation by TransS could be used to the other high-level NLP tasks.",positive
103,TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argumentrelation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.","In this paper, we propose a novel TransS-driven joint learning neural network framework by optimizing the discourse argument representations to improve implicit discourse relation recognition. We interpret the discourse relations as translation in low-dimensional embedding space, which reflects the geometric structure of argument-relation, and also can obtain the richer argument representations
based on the multi-level encoder. Different from the conventional approaches only considering the semantic features, we jointly leverage the latent geometric structure information and the semantic features to optimize the argument representations, which could improve the semantic understanding of discourse. Experimental results on the PDTB show the effectiveness of our model.",positive
104,A Formal Hierarchy of RNN Architectures,"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNNÃÂÃÂ¢ÃÂÃÂÃÂÃÂs memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsaturatedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture.","While neural networks are central to the performance of todayÃÂÃÂ¢ÃÂÃÂÃÂÃÂs strongest NLP systems, theoretical understanding of the formal properties of different kinds of networks is still limited. It is established, for example, that the Elman (1990) RNN is Turing-complete, given infinite precision and computation time (Siegelmann and Sontag, 1992, 1994; Chen et al., 2018). But tightening these unrealistic assumptions has serious implications for expressive power (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address.
Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than
conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like the quasirecurrent neural network (QRNN; Bradbury et al., 2016) and unigram rational RNN (Dodge et al., 2019) perform comparably to the LSTM, with a smaller computational budget. Still, the underlying simplicity of rational models raises the question of whether their expressive power is fundamentally limited compared to other RNNs.
In a separate line of work, Merrill (2019) introduced the saturated RNN1 as a formal model for analyzing the capacity of RNNs. A saturated RNN is a simplified network where all activation functions have been replaced by step functions. The saturated network may be seen intuitively as a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂstableÃÂÃÂ¢ÃÂÃÂÃÂÃÂ version of its original RNN, in which the in-
1Originally referred to as the asymptotic RNN.
ternal activations act discretely. A growing body of workÃÂÃÂ¢ÃÂÃÂÃÂÃÂincluding this paperÃÂÃÂ¢ÃÂÃÂÃÂÃÂfinds that the saturated theory predicts differences in practical learnable capacity for various RNN architectures (Weiss et al., 2018; Merrill, 2019; Suzgun et al., 2019a).
We compare the expressive power of rational and non-rational RNNs, distinguishing between state expressiveness (what kind and amount of information the RNN states can capture) and language expressiveness (what languages can be recognized when the state is passed to a classifier). To do this, we build on the theory of saturated RNNs.
State expressiveness We introduce a unified hierarchy (Figure 1) of the functions expressible by the states of rational and non-rational RNN encoders. The hierarchy is defined by two formal properties: space complexity, which is a measure of network memory,2 and rational recurrence, whether the internal structure of the RNN can be described by WFAs. The hierarchy reveals concrete differences between LSTMs and QRNNs, and further separates both from a class containing convolutional neural networks (CNNs, Lecun and Bengio, 1995; Kim, 2014), Elman RNNs, and gated recurrent units (GRU; Cho et al., 2014).
We provide the first formal proof that LSTMs can encode functions that rational recurrences cannot. On the other hand, we show that the saturated Elman RNN and GRU are rational recurrences with constant space complexity, whereas the QRNN has unbounded space complexity. We also show that an unrestricted WFA has rich expressive power beyond any saturated RNN we considerÃÂÃÂ¢ÃÂÃÂÃÂÃÂincluding the LSTM. This difference potentially opens the door to more expressive RNNs incorporating the computational efficiency of rational recurrences.
Language expressiveness When applied to classification tasks like language recognition, RNNs are typically combined with a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdecoderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ: additional layer(s) that map their hidden states to a prediction. Thus, despite differences in state expressiveness, rational RNNs might be able to achieve comparable empirical performance to non-rational RNNs on NLP tasks. In this work, we consider the setup in which the decoders only view the final hidden state of the RNN.3 We demonstrate that
2Space complexity measures the number of different configurations an RNN can reach as a function of input length. Formal definition deferred until Section 2.
3This is common, but not the only possibility. For example, an attention decoder observes the full sequence of states.
a sufficiently strong decoder can overcome some of the differences in state expressiveness between different models. For example, an LSTM can recognize anbn with a single decoding layer, whereas a QRNN provably cannot until the decoder has two layers. However, we also construct a language that an LSTM can recognize without a decoder, but a QRNN cannot recognize with any decoder. Thus, no decoder can fully compensate for the weakness of the QRNN compared to the LSTM.
Experiments Finally, we conduct experiments on formal languages, justifying that our theorems correctly predict which languages unsaturated recognizers trained by gradient descent can learn. Thus, we view our hierarchy as a useful formal tool for understanding the relative capabilities of different RNN architectures.
Roadmap We present the formal devices for our analysis of RNNs in Section 2. In Section 3 we develop our hierarchy of state expressiveness for single-layer RNNs. In Section 4, we shift to study RNNs as language recognizers. Finally, in Section 5, we provide empirical results evaluating the relevance of our predictions for unsaturated RNNs.",positive
105,A Formal Hierarchy of RNN Architectures,"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNNÃÂÃÂ¢ÃÂÃÂÃÂÃÂs memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsaturatedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture.","Finally, we study the theoretical case where the decoder is an arbitrary recursively enumerable (RE) function. We view this as a loose upper bound of stacking many layers after a rational encoder. What information is inherently lost by using a rational encoder? WFAs can uniquely encode each input, making them Turing-complete under this setup; however, this does not hold for rational s-RNNs.
RR-complete Assuming an RR-complete encoder, a WFA like Figure 5 can be used to encode each possible input sequence over ÃÂÃÂÃÂÃÂ£ to a unique number. We then use the decoder as an oracle to decide any RE language. Thus, an RR-complete encoder with an RE decoder is Turing-complete.
Bounded space However, the ÃÂÃÂÃÂÃÂ(log n) space bound of saturated rational RNNs like the s-QRNN means these models cannot fully encode the input. In other words, some information about the prefix x:t must be lost in ct. Thus, rational s-RNNs are not Turing-complete with an RE decoder.",positive
106,Language-aware Interlingua for Multilingual Neural Machine Translation,"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.","Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017) has significantly improved the translation quality due to its end-to-end modeling and continuous representation. While conventional NMT performs single pair translation well, training a separate model for each language pair is resource consuming, considering there are thousands of languages in the world. Therefore multilingual NMT is introduced to handle multiple language pairs in one model, reducing the online serving and offline training cost. Furthermore, the multilingual NMT framework facilitates the cross-lingual knowledge transfer to improve translation performance on low resource language pairs (Wang et al., 2019).
Despite all the mentioned advantages, multilingual NMT remains a challenging task since
the language diversity and model capacity limitations lead to inferior performance against individual models that are sufficiently trained. So recent efforts in multilingual NMT mainly focus on enlarging the model capacity, either by introducing multiple Encoders and Decoders to handle different languages (Firat et al., 2016; Zoph and Knight, 2016), or enhancing the attention mechanism with language-specific signals (Blackwood et al., 2018). On the other hand, there have been some efforts to model the specificity of different languages. Johnson et al. (2017) and Ha et al. (2016) tackle this by simply adding some pre-designed tokens at the beginning of the source/target sequence, but we argue that such signals are not strong enough to learn enough language-specific information to transform the continuous representation of each language into the shared semantic space based on our observations.
In this paper, we incorporate a language-aware Interlingua module into the Encoder-Decoder architecture. It explicitly models the shared semantic space for all languages and acts as a bridge between the Encoder and Decoder network. Specifically, we first introduce a language embedding to represent unique characteristics of each language and an interlingua embedding to capture the common semantics across languages. Then we use the two embeddings to augment the self-attention mechanism which transforms the Encoder representation into the shared semantic space. To minimize the information loss and keep the semantic consistency during transformation, we also introduce reconstruction loss and semantic consistency loss into the training objective. Besides, to further enhance the language-specific signal we incorporate language-aware positional embedding for both Encoder and Decoder, and take the language embedding as the initial state of the target side.
We conduct experiments on both standard WMT data sets and large scale in-house data sets. And our proposed model achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with sufficiently trained individual models.",positive
107,Language-aware Interlingua for Multilingual Neural Machine Translation,"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.","The Interlingua module uses multi-head attention mechanism, mapping the Encoder output Henc of different languages to a language-independent representation I .
I = FFN(ATT(Q,K, V )) (1)
Q = FFN(Lemb, Iemb) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂr (2)
K,V = Henc ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂn (3)
The Henc denotes the hidden states out of the Encoder, while the d is the hidden size, and the n denotes the length of the source sentence. ATT(.)
is the multi-head attention mechanism (Vaswani et al., 2017). The (K,V ) here are computed from the hidden states of the Encoder output Henc. The Q is composed of two parts in simple linear combination. One part is from the language-specific part Lemb, and the other part is a shared matrix Iemb, which we called interlingua embedding. Note that, the interlingua embedding Iemb has a fixed size of [dÃÂÃÂÃÂÃÂr]. the i-th column of Iemb represents a initial semantic subspace that guides what semantic information of the Henc should be attended to at the corresponding position i of the Interlingua output. The r means every Encoder Henc will be mapped into a fixed size representation of r hidden states, and it is set to 10 during all of our experiments, similar to the work of (VaÃÂÃÂÃÂÃÂzquez et al., 2018). By incorporating a shared interlingua embedding, we expect that it can exploit the semantics of various subspaces from encoded representation, and the same semantic components of different sentences from both same and different languages should be mapped into the same position i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ [1, r]. Language embedding Lemb is used as an indicator for the Interlingua that which language it is attending to, as different languages have their own characteristics. So we call the module language-aware Interlingua. FFN(.) is a simple position-wise feed-forward network. By introducing Interlingua module into the Encoder-Decoder structure, we explicitly model the intermediate semantic. In this framework, the language-sensitive Enc is to model the characteristics of each language, and the language-independent Interlingua to enhance cross-language knowledge transfer.",positive
108,Language-aware Interlingua for Multilingual Neural Machine Translation,"Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.","We introduce three types of training objectives in our model, similar to (Escolano et al., 2019).
(i) Translation objective: Generally, a bilingual NMT model adopts the cross-entropy loss as the training objective, which we denote asLs2t, meanwhile, we incorporate another loss Lt2s for translation from the target to the source.
(ii) Reconstruction objective: The Interlingua transforms the Encoder output into an intermediate representation I . During translation, the Decoder only uses the I instead of any Encoder information. Inspired by Lample et al. (2017), Tu et al. (2017) and Lample et al. (2018), we incorporate an reconstruction loss for the purpose of minimizing information loss. We denote the X ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² = Decoder(Interlingua(Encoder(X))) as the reconstruction of X . So we employ crossentropy between X ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² and X as our reconstruction loss, and denote Ls2s for the source, Lt2t for the target.
(iii) Semantic consistency objective: Obviously, sentences from different languages with the same semantics should have the same intermediate rep-
resentation. So we leverage a simple but effective method, cosine similarity to measure the consistency. Similar objectives were incorporated in zero-shot translation (Al-Shedivat and Parikh, 2019; Arivazhagan et al., 2019)
sim(Is, It) = 1
r rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 Isi ÃÂÃÂÃÂÃÂ· Iti ÃÂÃÂ¢ÃÂÃÂÃÂÃÂIsi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂItiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(4)
Where, Is and It denote the Interlingua representation of the source and target sides respectively. Ii is the i-th column of matrix I . Ldist = 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsim(Is, It) is used as distance loss in our training objective.
Finally, the objective function of our learning algorithm is thus:
L = Ls2t + Lt2s + Ls2s + Lt2t + Ldist (5)",positive
109,Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts,"Despite the pervasiveness of clinical depression in modern society, professional help remains highly stigmatized, inaccessible, and expensive. Accurately diagnosing depression is difficultÃÂÃÂ¢ÃÂÃÂÃÂÃÂ requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a method that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to identify high-level conversational contexts that influence patterns of language in depressed individuals. We show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression.","Beyond improving classification performance, the latent categorization of prompts yields insight about conversational contexts relevant for analyzing language patterns in depressed individuals.
To explore the learned categories, we isolate interviews from the complete corpus that are correctly labeled by our best-performing model. We say that the model ÃÂÃÂ¢ÃÂÃÂÃÂÃÂassignsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ an interview prompt to a given category if the promptÃÂÃÂ¢ÃÂÃÂÃÂÃÂs membership for that category (Equation 1) is stronger than for other categories. We now describe the various prompts assigned to different categories.3
Firstly, all prompts that are questions like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTell me more about thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWhen was the last time you had an argument?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, etc. are grouped together into
2Statistical significance is calculated from the test prediction using two-sided T-test for independent samples of scores
3To verify consistency of prompt categorization, we rerun the model with multiple initialization and they all yielded the same general trends as described in the paper.
12
a single category, which we refer to as the Starters category. Previous work has identified usefulness of such questions as conversation starters since they assist in creating a sense of closeness (Mcallister et al., 2004; Heritage and Robinson, 2006).
Secondly, there are several categories reserved exclusively for certain backchannels. Backchannels are short utterances that punctuate longer turns by another conversational participant (Yngve, 1970; Goodwin, 1986; Bavelas et al., 2000). Specifically, the model assigns the backchannels ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmhm,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmm,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnice,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂawesomeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ each to separate categories. Research shows that it is indeed useful to consider the effects different types of backchannels separately. For example, Bavelas et al. (2000) propose a distinction between specific backchannels (such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂniceÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂawesomeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and generic backchannels (such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmhmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), and Tolins and Fox Tree (2014) demonstrated that each backchannel type serves a different purpose in conversation.
Thirdly, apart from starters and backchannels, the model isolates one specific prompt - ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHave you been diagnosed with depression?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4 into a separate category. Clearly, this is an important prompt and it is encouraging to see that the model isolates it as useful. Interestingly, the model assigns the backchannel ÃÂÃÂ¢ÃÂÃÂÃÂÃÂawÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to the same category as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHave you been diagnosed with depression?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ suggesting that responses to both prompts yield similar signals for depression.
Lastly, the remaining five categories are empty - no prompt in the corpus has maximum salience with any of them. A likely explanation for this observation stems from the choice of normalizing factor Zki in Equation 3: it causes RÃÂÃÂÃÂÃÂ k i to regress to the unweighted average of response embeddings when all prompts in an interview have low salience with category k. Repeated empty categories then function as an ÃÂÃÂ¢ÃÂÃÂÃÂÃÂensemble modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for the average response embeddings, potentially improving predictive performance.",positive
110,Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts,"Despite the pervasiveness of clinical depression in modern society, professional help remains highly stigmatized, inaccessible, and expensive. Accurately diagnosing depression is difficultÃÂÃÂ¢ÃÂÃÂÃÂÃÂ requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a method that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to identify high-level conversational contexts that influence patterns of language in depressed individuals. We show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression.","In this section, we analyze major sources of error. We apply a similar reverse engineering method as in Section 4.6. For prompts in each category, we consider corresponding responses that result in strong incorrect signals (false positive or false negative) based on the categoryÃÂÃÂ¢ÃÂÃÂÃÂÃÂs weights in the decision layer. We focus on the categories with the most significance presence in the dataset: the categories corresponding to starters, the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmhmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ backchannel, and the prompt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHave you been diagnosed with depression?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
For the starters category, false positive-signal responses tend to contain a high presence of fillers and discourse markers (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂuh,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhuh,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpost mm traumatic stress uh no uh uh,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhmmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). It is possible that because the model learned to focus on short, low-semantic-content responses, it incorrectly correlates presence of fillers and discourse markers with depression. For the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmhmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ category, we identified several false negatives, in which the responses included concrete words like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂuh nice environmentÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂI love the landscapeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂI love the watersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Since the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmhmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ category focuses on vague, qualified language to predict depression (see Figure 3), the presence of concrete words in these responses could have misled the model. For the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHave you been diagnosed with depression?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ category, the misclassified interviews contained short responses to this prompt like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂso,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnever,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂyes,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂyeah,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂno,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as well as statements containing the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdepression.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ For this category, the model seems to incorrectly correlate short re-
sponses and direct mentions of depression with the depressed class.",positive
111,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","AIR iteratively builds justification chains given a query. AIR starts by initializing the query with the concatenated question and candidate answer text3. Then, AIR iteratively repeats the following two steps: (a) It retrieves the most salient justification sentence given the current query using an alignment-IR approach(Yadav et al., 2019a). The candidate justification sentences come from datasetspecific KBs. For example, in MultiRC, we use as candidates all the sentences from the paragraph associated with the given question. In QASC, which has a large KB4 of 17.4 million sentences), similar to Khot et al. (2019a) candidates are retrieved using the Heuristic+IR method which returns 80 candidate sentences for each candidate answer from the provided QASC KB. (b) it adjusts the query to focus on the missing information, i.e., the keywords that are not covered by the current evidence chain. AIR also dynamically adds new terms to the query from the previously retrieved justifications to nudge multi-hop retrieval. These two iterative steps repeat until a parameter-free termination condition is reached.
We first detail the important components of AIR. Alignment: To compute the similarity score between a given query and a sentence from KB, AIR
3Note that this work can be trivially adapted to reading comprehension tasks. In such tasks (e.g., SQuAD (Rajpurkar et al., 2018)), the initial query would contain just the question text.
4In large KB-based QA, AIR first uses an off-the-shelf Lucene BM25(Robertson et al., 2009) to retrieve a pool of candidate justification sentences from which the evidence chains are constructed.
uses a vanilla unsupervised alignment method of Yadav et al. (2019a) which uses only GloVe embeddings (Pennington et al., 2014).5 The alignment method computes the cosine similarity between the word embeddings of each token in the query and each token in the given KB sentence, resulting in a matrix of cosine similarity scores. For each query token, the algorithm select the most similar token in the evidence text using max-pooling. At the end, the element-wise dot product between this maxpooled vector of cosine-similarity scores and the vector containing the IDF values of the query tokens is calculated to produce the overall alignment score s for the given query Q and the supporting paragraph Pj :
5Alignment based on BERT embeddings marginally outperformed the one based on GloVe embeddings, but BERT embeddings were much more expensive to generate.
s(Q,Pj) = |Q|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 idf (qi) ÃÂÃÂÃÂÃÂ· align(qi, Pj) (1)
align(qi, Pj) = |Pj | max k=1 cosSim(qi, pk) (2)
where qi and pk are the ith and kth terms of the query (Q) and evidence sentence (Pj) respectively.
Remainder terms (Qr): Query reformulation in AIR is driven by the remainder terms, which are the set of query terms not yet covered in the justification set of i sentences (retrieved from the first i iterations of the retrieval process):
Qr(i) = t(Q)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
skÃÂÃÂ¢ÃÂÃÂÃÂÃÂSi
t(sk) (3)
where t(Q) represents the unique set of query terms, t(sk) represents the unique terms of the kth
justification, and Si represents the set of i justification sentences. Note that we use soft matching of alignment for the inclusion operation: we consider a query term to be included in the set of terms in the justifications if its cosine similarity with a justification term is larger than a similarity threshold M (we use M=0.95 for all our experiments - see section 5.2), thus ensuring that the two terms are similar in the embedding space.
Coverage (Qc): measures the coverage of the query keywords by the retrieved chain of justifications S:
Qc(i) = | ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
skÃÂÃÂ¢ÃÂÃÂÃÂÃÂSi t(Q) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© t(sk)| |t(Q)|
(4)
where |t(Q)| denotes the size of unique query terms.
The AIR retrieval process
Query reformulation: In each iteration j, AIR reformulates the query Q(j) to include only the terms not yet covered by the current justification chain, Qr(j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1). See, for example, the second hop in fig. 2. To mitigate ambiguous queries, the query is expanded with the terms from all the previously retrieved justification sentences only if the number of uncovered terms is less than T (we used T = 2 for MultiRC and T = 4 for QASC (see section 5.2). See, for example, the third hop in fig. 2, in which the query is expanded with the terms of all the previously retrieved justification sentences. Formally:
Q(j) = {
Qr(j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1), if |Qr(j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1)| > T Qr(j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1) + (t(sjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t(Q)), otherwise
(5)
where j is the current iteration index.
Stopping criteria: AIR stops its iterative evidence retrieval process when either of the following conditions is true: (a) no new query terms are discovered in the last justification retrieved, i.e., Qr(iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1) == Qr(i), or (b) all query terms are covered by justifications, i.e., Qc = 1.",positive
112,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","AIRÃÂÃÂ¢ÃÂÃÂÃÂÃÂs justification chains can be fed into any supervised answer classification method. For all experiments in this paper, we used RoBERTa (Liu et al., 2019), a state-of-the-art transformer-based method. In particular, for MultiRC, we concatenate the query (composed from question and candidate answer text) with the evidence text, with the
[SEP] token between the two texts. A sigmoid is used over the [CLS] representation to train a binary classification task6 (correct answer or not).
For QASC, we fine-tune RoBERTa as a multiplechoice QA 7 (MCQA) (Wolf et al., 2019) classifier with 8 choices using a softmax layer(similar to (Khot et al., 2019a)) instead of the sigmoid. The input text consists of eight queries (from eight candidate answers) and their corresponding eight evidence texts. Unlike the case of MultiRC, it is possible to train a MCQA classifier for QASC because every question has only 1 correct answer. We had also tried the binary classification approach for QASC but it resulted in nearly 5% lower performance for majority of the experiments in table 2.
In QA tasks that rely on large KBs there may exist multiple chains of evidence that support a correct answer. This is particularly relevant in QASC, whose KB contains 17.2M facts.8 Figure 1 shows an example of this situation. To utilize this type of redundancy in answer classification, we extend AIR to extract parallel evidence chains. That is, to extract N parallel chains, we run AIR N times, ensuring that the first justification sentences in each chain are different (in practice, we start a new chain for each justification in the top N retrieved sentences in the first hop). After retrieving N parallel evidence chains, we take the union of all the individual justification sentences to create the supporting evidence text for that candidate answer.",positive
113,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","We evaluated our approach on two datasets:
Multi-sentence reading comprehension (MultiRC), which is a reading comprehension dataset provided in the form of multiple-choice QA task (Khashabi et al., 2018a). Every question is based on a paragraph, which contains the gold justification sentences for each question. We use every sentence of the paragraph as candidate justifications for a given question. Here we use the original
6We used RoBERTa base with maximum sequence length of 512, batch size = 8, learning rate of 1e-5, and 5 number of epochs. RoBERTa-base always returned consistent performance on MultiRC experiments; many runs from RoBERTalarge failed to train (as explained by (Wolf et al., 2019)), and generated near random performance.
7We used similar hyperparameters as in the MultiRC experiments, but instead used RoBERTa-large, with maximum sequence length of 128.
8The dataset creators make a similar observation (Khot et al., 2019a).
MultiRC dataset,9 which includes the gold annotations for evidence text, unlike the version available on SuperGlue (Wang et al., 2019a).
Question Answering using Sentence Composition (QASC), a large KB-based multiple-choice QA dataset (Khot et al., 2019a). Each question is provided with 8 answer candidates, out of which 4 candidates are hard adversarial choices. Every question is annotated with a fixed set of two justification sentences for answering the question. The
9https://cogcomp.seas.upenn.edu/ multirc/
justification sentences are to be retrieved from a KB having 17.2 million facts. As shown in the example of fig. 1 and also highlighted by (Khot et al., 2019a), multiple evidence text are possible for a given question in QASC where the annotated gold justification sentences explain it more precisely.
We report overall question answering performance as well as evidence selection performance in table 1 for MultiRC, and table 2 for QASC10.
10https://leaderboard.allenai.org/qasc/ submissions/public",positive
114,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","In addition to previously-reported results, we include in the tables several in-house baselines. For MultiRC, we considered three baselines. The first baseline is where we feed all passage sentences to the RoBERTa classifier (row 11 in table 1). The second baseline uses the alignment method of (Kim et al., 2017) to retrieve the top k sentences (k = 2, 5). Since AIR uses the same alignment approach for retrieving justifications in each iteration, the comparison to this second baseline highlights the gains from our iterative process with query reformulation. The third baseline uses a supervised RoBERTa classifier trained to select the gold justifications for every query (rows 16ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ21 in table 1). Lastly, we also developed a RoBERTa-based iterative retriever by concatenating the query with the retrieved justification in the previous step. We retrain the RoBERTa iterative retriever in every step, using the new query in each step.
We considered two baselines for QASC. The first baseline does not include any justifications (row 7 in table 2). The second baseline uses the top k sentences retrieved by the alignment method
(row (8ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ12 in table 2).",positive
115,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","To understand the importance of modeling missing information in query reformulation, we analyzed a simple variant of AIR in which, rather the focusing on missing information, we simply concatenate the complete justification sentence to the query after each hop. To expose semantic drift, we retrieve a specified number of justification sentences. As seen in table 3, now the AIR(lexical)-uncontrolled and AIR-uncontrolled perform worse than both BM25 and the alignment method. This highlights that the focus on missing information during query reformulation is an important deterrent of semantic drift. We repeated the same experiment with the supervised RoBERTa retriever (trained iteratively for 2 steps) and the original parameter-free AIR, which decides its number of hops using the stopping conditions. Again, we observe similar performance drops in both: the RoBERTa retriever drops from 62.3% to 57.6% and AIR drops to 55.4%.",positive
116,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,"Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the corresponding QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) a stopping criterion that terminates retrieval when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.","We evaluate the sensitivity of AIR to the 2 hyper parameters: the threshold (Qr) for query expansion, and the cosine similarity threshold M in computation of alignment. As shown in table 5, evidence selection performance of AIR drops with the lower values of M but the drops are small, suggesting that AIR is robust to different M values.
Similarly, there is a drop in performance for MultiRC with the increase in the Qr threshold used for query expansion, hinting to the occurrence of semantic drift for higher values of Qr (table 4). This is because the candidate justifications are coming from a relatively small numbers of paragraphs in MultiRC; thus even shorter queries (= 2 words) can retrieve relevant justifications. On the other hand, the number of candidate justifications in QASC is much higher, which requires longer queries for disambiguation (>= 4 words).",positive
117,What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online, has made it impossible to fact-check every single suspicious claim, either manually or automatically. Alternatively, we can profile entire news outlets and look for those that are likely to publish fake or biased content. This approach makes it possible to detect likely ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfake newsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself on Twitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium on Facebook, Twitter, and YouTube). We further study (iii) what was written about the target medium on Wikipedia. The evaluation results show that what was written matters most, and that putting all information sources together yields huge improvements over the current state-of-the-art.","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3364ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3374 July 5 - 10, 2020. cÃÂÃÂÃÂÃÂ©2020 Association for Computational Linguistics
3364
Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online, has made it impossible to fact-check every single suspicious claim, either manually or automatically. Alternatively, we can profile entire news outlets and look for those that are likely to publish fake or biased content. This approach makes it possible to detect likely ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfake newsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the moment they are published, by simply checking the reliability of their source.
From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself on Twitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium on Facebook, Twitter, and YouTube). We further study (iii) what was written about the target medium on Wikipedia. The evaluation results show that what was written matters most, and that putting all information sources together yields huge improvements over the current state-of-the-art.",positive
118,What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online, has made it impossible to fact-check every single suspicious claim, either manually or automatically. Alternatively, we can profile entire news outlets and look for those that are likely to publish fake or biased content. This approach makes it possible to detect likely ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfake newsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself on Twitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium on Facebook, Twitter, and YouTube). We further study (iii) what was written about the target medium on Wikipedia. The evaluation results show that what was written matters most, and that putting all information sources together yields huge improvements over the current state-of-the-art.","While leveraging social information and temporal structure to predict the factuality of reporting of a news medium is not new (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016), modeling this at the medium level is a mostly unexplored problem. A popular approach to predict the factuality of a medium is to check the general stance of that medium concerning already fact-checked claims (Mukherjee and Weikum, 2015; Popat et al., 2017, 2018). Therefore, stance detection became an essential component in fact-checking systems (Baly et al., 2018b).
In political science, media profiling is essential for understanding media choice (Iyengar and Hahn, 2009), voting behavior (DellaVigna and Kaplan, 2007), and polarization (Graber and Dunaway, 2017). The outlet-level bias is measured as a similarity of the language used in news media to political speeches of congressional Republicans or Democrats, also used to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018).
Political bias was traditionally used as a feature for fact verification (Horne et al., 2018b). In terms of modeling, Horne et al. (2018a) focused on predicting whether an article is biased or not. Political bias prediction was explored by Potthast et al. (2018) and Saleh et al. (2019), where news articles were modeled as left vs. right, or as hyperpartisan vs. mainstream. Similarly, Kulkarni et al. (2018) explored the left vs. right bias at the article level, modeling both textual and URL contents of articles.
In our earlier research (Baly et al., 2018a), we analyzed both the political bias and the factuality of news media. We extracted features from several sources of information, including articles published by each medium, what is said about it on Wikipedia, metadata from its Twitter profile, in addition to some web features (URL structure and traffic information). The experiments on the Media Bias/Fact Check (MBFC) dataset showed that combining features from these different sources of information was beneficial for the final classification. Here, we expand this work by extracting new features from the existing sources of information, as well as by introducing new sources, mostly related to the social media context, thus achieving sizable improvements on the same dataset.
In follow-up work (Baly et al., 2019), we showed that jointly predicting the political bias and the factuality is beneficial, compared to predicting each of them independently. We used the same sources of information as in (Baly et al., 2018a), but the results were slightly lower. While here we focus on analyzing political bias and factuality separately, future work may analyze how the newly proposed features and sources affect the joint prediction.",positive
119,What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online, has made it impossible to fact-check every single suspicious claim, either manually or automatically. Alternatively, we can profile entire news outlets and look for those that are likely to publish fake or biased content. This approach makes it possible to detect likely ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfake newsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself on Twitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium on Facebook, Twitter, and YouTube). We further study (iii) what was written about the target medium on Wikipedia. The evaluation results show that what was written matters most, and that putting all information sources together yields huge improvements over the current state-of-the-art.","Table 3 demonstrates the evaluation results when using the proposed sources/features for the task of predicting the factuality of reporting of news media.
Similarly to the results for political bias prediction, rows 3ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ10 suggest that the features extracted from articles are more important than those coming from YouTube or from Twitter profiles, and that using BERT to encode the articles yields the best results. Note that overall, the results in this table are not as high as those for bias prediction. This reflects the level of difficulty of this task, and the fact that, in order to predict factuality, one needs external information or a knowledge base to be able to verify the published content.
The results in rows 11ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ16 show that combining the Twitter profile features with the BERT-encoded articles improves the performance over using the article text only.
Comparing rows 6 and 17 in Table 3, we can see that the Twitter follower features perform worse than using Twitter profiles features; this is the opposite of what we observed in Table 2. This makes sense since our main motivation to look at the followersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ profiles was to detect political bias, rather than factuality. Moreover, the metadata collected from media profiles about whether the corresponding account is verified, or its level of activity or connectivity (counts of friends and statuses) are stronger signals for this task.
Finally, rows 25ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ32 show the results for modeling combinations of the three aspects we are exploring in this paper. The best results are achieved using the best features selected from the What was written and the What was written about the target medium aspects, concatenated together. This combination achieves sizeable improvements compared to the baseline system from (Baly et al., 2018a): by +6.17 macro-F1 points absolute. This result indicates that looking at the audience of the medium is not as helpful for predicting factuality as it was for predicting political bias, and that looking at what was written about the medium on Wikipedia is more important for this task.",positive
120,Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information,"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard allwords English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.","Supervised WSD Supervised systems have to rely on expensive hand-labeled data to achieve good results (Pasini, 2020). The best approaches currently rely on neural networks. The model presented by Raganato et al. (2017) formulates the task as a token classification problem, with an LSTM with attention classifier producing a probability distribution over both words and senses. Subsequent work has shown that better results can be obtained by only having scores for senses or synsets (Vial et al., 2019). Shallower, simpler networks can achieve even better performances (Uslu et al., 2018).
Contextualized vectors can be exploited in token tagging architectures (Vial et al., 2019; Bevilacqua and Navigli, 2019; Hadiwinoto et al., 2019). However, purely supervised systems are dependent on the data they are trained on, therefore when some sense is underrepresented in the training corpus it is not easy for them to predict it.
LKBs in Supervised WSD More closely related to the core of our contribution, LKB information, such as natural language definitions of word meaning, can be exploited in neural token tagging architectures. For example, in GlossBERT (Huang et al., 2019) a pretrained BERT encoder is fed both the context sentence and the gloss, and is trained to predict whether the gloss correctly describes the use of the target word. Successful results have been obtained by encoding glosses in dense vectors (Luo et al., 2018).
In EWISE (Kumar et al., 2019), WSD is performed in a two-step process: first, gloss embeddings are produced through a training procedure that also takes into account the WordNetÃÂÃÂ¢ÃÂÃÂÃÂÃÂs graph structure; then, the gloss embeddings are scored via dot product with a contextual vector computed with an LSTM model, which is trained through regular categorical cross-entropy. Our work builds on top of EWISE in that it generalizes its sense vector dot product approach, but features a novel
mechanism that injects relational knowledge into the architecture through a simple additional sparse dot product operation. Moreover, we show that better performances can be obtained by training the output embedding matrix, and that different sense/synset vectors can be used to initialize the output embeddings.
Note that our approach is different from that of Vial et al. (2019), in that we do not conflate senses together through the use of WordNet hypernymy; rather, we mantain all the original meaning distinctions, and exploit the logit scores over the full vocabulary in a second, distinct step.",positive
121,Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information,"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard allwords English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.","The matrix multiplication in Equation 1 is wasteful during both training and inference, as it produces scores over the entire vocabulary V, even though the number of possible synsets is much smaller than the cardinality of V. Since the model is equally penalized by the cross-entropy loss when it gives a high score to a synset either related or unrelated to the correct one, there is little incentive to learn similar vectors for related synsets. Moreover, computing logits over the whole vocabulary does not bring any benefit in inference, as each score is computed independently, without taking into account connections between output classes.
We address this issue by devising an architecture, i.e., EWISER, that can inject into the network relatedness knowledge as encoded in an arbitrary graph, and use it in training as well as in inference.",positive
122,Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information,"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard allwords English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.","In our case, we build the graph and adjacency matrix A from the relations between synsets or senses in WordNet. As WordNet relations are not weighted, for every synset s we set AsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,s to 1/N , where N is the number of incoming connections. In this way we avoid imbalanced predictions towards synsets with more incoming connections.
We experiment with including different relations in A. Our base configuration includes similarity, verb group, and derivationally related3 edges. As for hypernymy and its inverse, hyponymy, we experiment with different possible ways of including them in A: (i) including only hypernymy (hyper); (ii) only hyponymy (hypo); (iii) both hypernymy
3We connect two synsets with a derivationally related edge if at least one pair of senses therein is connected via a derivationally related edge.
and hyponymy (hyper+hypo); (iv) the transitive closure over hypernymy (the set of relations that are obtained by following hypernymy paths) (hyper*); (v) the transitive closure over hypernymy and hyponymy (hyper+hypo*);
Informally, hypernymy and hyponymy correspond to different kinds of reasoning, which might be characterized as, respectively, inductive (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂif it is an electronic device, then it might be a mouseÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and deductive (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂif it is a mouse, then it is an electronic deviceÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). The closures are a way to flatten the hierarchy, thus enabling multi-hop reasoning by making the qs score dependent on the z scores for synsets whose path distance to s is greater than 1 in the original graph.
Fine-tuning the adjacency matrix If weights in A are frozen, every connected synset gives an equal contribution to the final score qs. However, it is also reasonable to assume that not all synsets are equally relevant. For example, the score for inanimate object should be less relevant than that for device for predicting the hardware meaning of mouse. Thus, we experiment on fine-tuning A by only updating non-zero weights.",positive
123,Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information,"Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set. As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard allwords English WSD evaluation benchmarks. On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.","We report in Table 2 the results of the evaluation of the use of synset embeddings for the initialization of the O output embeddings matrix.
In general, the approach enables much better F1 scores compared to the baseline, but is very dependent on the quality of the embeddings, and on whether they incorporate supervision from SemCor. When using Deconf, which uses the WordNet graph to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdeconflateÃÂÃÂ¢ÃÂÃÂÃÂÃÂ word-level Word2Vec vectors, with no use of training corpora, the O-freeze strategy produces the best result on No15ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, i.e., 72.2, with an absolute increase of 20 points over the baseline. However, O-freeze with Deconf also achieves the worst result on both ALL and No15, indicating that some form of biasing towards the most frequent synsets, which is an effect of corpus supervision, is required for the global evaluation. Fine-tuning O enables the model to obtain a decent
F1 score, with the exception ofO-thaw*, where the training run was underfitting. With LMMS, higher results are obtained, especially when freezing the weights. SensEmBERT with the LMMS backoff achieves the best results on both ALL and No15, with O-thaw* reaching at least 76.6 on ALL and No15. Probably due to the fact that SensEmBERT relies less on the supervision from SemCor, very strong results are obtained on No15ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as well, with a margin of over 12 points above the baseline.
As for the training scheme adopted, the best results are obtained from the freeze-then-thaw strategy with learning rate reduction (O-thaw*) and from the simple freezing of O. Thawing consistently raises the accuracy on ALL and No15, but lowers it on No15ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, meaning that the fine-tuning of O shifts the balance of the trade-off between performances on seen and unseen synsets to the benefit of the former. O-init still improves over the baseline, but is less effective than its alternatives.",positive
124,Generalizing Natural Language Analysis through Span-relation Representations,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","A large number of natural language processing (NLP) tasks exist to analyze various aspects of human language, including syntax (e.g., constituency and dependency parsing), semantics (e.g., semantic role labeling), information content (e.g., named entity recognition and relation extraction), or sentiment (e.g., sentiment analysis). At first glance, these tasks are seemingly very different in both the structure of their output and the variety of information that they try to capture. To handle these different characteristics, researchers usually use specially designed neural network architectures. In this paper we ask the simple questions: are the
task-specific architectures really necessary? Or with the appropriate representational methodology, can we devise a single model that can perform ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and achieve state-of-the-art performance on ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ a large number of natural language analysis tasks?
Interestingly, in the domain of efficient human annotation interfaces, it is already standard to use unified representations for a wide variety of NLP tasks. Figure 1 shows one example of the BRAT (Stenetorp et al., 2012) annotation interface, which has been used for annotating data for tasks as broad as part-of-speech tagging, named entity recognition, relation extraction, and many others. Notably, this interface has a single unified format that consists of spans (e.g., the span of an entity), labels on the spans (e.g., the variety of entity such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpersonÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlocationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), and labeled relations between the spans (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂborn-inÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). These labeled relations can form a tree or a graph structure, expressing the linguistic structure of sentences (e.g., dependency tree). We detail this BRAT format and how it can be used to represent a wide number of natural language analysis tasks in Section 2.
The simple hypothesis behind our paper is: if humans can perform natural language analysis in a single unified format, then perhaps machines can as well. Fortunately, there already exist NLP models that perform span prediction and prediction of relations between pairs of spans, such as the endto-end coreference model of Lee et al. (2017). We extend this model with minor architectural modifications (which are not our core contributions) and pre-trained contextualized representations (e.g.,
BERT; Devlin et al. (2019)1) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Consti.), semantic role labeling (SRL), aspect based sentiment analysis (ABSA), and opinion role labeling (ORL). While previous work has used similar formalisms to understand the representations learned by pretrained embeddings (Tenney et al., 2019a,b), to the best of our knowledge this is the first work that uses such a unified model to actually perform analysis. Moreover, we demonstrate that despite the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs simplicity, it can achieve comparable performance with special-purpose state-of-the-art models on the tasks above (Table 1). We also demonstrate that this framework allows us to easily perform multi-task learning (MTL), leading to improvements when there are related tasks to be learned from or data is sparse. Further analysis shows that dissimilar tasks exhibit divergent attention patterns, which explains why MTL is harmful on certain tasks. We have released our code and the General Language Analysis Datasets (GLAD) benchmark with 8 datasets covering 10 tasks in the BRAT format
1In contrast to work on pre-trained contextualized representations like ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019) that learn unified features to represent the input in different tasks, we propose a unified representational methodology that represents the output of different tasks. Analysis models using BERT still use special-purpose output predictors for specific tasks or task classes.
at https://github.com/neulab/cmu-multinlp, and provide a leaderboard to facilitate future work on generalized models for NLP.",positive
125,Generalizing Natural Language Analysis through Span-relation Representations,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","In this section, we explain how the BRAT format can be used to represent a large number of tasks. There are two fundamental types of annotations: span annotations and relation annotations. Given a sentence x = [w1, w2, ..., wn] of n tokens, a span annotation (si, li) consists of a contiguous span of tokens si = [wbi , wbi+1, ..., wei ] and its label li (li ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L), where bi/ei are the start/end indices respectively, and L is a set of span labels. A relation annotation (sj , sk, rjk) refers to a relation rjk (rjk ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R) between the head span sj and the tail span sk, where R is a set of relation types. This span-relation representation can easily express many tasks by defining L and R accordingly, as summarized in Table 2a and Table 2b. These tasks fall in two categories: span-oriented tasks, where the goal is to predict labeled spans (e.g., named entities in NER) and relation-oriented tasks, where the goal is to predict relations between two spans (e.g., relation between two entities in RE). For example, constituency parsing (Collins, 1997) is a span-oriented task aiming to produce a syntactic parse tree for a sentence, where each node of the tree is an individual span associated with a constituent label. Coreference resolution (Pradhan et al., 2012) is a relation-oriented task that links an expression to its mentions within or beyond a single sentence. Dependency parsing (KuÃÂÃÂÃÂÃÂbler et al.,
2009) is also a relation-oriented task that aims to relate a word (single-word span) to its syntactic parent word with the corresponding dependency type. Detailed explanations of all tasks can be found in Appendix A.
While the tasks above represent a remarkably broad swath of NLP, it is worth mentioning what we have not covered, to properly scope this work. Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work.",positive
126,Generalizing Natural Language Analysis through Span-relation Representations,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","We compare the SpanRel model with state-of-theart task-specific models by training on data from a single task. By doing so we attempt to answer the
research question ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcan a single model with minimal task-specific engineering achieve competitive or superior performance to other models that have been specifically engineered?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ We select competitive SOTA models mainly based on settings, e.g., single-task learning and end-to-end extraction of spans and relations. To make fair comparisons, token embeddings (GloVe, ELMo, BERT) and other hyperparameters (e.g., the number of antecedents in Coref. and the maximal span length in SRL) in our method are set to match those used by SOTA models, to focus on differences brought about by the model architecture.
As shown in Table 4, the SpanRel model achieves comparable performances as task-specific SOTA methods (regardless of whether the token representation is contextualized or not). This indicates that the span-relation format can generically represent a large number of natural language analysis tasks and it is possible to devise a single unified model that achieves strong performance on all of them. It provides a strong and generic baseline for natural language analysis tasks and a way to examine the usefulness of task-specific designs.",positive
127,Generalizing Natural Language Analysis through Span-relation Representations,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.","To demonstrate the benefit of the SpanRel model in MTL, we perform single-task learning (STL) and MTL across all tasks using end-to-end settings.3 Following Liu et al. (2019), we perform MTL+finetuning and show the results in separate columns of Table 5. Contextualized token representations yield significantly better results than GloVe on all tasks, indicating that pre-training on large corpora is almost universally helpful to NLP tasks. Comparing the results of MTL+fine-tuning with STL, we found that performance with GloVe drops on 8 out of 15 tasks, most of which are tasks with relatively sparse data. It is probably because the capacity of the GloVe-based model is too small to store all the patterns required by different tasks. The results of contextualized representations are mixed, with some tasks being improved and others remaining the same or degrading. We hypothesize that this is because different tasks capture different linguistic aspects, thus are not equally helpful to each other. Reconciling these seemingly different tasks in the same model might be harmful to some tasks.
3Span-based F1 is used as the evaluation metric in SemEval-2010 Task 8 and SemEval-2014 Task 4 as opposed to macro F1 and accuracy reported in the original papers because we aim at end-to-end extractions.
Notably, as the contextualized representations become stronger, the performance of MTL+FT becomes more favorable. 5 out of 15 tasks (NER, RE, OpenIE, SRL, ORL) observe statistically significant improvements (p-value < 0.05 with paired bootstrap re-sampling) with SpanBERT, a contextualized embedding pre-trained with span-based training objectives, while only one task degrades (ABSA), indicating its superiority in reconciling spans from different tasks. The GLAD benchmark provides a holistic testbed for evaluating natural language analysis capability.
Task Relatedness Analysis To further investigate how different tasks interact with each other, we choose five source tasks (i.e., tasks used to improve other tasks, e.g., POS, NER, Consti., Dep., and SRL) that have been widely used in MTL (Hashimoto et al., 2017; Strubell et al., 2018) and six target tasks (i.e., tasks to be improved, e.g., OpenIE, NER, RE, ABSA, ORL, and SRL) to perform pairwise multi-task learning.
We hypothesize that although language modeling pre-training is theoretically orthogonal to MTL (Swayamdipta et al., 2018), in practice their benefits tends to overlap. To analyze these two factors separately, we start with a weak representation GloVe to study task relatedness, then move to BERT to demonstrate how much we can still improve with MTL given strong and contextualized representations. As shown in Table 6 (GloVe), tasks are not equally useful to each other. Notably, (1) for OpenIE and ORL, multi-task learning with SRL improves the performance significantly, while other tasks lead to less or no improvements. (2) Dependency parsing and SRL are generic source tasks that are beneficial to most of the target tasks. This unified SpanRel makes it easy to perform MTL and decide beneficial source tasks.
Next, we demonstrate that our framework also provides a platform for analysis of similarities and differences between different tasks. Inspired by the intuition that the attention coefficients are somewhat indicative of a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs internal focus (Li et al., 2016; Vig, 2019; Clark et al., 2019), we hypothesize that the similarity or difference between attention mechanisms may be correlated with similarity between tasks, or even the success or failure of MTL. To test this hypothesis, we extract the attention maps of two BERT-based SpanRel models (trained on a source tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² and a target task t separately) over sentencesXt from the target task, and compute
their similarity using the Frobenius norm:
simk(t, tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 |Xt| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xÃÂÃÂ¢ÃÂÃÂÃÂÃÂXt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥Atk(x)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²k (x)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ F ,
where Atk(x) is the attention map extracted from the k-th head by running the model trained from task t on sentence x. We select OpenIE as the target task because it shows the largest performance variation when paired with different source tasks (34.0 - 38.8) in Table 6. We visualize the attention similarity of all heads in BERT (12 layers ÃÂÃÂÃÂÃÂ 12 heads) between two mutually harmful tasks (OpenIE/POS on the left) and between two mutually helpful tasks (OpenIE/SRL on the right) in Figure 2a. A common trend is that heads in higher layers exhibit more divergence, probably because they are closer to the prediction layer, thus easier to be affected by the end task. Overall, it can be seen that OpenIE/POS has much more attention divergence than OpenIE/SRL. A notable difference is that almost all heads in the last two layers of the OpenIE/POS models differ significantly, while some heads in the last two layers of the OpenIE/SRL models still behave similarly, providing evidence that failure of MTL can be attributed to the fact that dissimilar tasks requires different attention patterns. We further compute average attention similarities for all source tasks in Figure 2b, and we can see that there is a strong correlation (Pearson correlation of 0.97) between the attentions similarity and the
-5
0
12
la ye rs
heads 1 heads 121 12 1
(a) Attention similarity between OpenIE/POS (left), and between OpenIE/SRL (right) for all heads.
34
36
38
40
-2.3 -2.2 -2.1 -2
p e rf
o rm
a n c e
similarity
POSNER consti.
dep.SRL
(b) Correlation between attention similarity and MTL performance.
Figure 2: Attention-based task relatedness analysis.
performance of pairwise MTL, supporting our hypothesis that attention pattern similarities can be used to predict improvements of MTL.
MTL under Different Settings We analyze how token representations and sizes of the target dataset affect the performance of MTL. Comparing BERT and GloVe in Table 6, the improvements become smaller or vanish as the token representation becomes stronger, e.g., improvement on OpenIE with SRL reduces from 5.8 to 1.6. This is expected because both large-scale pre-training and MTL aim to learn general representations and their benefits tend to overlap in practice. Interestingly, some helpful source tasks become harmful when we shift from GloVe to BERT, such as OpenIE paired with POS. We conjecture that the gains of MTL might have already been achieved by BERT, but the task-specific characteristics of POS hurt the performance of OpenIE. We did not observe many tasks benefitting from MTL for the GloVe-based model in Table 5
because it is trained on all tasks (instead of two), which is beyond its limited model capacity. The improvements of MTL shrink as the size of the SRL datasets increases, as shown in Figure 3, indicating that MTL is useful when the target data is sparse.
Time Complexity Analysis Time complexities of span and relation prediction are O(l ÃÂÃÂÃÂÃÂ· n) and O(K2) = O(ÃÂÃÂÃÂÃÂ2 ÃÂÃÂÃÂÃÂ· n2) respectively for a sentence of n tokens (Section 3). The time complexity of BERT is O(L ÃÂÃÂÃÂÃÂ· n2), dominated by its L selfattention layers. Since the pruning threshold ÃÂÃÂÃÂÃÂ is usually less than 1, the computational overhead introduced by the span-relation output layer is much less than BERT. In practice, we observe that the training/testing time is mainly spent by BERT. For SRL, one of the most computation-intensive tasks with long spans and dense span/relation annotations, 85.5% of the time is spent by BERT. For POS, a less heavy task, the time spent by BERT increases to 98.5%. Another option for span prediction is to formulate it as a sequence labeling task, as in previous works (Lample et al., 2016; He et al., 2017), where time complexity is O(n). Although slower than token-based labeling models, span-based models offer the advantages of being able to model overlapping spans and use span-level information for label prediction (Lee et al., 2017).",positive
128,Character-Level Translation with Self-attention,"We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.1","The transformer (Vaswani et al., 2017) is an attention-driven encoder-decoder model that has achieved state-of-the-art performance on a number of sequence modeling tasks in NLP. Instead of using recurrence, the transformer uses only feedforward layers based on self-attention. The standard transformer architecture consists of six stacked encoder layers that process the input using selfattention and six decoder layers that autoregressively generate the output sequence.
The original transformer (Vaswani et al., 2017) computes a scaled dot-product attention by taking as input query Q, key K, and value V matrices:
Attention(Q,K, V ) = softmax ( QKTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
dk
) V,
where ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ dk is a scaling factor. For the encoder, Q, K and V are equivalent, thus, given an input sequence with length N , Attention performs N2 comparisons, relating each word position with the rest of the words in the input sequence. In practice, Q, K, and V are projected into different representation subspaces (called heads), to perform MultiHead Attention, with each head learning different word relations, some of which might be interpretable (Vaswani et al., 2017; Voita et al., 2019).
Intuitively, attention as an operation might not be as meaningful for encoding individual characters as it is for words, because individual character representations might provide limited semantic information for learning meaningful relations on the sentence level. However, recent work on language modeling (Al-Rfou et al., 2019) has surprisingly shown that attention can be very effective for modeling characters, raising the question of how well the transformer would work on characterlevel bilingual and multilingual translation, and what architectures would be suitable for this task. These are the questions this paper sets out to investigate.",positive
129,Character-Level Translation with Self-attention,"We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.1","To gain a better understanding of the multilingual models, we analyze their learned character alignments as inferred from the model attention probabilities. For each input language (e.g., FR), we compare the alignments learned by each of our multilingual models (e.g., FR + ES ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ EN model) to the alignments learned by the corresponding bilingual model (e.g., FR ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ EN). Our intuition is that the bilingual models have the greatest flexibility to learn high-quality alignments because they are not distracted by other input languages. Multilingual models, by contrast, might learn lower quality alignments because either (i) the architecture is not robust enough for multilingual training; or (ii) the languages are too dissimilar to allow for effective joint training, prompting the model to learn alternative alignment strategies to accommodate for all languages.
We quantify the alignments using canonical correlation analysis (CCA) (Morcos et al., 2018). First, we sample 500 random sentences from each of our UN testing datasets (FR, ES, or ZH) and then produce alignment matrices by extracting the encoder-decoder attention from the last layer of each model. We use CCA to project each alignment matrix to a common vector space and infer the correlation. We analyze our transformer and convtransformer models separately. Our results are in Figure 3, while Appendix B contains example alignment visualizations.
For similar source and target languages (e.g., the FR+ESÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN model), we observe a strong pos-
1595
itive correlation to the bilingual models, indicating that alignments can be simultaneously learned. When introducing a distant source language (ZH) in the training, we observe a drop in correlation, for FR and ES, and an even larger drop for ZH. This result is in line with our BLEU results from Section 5.1, suggesting that multilingual training on distant input languages is more challenging than multilingual training on similar input languages. The convtransformer is more robust to the introduction of a distant language than the transformer (p < 0.005 for FR and ES inputs, according to a one-way ANOVA test). Our results also suggest that more sophisticated attention architectures might need to be developed when training multilingual models on several distant input languages.",positive
130,Character-Level Translation with Self-attention,"We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.1","Tables 3, 4, and 5 contain example translations produced by our different bilingual and multilingual models trained on the UN datasets.
B Visualization of Attention
In Figures 4,5, 6 and 7 we plot example alignments produced by our different bilingual and multilingual models trained on the UN datasets, always testing on translation from FR to EN. The alignments are produced by extracting the encoderdecoder attention of the last decoder layer of our transformer/convtransformer models.
We observe the following patterns: (i) for bilingual translation (Figure 4), the convtransformer has a sharper weight distribution on the matching characters and words than the transformer; (ii) for multilingual translation of close languages (FR+ESÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, Figure 5), both transformer and convtransformer are able to preserve the word alignments, but the alignments produced by the convtransformer appear to be slightly less noisy; (iii) for multilingual translation of distant languages (FR+ZHÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, Figure 6), the character alignments of the transformer become visually much noisier and concentrate on a few individual characters, with many word alignments dissolving. The convtransformer character alignments remain more spread out, and word align-
1597
ment appears to be better preserved. This is another indication that the convtransformer is more robust for multilingual translation of distant languages. (iv) for multilingual translation with three inputs, where two of the three languages are close (FR+ES+ZHÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, Figure 7), we observe a similar pattern, with word alignments being better preserved by the convtransformer.
1598
1599
1600
1601
(a) transformer trained on FRÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, testing with FR as input.
(b) convtransformer trained on FRÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, testing with FR as input.
Figure 4: Example alignments produced by character-level models trained on FRÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN.
1602
(a) transformer trained on FR+ESÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, testing with FR as input.
(b) convtransformer trained on FR+ESÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, testing with FR as input
Figure 5: Example alignments produced by character-level models trained on FR+ESÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN.
1603
1604",positive
131,Cross-media Structured Common Space for Multimedia Event Extraction,"We introduce a new task, MultiMedia Event Extraction (ME), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","Traditional event extraction methods target a single modality, such as text (Wadden et al., 2019), images (Yatskar et al., 2016) or videos (Ye et al., 2015; Caba Heilbron et al., 2015; Soomro et al., 2012). However, the practice of contemporary journalism (Stephens, 1998) distributes news via multimedia. By randomly sampling 100 multimedia news articles from the Voice of America (VOA), we find that 33% of images in the articles contain visual objects that serve as event arguments and are not mentioned in the text. Take ÃÂÃÂ¢ÃÂÃÂÃÂÃÂThese authors contributed equally to this work.
1Our data and code are available at http://blender. cs.illinois.edu/software/m2e2
Figure 1 as an example, we can extract the Agent and Person arguments of the Movement.Transport event from text, but can extract the Vehicle argument only from the image. Nevertheless, event extraction is independently studied in Computer Vision (CV) and Natural Language Processing (NLP), with major differences in task definition, data domain, methodology, and terminology. Motivated by the complementary and holistic nature of multimedia data, we propose MultiMedia Event Extraction (M2E2), a new task that aims to jointly extract events and arguments from multiple modalities. We construct the first benchmark and evaluation dataset for this task, which consists of 245 fully annotated news articles.
We propose the first method, Weakly Aligned Structured Embedding (WASE), for extracting events and arguments from multiple modalities. Complex event structures have not been covered by existing multimedia representation methods (Wu et al., 2019b; Faghri et al., 2017; Karpathy and Fei-Fei, 2015), so we propose to learn a structured multimedia embedding space. More specifically, given a multimedia document, we represent each image or sentence as a graph, where each node represents an event or entity and each
edge represents an argument role. The node and edge embeddings are represented in a multimedia common semantic space, as they are trained to resolve event co-reference across modalities and to match images with relevant sentences. This enables us to jointly classify events and argument roles from both modalities. A major challenge is the lack of multimedia event argument annotations, which are costly to obtain due to the annotation complexity. Therefore, we propose a weakly supervised framework, which takes advantage of annotated uni-modal corpora to separately learn visual and textual event extraction, and uses an image-caption dataset to align the modalities.
We evaluate WASE on the new task of M2E2. Compared to the state-of-the-art uni-modal methods and multimedia flat representations, our method significantly outperforms on both event extraction and argument role labeling tasks in all settings. Moreover, it extracts 21.4% more event mentions than text-only baselines. The training and evaluation are done on heterogeneous data sets from multiple sources, domains and data modalities, demonstrating the scalability and transferability of the proposed model. In summary, this paper makes the following contributions:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We propose a new task, MultiMedia Event Extraction, and construct the first annotated
news dataset as a benchmark to support deep analysis of cross-media events.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We develop a weakly supervised training framework, which utilizes existing single-
modal annotated corpora, and enables joint inference without cross-modal annotation.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Our proposed method, WASE, is the first to leverage structured representations and
graph-based neural networks for multimedia common space embedding.",positive
132,Cross-media Structured Common Space for Multimedia Event Extraction,"We introduce a new task, MultiMedia Event Extraction (ME), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","Each input document consists of a set of images M = {m1,m2, . . . } and a set of sentences S = {s1, s2, . . . }. Each sentence s can be represented as a sequence of tokens s = (w1, w2, . . . ), where wi is a token from the document vocabulary W . The input also includes a set of entities T = {t1, t2, . . . } extracted from the document text. An entity is an individually unique object in
the real world, such as a person, an organization, a facility, a location, a geopolitical entity, a weapon, or a vehicle. The objective of M2E2is twofold:
Event Extraction: Given a multimedia document, extract a set of event mentions, where each event mention e has a type ye and is grounded on a text trigger word w or an image m or both, i.e.,
e = (ye, {w,m}).
Note that for an event, w and m can both exist, which means the visual event mention and the textual event mention refer to the same event. For example in Figure 1, deploy indicates the same Movement.Transport event as the image. We consider the event e as text-only event if it only has textual mention w, and as image-only event if it only contains visual mention m, and as multimedia event if both w and m exist.
Argument Extraction: The second task is to extract a set of arguments of event mention e. Each argument a has an argument role type ya, and is grounded on a text entity t or an image object o (represented as a bounding box), or both,
a = (ya, {t, o}) .
The arguments of visual and textual event mentions are merged if they refer to the same realworld event, as shown in Figure 1.",positive
133,Cross-media Structured Common Space for Multimedia Event Extraction,"We introduce a new task, MultiMedia Event Extraction (ME), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","In order to make the event and argument classifier shared across modalities, the image and text graph should be encoded to the same space. However, it is extremely costly to obtain the parallel text and image event annotation. Hence, we use event and argument annotations in separate modalities (i.e., ACE and imSitu datasets) to train classifiers, and simultaneously use VOA news image and caption pairs to align the two modalities. To this end, we learn to embed the nodes of each image graph close to the nodes of the corresponding caption graph, and far from those in irrelevant caption graphs. Since there is no ground truth alignment between the image nodes and caption nodes, we use image and caption pairs for weakly supervised training, to learn a soft alignment from each words to image objects and vice versa.
ÃÂÃÂÃÂÃÂ±ij = exp (wCi o C j ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp (w C i o C jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² ) , ÃÂÃÂÃÂÃÂ²ji =
exp (wCi o C j ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp (w C iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² oCj )
,
where wi indicates the i th word in caption sentence s and oj represents the j th object of image
m. Then, we compute a weighted average of softly aligned nodes for each node in other modality, i.e.,
wÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j
ÃÂÃÂÃÂÃÂ±ijo C j , o ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² j =
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
i
ÃÂÃÂÃÂÃÂ²jiw C i . (4)
We define the alignment cost of the image-caption pair as the Euclidean distance between each node to its aligned representation,
ÃÂÃÂ£ÃÂÃÂÃÂÃÂs,mÃÂÃÂ£ÃÂÃÂÃÂÃÂ = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
i
||wi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂw ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i|| 2 2 +
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j
||oj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ o ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² j || 2 2
We use a triplet loss to pull relevant image-caption pairs close while pushing irrelevant ones apart:
Lc = max(0, 1 + ÃÂÃÂ£ÃÂÃÂÃÂÃÂs,mÃÂÃÂ£ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ£ÃÂÃÂÃÂÃÂs,m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ),
where mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is a randomly sampled negative image that does not match s. Note that in order to learn the alignment between the image and the trigger word, we treat the image as a special object when learning cross-media alignment.
The common space enables the event and argument classifiers to share weights across modalities, and be trained jointly on the ACE and imSitu datasets, by minimizing the following objective functions:
Le = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
w
logP (ye|w)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
m
logP (ye|m),
La = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
t
logP (ya|t)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
o
logP (ya|o),
All tasks are jointly optimized:
L = Lv + Lr + Le + La + Lc",positive
134,Cross-media Structured Common Space for Multimedia Event Extraction,"We introduce a new task, MultiMedia Event Extraction (ME), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","In the test phase, our method takes a multimedia document with sentences S = {s1, s2, . . . } and images M = {m1,m2, . . . , } as input. We first generate the structured common embedding for each sentence and each image, and then compute pairwise similarities ÃÂÃÂ£ÃÂÃÂÃÂÃÂs,mÃÂÃÂ£ÃÂÃÂÃÂÃÂ. We pair each sentence s with the closest image m, and aggregate the features of each word of s with the aligned representation from m by weighted averaging:
wÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ³)wi + ÃÂÃÂÃÂÃÂ³w ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i, (5)
where ÃÂÃÂÃÂÃÂ³ = exp(ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂs,mÃÂÃÂ£ÃÂÃÂÃÂÃÂ) and wÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i is derived from m using Equation 4. We use wÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i to classify each
word into an event type and to classify each entity into a role with multimedia classifiers in Equation 2. To this end, we define tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i similar to w ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i but using ti and t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i. Similarly, for each image m we find the closest sentence s, compute the aggregated multimedia features mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² and oÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i , and feed into the shared classifiers (Equation 3) to predict visual event and argument roles. Finally, we corefer the cross-media events of the same event type if the similarity ÃÂÃÂ£ÃÂÃÂÃÂÃÂs,mÃÂÃÂ£ÃÂÃÂÃÂÃÂ is higher than a threshold.",positive
135,Cross-media Structured Common Space for Multimedia Event Extraction,"We introduce a new task, MultiMedia Event Extraction (ME), which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.","Our cross-media joint training approach successfully boosts both event extraction and argument role labeling performance. For example, in Figure 5 (a), the text-only model can not extract Jus-
7We do not use coreference clustering metrics because we only focus on mention-level cross-media event coreference instead of the full coreference in all documents.
tice.Arrest event, but the joint model can use the image as background to detect the event type. In Figure 5 (b), the image-only model detects the image as Conflict.Demonstration, but the sentences in the same document help our model not to label it as Conflict.Demonstration. Compared with multimedia flat embedding in Figure 6, WASE can learn structures such as Artifact is on top of Vehicle, and the person in the middle of Justice.Arrest is Entity instead of Agent.
Iraqi security forces search [Justice.Arrest] a civilian in the city of Mosul. People celebrate Supreme Court ruling on Same Sex Marriage in front of the Supreme Court in Washington.
Figure 5: Image helps textual event extraction, and surrounding sentence helps visual event extraction.
Flat Event Movement.Transport
Role Artifact = none
Ours Event Movement.Transport
Role Artifact = man
Flat Event Justice:ArrestJail
Role Agent = man
Ours Event Conflict.Attack
Role Entity = man
Figure 6: Comparison with multimedia flat embedding.",positive
136,KLEJ: Comprehensive Benchmark for Polish Language Understanding,"In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.","Architecture HerBERT is a multi-layer bidirectional Transformer. We use BERTBASE architecture configuration with 12 layers, 12 attention heads and hidden dimension of 768.
Loss We train HerBERT with a MLM objective. According to the updated version of BERT, we always mask all tokens corresponding to the randomly picked word. Whole word masking objective is more difficult to learn than predicting subword tokens (Joshi et al., 2019; Martin et al., 2019).
In the original BERT training setup tokens are masked statically during the text preprocessing phase. In HerBERT, we chose to use dynamic token masking, which follows the training setup of the RoBERTa model (Liu et al., 2019).
We decided not to use the NSP objective. Previous studies by Yang et al. (2019) and Liu et al. (2019) showed that this objective is too easy and does not improve performance on downstream tasks.
Data preprocessing We tokenize corpus data into subword tokens using BPE. We learn BPE splits on Wolne Lektury and a publicly available subset of National Corpus of Polish. We choose these two datasets because of their higher quality compared to the rest of our corpus. We limit the vocabulary size to 50k tokens.
Our datasets contain a lot of small fragments of coherent text that should be treated as separate documents. We remove degenerated documents that consist of less than 20 tokens from available corpora. Maximal segment length is 512 as it was originally proposed in BERT. We do not accumulate short examples into full 512 token segments because such sequences would be incoherent with frequent topic changes. The only exception to this rule is the Open Subtitles dataset, where subsequent parts of dialogues were connected to form larger documents. The aforementioned training
setup gives us a slightly better performance on downstream tasks than simply selecting all available data.
Hyperparameters We train HerBERT using Adam optimizer (Kingma and Ba, 2014) with parameters: ÃÂÃÂÃÂÃÂ²1 = 0.9, ÃÂÃÂÃÂÃÂ²2 = 0.999, = 10ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ8. We use learning rate burn-in over the first 500 steps, reaching a peak value of 10ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4; the learning rate is then linearly decayed for the rest of the training. We train the model with a batch size of 570. HerBERT was trained for 180k steps, without showing signs of saturation.",positive
137,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEqual Contribution
Branco, 2012), and machine translation (Hasler et al., 2017).
In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; AluÃÂÃÂÃÂÃÂ±ÃÂÃÂÃÂÃÂsio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility.
These limitations in evaluation data prevent studying modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we currently do not measure their performance in more abstractive scenarios, i.e. cases with substantial modifications to the original sentences.
In this paper we introduce ASSET (Abstractive Sentence Simplification Evaluation and Tuning), a new dataset for tuning and evaluation of automatic SS models. ASSET consists of 23,590 human simplifications associated with the 2,359 original sentences from TurkCorpus (10 simplifications per
original sentence). Simplifications in ASSET were collected via crowdsourcing (ÃÂÃÂÃÂÃÂ§ 3), and encompass a variety of rewriting transformations (ÃÂÃÂÃÂÃÂ§ 4), which make them simpler than those in TurkCorpus and HSplit (ÃÂÃÂÃÂÃÂ§ 5), thus providing an additional suitable benchmark for comparing and evaluating automatic SS models. In addition, we study the applicability of standard metrics for evaluating SS using simplifications in ASSET as references (ÃÂÃÂÃÂÃÂ§ 6). We analyse whether BLEU (Papineni et al., 2002) or SARI (Xu et al., 2016) scores correlate with human judgements of fluency, adequacy and simplicity, and find that neither of the metrics shows a strong correlation with simplicity ratings. This motivates the need for developing better metrics for assessing SS when multiple rewriting transformations are performed.
We make the following contributions:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A high quality large dataset for tuning and evaluation of SS models containing simplifications produced by applying multiple rewriting transformations.1
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ An analysis of the characteristics of the dataset that turn it into a new suitable benchmark for evaluation.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A study questioning the suitability of popular metrics for evaluating automatic simplifications in a multiple-transformation scenario.",positive
138,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","A few corpus studies have been carried out to analyse how humans simplify sentences, and to attempt to determine the rewriting transformations that are performed.
Petersen and Ostendorf (2007) analysed a corpus of 104 original and professionally simplified news articles in English. Sentences were manually aligned and each simplification instance was categorised as dropped (1-to-0 alignment), split (1-to-N), total (1-to-1) or merged (2-to-1). Some splits were further sub-categorised as edited (i.e. the sentence was split and some part was dropped) or different (i.e. same information but very different wording). This provides evidence that sentence splitting and deletion of information can be performed simultaneously.
1ASSET is released with a CC-BY-NC license at https://github.com/facebookresearch/ asset.
AluÃÂÃÂÃÂÃÂ±ÃÂÃÂÃÂÃÂsio et al. (2008) studied six corpora of simple texts (different genres) and a corpus of complex news texts in Brazilian Portuguese, to produce a manual for Portuguese text simplification (Specia et al., 2008). It contains several rules to perform the task focused on syntactic alterations: to split adverbial/coordinated/subordinated sentences, to reorder clauses to a subject-verb-object structure, to transform passive to active voice, among others.
Bott and Saggion (2011) worked with a dataset of 200 news articles in Spanish with their corresponding manual simplifications. After automatically aligning the sentences, the authors determined the simplification transformations performed: change (e.g. difficult words, pronouns, voice of verb), delete (words, phrases or clauses), insert (word or phrases), split (relative clauses, coordination, etc.), proximisation (add locative phrases, change from third to second person), reorder, select, and join (sentences).
From all these studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes.",positive
139,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest of the sentences left unchanged.
More diverse simplifications are available in the Newsela corpus (Xu et al., 2015), a dataset of 1,130 news articles that were each manually simplified
to up to 5 levels of simplicity. The parallel articles can be automatically aligned at the sentence level to train and test simplification models (AlvaManchego et al., 2017; SÃÂÃÂÃÂÃÂtajner et al., 2018). However, the Newsela corpus can only be accessed after signing a restrictive license that prevents publicly sharing train/test splits of the dataset, which impedes reproducibility.
Evaluating models on automatically-aligned sentences is problematic. Even more so if only one (potentially noisy) reference simplification for each original sentence is available. With this concern in mind, Xu et al. (2016) collected the TurkCorpus, a dataset with 2,359 original sentences from EW, each with 8 manual reference simplifications. The dataset is divided into two subsets: 2,000 sentences for validation and 359 for testing of sentence simplification models. TurkCorpus is suitable for automatic evaluation that involves metrics requiring multiple references, such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). However, Xu et al. (2016) focused on simplifications through lexical paraphrasing, instructing annotators to rewrite sentences by reducing the number of difficult words or idioms, but without deleting content or splitting the sentences. This prevents evaluating a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to perform a more diverse set of rewriting transformations when simplifying sentences. HSplit (Sulem et al., 2018a), on the other hand, provides simplifications involving only splitting for sentences in the test set of TurkCorpus. We build on TurkCorpus and HSplit by collecting a dataset that provides several manuallyproduced simplifications involving multiple types of rewriting transformations.",positive
140,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","In order to quantify the rewriting transformations, we computed several low-level features for all simplification instances using the tseval package (Martin et al., 2018):
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Number of sentence splits: Corresponds to the difference between the number of sentences in the simplification and the number of sentences in the original sentence. In tseval, the number of sentences is calculated using NLTK (Loper and Bird, 2002).
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Compression level: Number of characters in the simplification divided by the number of characters in the original sentence.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Replace-only Levenshtein distance: Computed as the normalised character-level Levenshtein distance (Levenshtein, 1966) for replace operations only, between the original sentence and the simplification. Replace-only Levenshtein distance is computed as follows (with o the original sentence and s the simplification):
replace ops(o, s)
min(len(o), len(s))
We do not consider insertions and deletions in the Levenshtein distance computation so that this feature is independent from the compression level. It therefore serves as a proxy for measuring the lexical paraphrases of the simplification.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Proportion of words deleted, added and reordered: Number of words deleted/reordered from the original sentence divided by the number of words in the original sentence; and the number of words that were added to the original sentence divided by the number of words in the simplification.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Exact match: Boolean feature that equals to true when the original sentence and the simplification are exactly the same, to account for unchanged sentences.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Word deletion only: Boolean feature that equals to true when the simplification is obtained only by deleting words from the original sentence. This feature captures extractive compression.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Lexical complexity score ratio: We compute the score as the mean squared log-ranks of content words in a sentence (i.e. without stopwords). We use the 50k most frequent words of the FastText word embeddings vocabulary (Bojanowski et al., 2016). This vocabulary was originally sorted with frequencies of words in the Common Crawl. This score is a proxy to the lexical complexity of the sentence given that word ranks (in a frequency table) have been shown to be best indicators of word complexity (Paetzold and Specia, 2016). The ratio is then the value of this score on the simplification divided by that of the original sentence.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Dependency tree depth ratio: We compute the ratio of the depth of the dependency parse tree of the simplification relative to that of the original sentence. When a simplification is composed by more than one sentence, we choose the maximum depth of all dependency trees. Parsing is performed using spaCy.4 This feature serves as a proxy to measure improvements in structural simplicity.
Each feature was computed for all simplification instances in the dataset and then aggregated as a histogram (Figure 1) and as a percentage (Table 3).",positive
141,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","Figure 1 shows the density of all features in ASSET, and compares them with those in TurkCorpus and
4github.com/explosion/spaCy
HSplit. Table 3 highlights some of these statistics. In particular, we report the percentage of sentences that: have at least one sentence split, have a compression level of 75% or lower, have at least one reordered word, are exact copies of the original sentences, and operated word deletion only (e.g. by removing only an adverb).
Sentence splits are practically non-existent in TurkCorpus (only 4.6% have one split or more), and are more present and distributed in HSplit. In ASSET, annotators tended to not split sentences, and those who did mostly divided the original sentence into just two sentences (1 split).
Compression is a differentiating feature of ASSET. Both TurkCorpus and HSplit have high density of a compression ratio of 1.0, which means that no compression was performed. In fact, HSplit has several instances with compression levels greater than 1.0, which could be explained by splitting requiring adding words to preserve fluency. In contrast, ASSET offers more variability, perhaps signalling that annotators consider deleting infor-
mation as an important simplification operation. By analysing replace-only Levenshtein distance, we can see that simplifications in ASSET paraphrase the input more. For TurkCorpus and HSplit, most simplifications are similar to their original counterparts (higher densities closer to 0). On the other hand, ASSETÃÂÃÂ¢ÃÂÃÂÃÂÃÂs simplifications are distributed in all levels, indicating more diversity in the rewordings performed. This observation is complemented by the distributions of deleted, added and reordered words. Both TurkCorpus and HSplit have high densities of ratios close to 0.0 in all these features, while ASSETÃÂÃÂ¢ÃÂÃÂÃÂÃÂs are more distributed. Moreover, these ratios are rarely equal to 0 (low density), meaning that for most simplifications, at least some effort was put into rewriting the original sentence. This is comfirmed by the low percentage of exact matches in ASSET (0.4%) with respect to TurkCorpus (16.3%) and HSplit (26.5%). Once again, it suggests that more rewriting transformations are being performed in ASSET.
In terms of lexical complexity, HSplit has a high density of ratios close to 1.0 due to its simplifications being structural and not lexical. TurkCorpus offers more variability, as expected, but still their simplifications contain a high number of words that are equally complex, perhaps due to most simplifications just changing a few words. On the other hand, ASSETÃÂÃÂ¢ÃÂÃÂÃÂÃÂs simplifications are more distributed across different levels of reductions in lexical complexity.
Finally, all datasets show high densities of a 1.0 ratio in dependency tree depth. This could mean that significant structural changes were not made, which is indicated by most instances corresponding
to operations other than splitting. However, ASSET still contains more simplifications that reduce syntactic complexity than TurkCorpus and HSplit.",positive
142,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","Preference judgments were crowdsourced with a protocol similar to that of the simplifications (ÃÂÃÂÃÂÃÂ§ 3.1).
Selecting Human Judges. Workers needed to comply with the same basic requirements as described in ÃÂÃÂÃÂÃÂ§ 3.1. For this task, the Qualification Test (QT) consisted in rating the quality of simplifications based on three criteria: fluency (or grammaticality), adequacy (or meaning preservation), and simplicity. Each HIT consisted of six originalsimplified sentence pairs, and workers were asked to use a continuous scale (0-100) to submit their level of agreement (0: Strongly disagree, 100: Strongly agree) with the following statements:
1. The Simplified sentence adequately expresses the meaning of the Original, perhaps omitting the least important information.
2. The Simplified sentence is fluent, there are no grammatical errors.
3. The Simplified sentence is easier to understand than the Original sentence.
Using continuous scales when crowdsourcing human evaluations is common practice in Machine Translation (Bojar et al., 2018; Barrault et al., 2019), since it results in higher levels of interannotator consistency (Graham et al., 2013). The six sentence pairs for the Rating QT consisted of:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Three submissions to the Annotation QT, manually selected so that one contains splitting, one has a medium level of compression, and one contains grammatical and spelling mistakes. These allowed to check that the particular characteristics of each sentence pair affect the corresponding evaluation criteria.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ One sentence pair extracted from WikiLarge (Zhang and Lapata, 2017) that contains several sentence splits. This instance appeared twice in the HIT and allowed checking for intra-annotator consistency.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ One sentence pair from WikiLarge where the Original and the Simplification had no relation to each other. This served to check the attention level of the worker.
All submitted ratings were manually reviewed to validate the quality control established and to select the qualified workers for the task.
Preference Task. For each of the 359 original sentences in the test set, we randomly sampled one reference simplification from ASSET and one from TurkCorpus, and then asked qualified workers to choose which simplification answers best each of the following questions:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Fluency: Which sentence is more fluent?
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Meaning: Which sentence expresses the original meaning the best?
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Simplicity: Which sentence is easier to read and understand?
Workers were also allowed to judge simplifications as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsimilarÃÂÃÂ¢ÃÂÃÂÃÂÃÂ when they could not determine which one was better. The same process was followed to compare simplifications in ASSET against those in HSplit. Each HIT consisted of 10 sentence pairs.",positive
143,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","Evaluation Metrics. We analysed the behaviour of two standard metrics in automatic evaluation of SS outputs: BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU is a precision-oriented metric that relies on the number of n-grams in the output that match n-grams in the references, independently of position. SARI measures improvement in the simplicity of a sentence based on the n-grams added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preservation (SÃÂÃÂÃÂÃÂtajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible,
5https://github.com/feralvam/easse
even though BLEU was designed as a corpus-level metric.
System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification.
Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transformations (e.g. sentence splitting, compression, paraphrases) would match that from human simplifications in ASSET. That was done so that we could obtain a sample that has variability in the types of rewritings performed. For each sentence pair (original and automatic simplification), we crowdsourced 15 human ratings on fluency (i.e. grammaticality), adequacy (i.e. meaning preservation) and simplicity, using the same worker selection criteria and HIT design of the Qualification Test as in ÃÂÃÂÃÂÃÂ§ 5.1.",positive
144,ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.","We computed the Pearson correlation between the normalised ratings and the evaluation metrics of our interest (BLEU and SARI) using ASSET or TurkCorpus as the set of references. We refrained from experimenting with HSplit since neither BLEU nor SARI correlate with human judgements when calculated using that dataset as references (Sulem et al., 2018a). Results are reported in Table 5.
BLEU shows a strong positive correlation with Meaning Preservation using either simplifications from ASSET or TurkCorpus as references. There is also some positive correlation with Fluency judgements, but that is not always the case for Simplicity: no correlation when using TurkCorpus and moderate when using ASSET. This is in line with previous studies that have shown that BLEU is not a good estimate for simplicity (Wubben et al., 2012; Xu et al., 2016; Sulem et al., 2018b).
In the case of SARI, correlations are positive but low with all criteria and significant only for simplicity with ASSETÃÂÃÂ¢ÃÂÃÂÃÂÃÂs references. Xu et al. (2016) showed that SARI correlated with human judgements of simplicity gain, when instructing judges to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂgrade the quality of the variations by identifying the words/phrases that are altered, and counting how many of them are good simplificationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.6 The judgements they requested differ from the ones we collected, since theirs were tailored to rate simplifications produced by lexical paraphrasing only. These results show that SARI might not be suitable for the evaluation of automatic simplifications with multiple rewrite operations.
In Table 6, we further analyse the human ratings collected, and compute their correlations with similar text features as in ÃÂÃÂÃÂÃÂ§ 4. The results shown re-
6https://github.com/cocoxu/ simplification/tree/master/HIT_MTurk_ crowdsourcing
inforce our previous observations that judgements on Meaning correlate with making few changes to the sentence: strong negative correlation with Levenshtein distance, and strong negative correlation with proportion of words added, deleted, and reordered. No conclusions could be drawn with respect to Simplicity.",positive
145,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Paraphrase generation (McKeown, 1983; Barzilay and Lee, 2003) has seen a recent surge of interest, both with large-scale dataset collection and curation (Lan et al., 2017; Wieting and Gimpel, 2018) and with modeling advances such as deep generative models (Gupta et al., 2018; Li et al., 2019). Paraphrasing models have proven to be especially useful if they expose control mechanisms that can be manipulated to produce diverse paraphrases (Iyyer et al., 2018; Chen et al., 2019b; Park et al., 2019), which allows these models to be employed for data augmentation (Yu et al., 2018) and
1Data and code are available at https://github. com/tagoyal/sow-reap-paraphrasing
adversarial example generation (Iyyer et al., 2018). However, prior methods involving syntactic control mechanisms do not effectively cover the space of paraphrase possibilities. Using syntactic templates covering the top of the parse tree (Iyyer et al., 2018) is inflexible, and using fully-specified exemplar sentences (Chen et al., 2019b) poses the problem of how to effectively retrieve such sentences. For a particular input sentence, it is challenging to use these past approaches to enumerate the set of reorderings that make sense for that sentence.
In this paper, we propose a two-stage approach to address these limitations, outlined in Figure 1. First, we use an encoder-decoder model (SOW, for Source Order reWriting) to apply transduction operations over various abstracted versions of the input sentence. These transductions yield possible reorderings of the words and constituents, which can be combined to obtain multiple feasible rearrangements of the input sentence. Each rearrangement specifies an order that we should visit words of the source sentence; note that such orderings could encourage a model to passivize (visit the object before the subject), topicalize, or reorder clauses. These orderings are encoded for our encoder-decoder paraphrase model (REAP, for REarrangement Aware Paraphrasing) by way of po-
sition embeddings, which are added to the source sentence encoding to specify the desired order of generation (see Figure 2). This overall workflow is inspired by the pre-ordering literature in machine translation (Xia and McCord, 2004; Collins et al., 2005); however, our setting explicitly requires entertaining a diverse set of possible orderings corresponding to different paraphrasing phenomena.
We train and evaluate our approach on the largescale English paraphrase dataset PARANMT-50M (Wieting and Gimpel, 2018). Results show that our approach generates considerably more diverse paraphrases while retaining the quality exhibited by strong baseline models. We further demonstrate that the proposed syntax-based transduction procedure generates a feasible set of rearrangements for the input sentence. Finally, we show that position embeddings provide a simple yet effective way to encode reordering information, and that the generated paraphrases exhibit high compliance with the desired reordering input.",positive
146,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Given an input sentence x = {x1, x2, . . . , xn}, our goal is to generate a set of structurally distinct paraphrases Y = {y1,y2, . . . ,yk}. We achieve this by first producing k diverse reorderings for the input sentence, R = {r1, r2, . . . , rk}, that guide the generation order of each corresponding y. Each reordering is represented as a permutation of the source sentence indices.
Our method centers around a sequence-tosequence model which can generate a paraphrase roughly respecting a particular ordering of the input tokens. Formally, this is a model P (y | x, r). First, we assume access to the set of target reorderings R and describe this rearrangement aware paraphrasing model (REAP) in Section 2.2. Then, in Section 2.3, we outline our reordering approach, including the source order rewriting (SOW) model, which produces the set of reorderings appropriate for a given input sentence x during inference (xÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R).",positive
147,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Let r = {r1, r2, . . . , rn} indicate the target reordering corresponding to the input tokens x. We want the model to approximately attend to tokens in this specified order when generating the final output paraphrase. For instance, in the example in Figure 1, the reordering specifies that when producing the paraphrase, the model should generate content related to the game before content related to Clippers in the output. In this case, based on the rearrangement being applied, the model will most likely use passivization in its generation, although this is not strictly enforced.
The architecture for our model P (y | x, r) is outlined in Figure 2. Consider an encoder-decoder architecture with a stack of M layers in the encoder
and N layers in the decoder. We make the target reordering r accessible to this transformer model through an additional set of positional embeddings PEr. We use the sinusoidal function to construct these following Vaswani et al. (2017).
Let EM = encoderM (x) be the output of the M th (last) layer of the encoder. The specialpurpose position embeddings are added to the output of this layer (see Figure 2): E = EM + PEr. Note that these are separate from standard position embeddings added at the input layer; such embeddings are also used in our model to encode the original order of the source sentence. The transformer decoder model attends over E while computing attention and the presence of the position embeddings should encourage the generation to obey the desired ordering r, while still conforming to the decoder language model. Our experiments in Section 4.3 show that this position embedding method is able to successfully guide the generation of paraphrases, conditioning on both the input sentence semantics as well as the desired ordering.",positive
148,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","We now outline our approach for generating these desired reorderings r. We do this by predicting phrasal rearrangements with the SOW model at various levels of syntactic abstraction of the sentence. We combine multiple such phrase-level rearrangements to obtain a set R of sentence-level rearrangements. This is done using a top-down approach, starting at the root node of the parse tree. The overall recursive procedure is outlined in Algorithm 1.
One step of the recursive algorithm has three
Algorithm 1 REORDER(t) Input: Sub-tree t of the input parse tree Output: Top-k list of reorderings for tÃÂÃÂ¢ÃÂÃÂÃÂÃÂs yield T = SELECTSEGMENTPAIRS(t) // Step 1 R = INITIALIZEBEAM(size = k) for (A,B) in T do z = REORDERPHRASE(t, A,B) // Step 2 RA(1, . . . , k) = REORDER(tA) // k orderings RB(1, . . . , k) = REORDER(tB) // k orderings for ra, rb in RA ÃÂÃÂÃÂÃÂRB do r = COMBINE(z, ra, rb) // Step 3 score(r) = score(z)+score(ra)+score(rb) R.push(r, score(r))
end for end for return R
major steps: Figure 3 shows the overall workflow for one iteration (here, the root node of the sentence is selected for illustration). First, we select sub-phrase pairs of the input phrase that respect parse-tree boundaries, where each pair consists of non-overlapping phrases (Step 1). Since the aim is to learn generic syntax-governed rearrangements, we abstract out the two sub-phrases, and replace them with non-terminal symbols, retaining only the constituent tag information. For example, we show three phrase pairs in Figure 3 that can be abstracted away to yield the reduced forms of the sentences. We then use a seq2seq model to obtain rearrangements for each abstracted phrase (Step 2). Finally, this top-level rearrangement is
combined with recursively-constructed phrase rearrangements within the abstracted phrases to obtain sentence-level rearrangements (Step 3).
Step 1: SELECTSEGMENTPAIRS We begin by selecting phrase tuples that form the input to our seq2seq model. A phrase tuple (t, A,B) consists of a sub-tree t with the constituents A and B abstracted out (replaced by their syntactic categories). For instance, in Figure 3, the S0, S, and VP2 nodes circled in red form a phrase tuple. Multiple distinct combinations of A and B are possible.2
Step 2: REORDERPHRASE Next, we obtain rearrangements for each phrase tuple (t, A,B). We first form an input consisting of the yield of t with A and B abstracted out; e.g. If S I will VP, shown in red in Figure 3. We use a sequence-to-sequence model (the SOW model) that takes this string as input and produces a corresponding output sequence. We then perform word-level alignment between the input and generated output sequences (using cosine similarity between GloVe embeddings) to obtain the rearrangement that must be applied to the input sequence.3 The log probability of the output sequence serves as a score for this rearrangement.
SOW model The SOW model is a sequence-tosequence model P (yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² | xÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², o), following the transformer framework in Section 2.1.4 Both xÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² and yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² are encoded using the word pieces vocabulary; additionally, embeddings corresponding to the POS tags and constituent labels (for non-terminals) are added to the input embeddings.
For instance, in Figure 3, If S I will VP and I will VP if S is an example of an (xÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²), pair. While not formally required, Algorithm 1 ensures that there are always exactly two non-terminal labels in these sequences. o is a variable that takes values MONOTONE or FLIP. This encodes a preference to keep the two abstracted nodes in the same order or to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂflipÃÂÃÂ¢ÃÂÃÂÃÂÃÂ them in the output.5 o is encoded in the model with additional positional encodings of the form {. . . 0, 0, 1, 0, . . . 2, 0 . . . } for monotone and
2In order to limit the number of such pairs, we employ a threshold on the fraction of non-abstracted words remaining in the phrase, outlined in more detail in the Appendix.
3We experimented with a pointer network to predict indices directly; however, the approach of generate and then align post hoc resulted in a much more stable model.
4See Appendix for SOW model architecture diagram. 5In syntactic translation systems, rules similarly can be divided by whether they preserve order or invert it (Wu, 1997).
{. . . 0, 0, 2, 0, . . . 1, 0 . . . } for flipped, wherein the non-zero positions correspond to the positions of the abstracted non-terminals in the phrase. These positional embeddings for the SOW MODEL are handled analogously to the r embeddings for the REAP model. During inference, we use both the monotone rearrangement and flip rearrangement to generate two reorderings, one of each type, for each phrase tuple.
We describe training of this model in Section 3.
Step 3: COMBINE The previous step gives a rearrangement for the subtree t. To obtain a sentence-level rearrangement from this, we first recursively apply the REORDER algorithm on subtrees tA and tB which returns the top-k rearrangements of each subtree. We iterate over each rearrangement pair (ra, rb), applying these reorderings to the abstracted phrases A and B. This is illustrated on the left side of Figure 3. The sentence-level representations, thus obtained, are scored by taking a mean over all the phrase-level rearrangements involved.",positive
149,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","To train our REAP model (outlined in Section 2.2), we take existing paraphrase pairs (x,yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and derive pseudo-ground truth rearrangements rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ of the source sentence tokens based on their alignment with the target sentence. To obtain these rearrangements, we first get contextual embeddings (Devlin et al., 2019) for all tokens in the source and target sentences. We follow the strategy outlined in Lerner and Petrov (2013) and perform reorderings as we traverse down the dependency tree. Starting at the root node of the source sentence, we determine the order between the head and its children (independent of other decisions) based on the order
of the corresponding aligned words in the target sentence. We continue this traversal recursively to get the sentence level-rearrangement. This mirrors the rearrangement strategy from Section 2.3, which operates over constituency parse tree instead of the dependency parse.
Given triples (x, rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ,yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), we can train our REAP model to generate the final paraphrases conditioning on the pseudo-ground truth reorderings.",positive
150,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","The PARANMT-50M dataset contains sentencelevel paraphrase pairs. However, in order to train our SOW model (outlined in section 2.3), we need to see phrase-level paraphrases with syntactic abstractions in them. We extract these from the PARANMT-50M dataset using the following procedure, shown in Figure 4. We follow Zhang et al. (2020) and compute a phrase alignment score between all pairs of constituents in a sentence and its paraphrase.6 From this set of phrase alignment scores, we compute a partial one-to-one mapping between phrases (colored shapes in Figure 4); that is, not all phrases get aligned, but the subset that do are aligned one-to-one. Finally, we extract aligned chunks similar to rule alignment in syntactic translation (Galley et al., 2004): when aligned phrases A and AÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² subsume aligned phrase pairs (B,C) and (BÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², C ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) respectively, we can extract the aligned tuple (tA, B,C) and (tAÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² , BÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², C ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²). The phrases (B,C) and (BÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², C ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) are abstracted out to construct training data for the phrase-level transducer, including supervision of whether o = MONOTONE or FLIP. Using the above alignment strategy, we were able to obtain over 1 million aligned phrase pairs.",positive
151,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Baselines We compare our model against the Syntactically Controlled Paraphrase Network (SCPN) model proposed in prior work (Iyyer et al., 2018). It produces 10 distinct paraphrase outputs conditioned on a pre-enumerated list of syntactic templates. This approach has been shown to outperform other paraphrase approaches that condition on interpretable intermediate structures (Chen et al., 2019b). Additionally, we report results on the following baseline models: i) A copy-input model that outputs the input sentence exactly. ii) A vanilla seq2seq model that uses the same transformer encoder-decoder architecture from Section 2.1 but does not condition on any target rearrangement. We use top-k sampling (Fan et al., 2018) to generate 10 paraphrases from this model.7 iii) A diversedecoding model that uses the above transformer seq2seq model with diverse decoding (Kumar et al., 2019) during generation. Here, the induced diversity is uncontrolled and aimed at maximizing metrics such as distinct n-grams and edit distance between the generated sentences. iv) A LSTM version of our model where the REAP model uses LSTMs with attention (Bahdanau et al., 2014) and copy (See et al., 2017) instead of transformers. We still use the transformer-based phrase transducer to obtain the source sentence reorderings, and still use positional encodings in the LSTM attention.
Similar to Cho et al. (2019), we report two types of metrics: 1. Quality: Given k generated paraphrases Y = {y1,y2 . . .yk} for each input sentence in the test set, we select yÃÂÃÂÃÂÃÂbest that achieves the best (oracle) sentence-level score with the ground truth paraphrase y. The corpus level evaluation is performed using pairs (yÃÂÃÂÃÂÃÂbest,y). 2. Diversity: We calculate BLEU or WER be7Prior work (Wang et al., 2019; Li et al., 2019) has shown that such a transformer-based model provides a strong baseline and outperforms previous LSTM-based (Hasan et al., 2016) and VAE-based (Gupta et al., 2018) approaches.
tween all pairs (yi,yj) generated by a single model on a single sentence, then macro-average these values at a corpus-level.
In addition to these metrics, we use the paraphrase similarity model proposed by Wieting et al. (2017) to compute a paraphrase score for generated outputs with respect to the input. Similar to Iyyer et al. (2018), we use this score to filter out low quality paraphrases. We report on the rejection rate according to this criterion for all models. Note that our diversity metric is computed after filtering as it is easy to get high diversity by including nonsensical paraphrase candidates that differ semantically.
Table 1 outlines the performance of the different models. The results show that our proposed model substantially outperforms the SCPN model across all quality metrics.8 Furthermore, our LSTM model also beats the performance of the SCPN model, demonstrating that the gain in quality cannot completely be attributed to the use of transformers. The quality of our full model (with rearrangements) is also comparable to the quality of the vanilla seq2seq model (without rearrangements). This demonstrates that the inclusion of rearrangements from the syntax-based neural transducer do not hurt quality, while leading to a substantially improved diversity performance.
The SCPN model has a high rejection score of 40.6%. This demonstrates that out of the 10 templates used to generate paraphrases for each sentence, on average 4 were not appropriate for the given sentence, and therefore get rejected. On the other hand, for our model, only 15.9% of the generated paraphrases get rejected, implying that the rearrangements produced were generally meaningful. This is comparable to the 12.7% rejection rate
8The difference in performance between our proposed model and baseline models is statistically significant according to a paired bootstrap test.
exhibited by the vanilla seq2seq model that does not condition on any syntax or rearrangement, and is therefore never obliged to conform to an inappropriate structure.
Finally, our model exhibits a much higher diversity within the generated paraphrases compared to the transformer seq2seq baseline. As expected, the SCPN model produces slightly more diverse paraphrases as it explicitly conditions the generations on templates with very different top level structures. However, this is often at the cost of semantic equivalence, as demonstrated by both quantitative and human evaluation (next section). A similar trend was observed with the diverse-decoding scheme. Although it leads to more diverse generations, there is a substantial decrease in quality compared to SOW-REAP and the seq2seq model. Moreover, the paraphrases have a higher rejection rate (21.3%), suggesting that diverse decoding is more likely to produce nonsensical paraphrases. A similar phenomenon is also reported by Iyyer et al. (2018), wherein diverse-decoding resulted in paraphrases with different semantics than the input.
Syntactic Exemplars In addition to SCPN, we compare our proposed model against the controllable generation method of Chen et al. (2019b). Their model uses an exemplar sentence as a syntactic guide during generation; the generated paraphrase is trained to incorporate the semantics of the input sentence while emulating the syntactic structure of the exemplar (see Appendix D for examples). However, their proposed approach depends on the availability of such exemplars at test time; they manually constructed these for their test set (800 examples). Since we do not have such example sentences available for our test data, we report results of our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance on their test data.
Note that Chen et al. (2019b) carefully curated the exemplar to be syntactically similar to the actual target paraphrase. Therefore, for fair comparison, we report results using the ground truth ordering (that similarly leverages the target sentence to obtain a source reordering), followed by the REAP model. This model (ground truth order + REAP) achieves a 1-best BLEU score of 20.9, outperforming both the prior works: Chen et al. (2019b) (13.6 BLEU) and SCPN (17.8 BLEU with template, 19.2 BLEU with full parse). Furthermore, our full SOWREAP model gets an oracle-BLEU (across 10 sentences) score of 23.8. These results show that our proposed formulation outperforms other controllable baselines, while being more flexible.",positive
152,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Table 2 provides examples of paraphrase outputs produced by our approach and SCPN. The examples show that our model exhibits syntactic diversity while producing reasonable paraphrases of the input sentence. On the other hand, SCPN tends to generate non-paraphrases in order to conform to a given template, which contributes to increased diversity but at the cost of semantic equivalence. In Table 3, we show the corresponding sequence of rules that apply to an input sentence, and the final generated output according to that input rearrangement. Note that for our model, on average, 1.8 phrase-level reorderings were combined to produce sentence-level reorderings (we restrict to a maximum of 3). More examples along with the input rule sequence (for our model) and syntactic templates (for SCPN) are provided in the Appendix.
Human Evaluation We also performed human evaluation on Amazon Mechanical Turk to evalu-
ate the quality of the generated paraphrases. We randomly sampled 100 sentences from the development set. For each of these sentences, we obtained 3 generated paraphrases from each of the following models: i) SCPN, ii) vanilla sequence-to-sequence and iii) our proposed SOW-REAP model. We follow earlier work (Kok and Brockett, 2010; Iyyer et al., 2018) and obtain quality annotations on a 3 point scale: 0 denotes not a paraphrase, 1 denotes that the input sentence and the generated sentence are paraphrases, but the generated sentence might contain grammatical errors, 2 indicates that the input and the candidate are paraphrases. To emulate the human evaluation design in Iyyer et al. (2018), we sample paraphrases after filtering using the criterion outlined in the previous section and obtain three judgements per sentence and its 9 paraphrase candidates. Table 4 outlines the results from the human evaluation. As we can see, the results indicate
that the quality of the paraphrases generated from our model is substantially better than the SCPN model.9 Furthermore, similar to quantitative evaluation, the human evaluation also demonstrates that the performance of this model is similar to that of the vanilla sequence-to-sequence model, indicating that the inclusion of target rearrangements do not hurt performance.",positive
153,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Next, we intrinsically evaluate the performance of our SOW model (Section 2.3). Specifically, given a budget of 10 reorderings, we want to understand how close our SOW model comes to covering the target ordering. We do this by evaluating the REAP model in terms of oracle perplexity (of the ground truth paraphrase) and oracle BLEU over these 10 orderings.
We evaluate our proposed approach against 3 systems: a) Monotone reordering {1, 2, . . . , n}. b) Random permutation, by randomly permuting the children of each node as we traverse down the constituency parse tree. c) Ground Truth, using the pseudo-ground truth rearrangement (outlined in Section 3) between the source and ground-truth target sentence. This serves as an upper bound for the reorderingsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance, as obtained by the recursive phrase-level transducer.
9The difference of our model performance with SCPN is statistically significant, while that with baseline seq2seq is not according to a paired bootstrap test.
Table 5 outlines the results for 10 generated paraphrases from each rearrangement strategy. Our proposed approach outperforms the baseline monotone and random reordering strategies. Furthermore, the SOW modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs oracle perplexity is close to that of the ground truth reorderingÃÂÃÂ¢ÃÂÃÂÃÂÃÂs perplexity, showing that the proposed approach is capable of generating a diverse set of rearrangements such that one of them often comes close to the target rearrangement. The comparatively high performance of the ground truth reorderings demonstrates that the positional embeddings are effective at guiding the REAP modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generation.",positive
154,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Finally, we evaluate whether the generated paraphrases follow the target reordering r. Note that we do not expect or want our REAP model to be absolutely compliant with this input reordering since the model should be able to correct for the mistakes make by the SOW model and still generate valid paraphrases. Therefore, we perform reordering compliance experiments on only the monotone reordering and the pseudo-ground truth reorderings (rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, construction outlined in Section 3), since these certainly correspond to valid paraphrases.
For sentences in the test set, we generate paraphrases using monotone reordering and pseudoground truth reordering as inputs to REAP. We get the 1-best paraphrase and compute the degree of rearrangement10 between the input sentence and
10Quantified by KendallÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Tau rank correlation between original source order and targeted/generated order. Higher
the generated sentence. In Figure 5, we plot this as a function of the target degree of rearrangement, i.e., the rearrangement between the input sentence x and the ground truth sentence yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The dotted line denotes the ideal performance of the model in terms of agreement with the perfect reordering rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The plot shows that the REAP model performs as desired; the monotone generation results in high KendallÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Tau between input and output. Conditioning on the pseudo-ground truth reorderings (rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) produces rearrangements that exhibit the same amount of reordering as the ideal rearrangement.",positive
155,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","Paraphrase Generation Compared to prior seq2seq approaches for paraphrasing (Hasan et al., 2016; Gupta et al., 2018; Li et al., 2018), our model is able to achieve much stronger controllability with an interpretable control mechanism. Like these approaches, we can leverage a wide variety of resources to train on, including backtranslation (Pavlick et al., 2015; Wieting and Gimpel, 2018; Hu et al., 2019) or other curated data sources (Fader et al., 2013; Lan et al., 2017).
Controlled Generation Recent work on controlled generation aims at controlling attributes such as sentiment (Shen et al., 2017), gender or political slant (Prabhumoye et al., 2018), topic (Wang et al., 2017), etc. However, these methods cannot achieve fine-grained control over a property like syntax. Prior work on diverse paraphrase generation can be divided into three groups: diverse decoding, latent variable modeling, and syntax-based. The first group uses heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search decoding (Vijayakumar et al., 2018; Kumar et al., 2019). The second group includes approaches that use uninterpretable latent variables to separate syntax and semantics (Chen et al., 2019a), perturb latent representations to enforce diversity (Gupta et al., 2018; Park et al., 2019) or condition on latent codes used to represent different re-writing patterns (Xu et al., 2018; An and Liu, 2019). Qian et al. (2019) uses distinct generators to output diverse paraphrases. These methods achieve some diversity, but do not control generation in an interpretable manner. Finally, methods that use explicit syntactic structures (Iyyer et al., 2018; Chen et al., 2019b) may try to force a
KendallÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Tau indicates lower rearrangement and vice-versa.
sentence to conform to unsuitable syntax. Phraselevel approaches (Li et al., 2019) are inherently less flexible than our approach.
Machine Translation Our work is inspired by pre-ordering literature in machine translation. These systems either use hand-crafted rules designed for specific languages (Collins et al., 2005; Wang et al., 2007) or automatically learn rewriting patterns based on syntax (Xia and McCord, 2004; Dyer and Resnik, 2010; Genzel, 2010; Khalilov and Simaan, 2011; Lerner and Petrov, 2013). There also exist approaches that do not rely on syntactic parsers, but induce hierarchical representations to leverage for pre-ordering (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011). In the context of translation, there is often a canonical reordering that should be applied to align better with the target language; for instance, head-final languages like Japanese exhibit highly regular syntax-governed reorderings compared to English. However, in diverse paraphrase generation, there doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt exist a single canonical reordering, making our problem quite different.
In concurrent work, Chen et al. (2020) similarly use an additional set of position embeddings to guide the order of generated words for machine translation. This demonstrates that the REAP technique is effective for other tasks also. However, they do not tackle the problem of generating plausible reorderings and therefore their technique is less flexible than our full SOW-REAP model.",positive
156,Neural Syntactic Preordering for Controlled Paraphrase Generation,"Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreorderÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.1","In Table 8, we provide examples of paraphrases generated by our system (SOW-REAP) and the baseline SCPN (Iyyer et al., 2018) system. We additionally include the phrase level transductions applied to obtain the sentence level reordering by our system (column 1) and the input template that the corresponding SCPN generation was conditioned on (Column 3).
251
F Implementation Details
The hyperparameters values used in REAP (see Table 9) and SOW (see Table 10) models. Note that we do not use coverage loss for the SOW model.",positive
157,Neural Temporal Opinion Modelling for Opinion Prediction on Twitter,"Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.","Social media platforms allow users to express their opinions online towards various subject matters. Despite much progress in sentiment analysis in social media, the prediction of opinions, however, remains challenging. Opinion formation is a complex process. An individualÃÂÃÂ¢ÃÂÃÂÃÂÃÂs opinion could be influenced by their own prior belief, their social circles and external factors. Existing studies often assume that socially connected users hold similar opinions. Social network information is integrated with user representations via weighted links and encoded using neural networks with attentions or more recently Graphical Convolutional Networks (GCNs) (Chen et al., 2016; Li and Goldwasser, 2019). This strand of work, including (Chen et al., 2018; Zhu et al., 2020; Del Tredici et al., 2019), leverages both the chronological tweet sequence and social networks to predict usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ opinions.
The majority of previous work requires a manual segmentation of a tweet sequence into equallyspaced intervals based on either tweet counts or
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂCorresponding author
time duration. Models trained on the current interval are used to predict usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ opinions in the next interval. However, we argue that such a manual segmentation may not be appropriate since users post tweets at different frequency. Also, the time interval between two consecutively published tweets by a user is important to study the underlying opinion dynamics system and hence should be treated as a random variable.
Inspired by the multivariate Hawkes process (Aalen et al., 2008; Du et al., 2016), we propose to model a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs posting behaviour by a temporal point process that when user u posts a tweet d at time t, they need to decide on whether they want to post a new topic/opinion, or post a topic/opinion influenced by past tweets either posted by other users or by themselves. We thus propose a neural temporal opinion model to jointly predict the time when the new post will be published and its associated stance. Instead of using the fixed formulation of the multivariate Hawkes process, the intensity function of the point process is automatically learned by a gated recurrent neural network. In addition, oneÃÂÃÂ¢ÃÂÃÂÃÂÃÂs neighbourhood context and the topics of their previously published tweets are also taken into account for the prediction of both the posting time and stance of the next tweet.
To the best of our knowledge, this is the first work to exploit the temporal point process for opinion prediction on Twitter. Experimental results on the two Twitter datasets relating to Brexit and US general election show that our proposed model outperforms existing approaches on both stance and posting time prediction.",positive
158,Neural Temporal Opinion Modelling for Opinion Prediction on Twitter,"Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.","We present in Figure 1 the overall architecture of our proposed Neural Temporal Opinion Model (NTOM). The input to the model at time step i
consists of userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs own tweet xi, bag-of-word representation xbi , time interval ÃÂÃÂÃÂÃÂi between the iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1th tweet and the ith tweet, user embedding u, and neighboursÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tweet queue {di,1, di,2, . . . , di,L}. At first, a Bi-LSTM layer is applied to extract features from input tweets. Then the neighborhood tweets are processed by a stacked Bi-LSTM/LSTM layer for the extraction of neighborhood context, which is fed into an attention module queried by the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs own tweet hi and topic zi. The output of attention module is concatenated with tweet representation, time interval ÃÂÃÂÃÂÃÂi, user representation u, and topic representation zi, which is encoded from xbi via a Variational Autoencoder (VAE). Finally, the combined representation is sent to a GRU cell, whose hidden state participates in computing the intensity function and the softmax function, for the prediction of the posting time interval and the stance label of the next tweet. In the following, we elaborate the model in more details: Tweet representation: Words in tweets are mapped to pre-trained word embeddings (Baziotis et al., 2017)1, which is specially trained for tweets. Then Bi-LSTM is used to generate the tweet representation. Topic extraction: The topic representation zi in Figure 1 captures the topic focus of the ith tweet. It is learned by VAE (Kingma and Welling, 2014), which approximates the intractable true posterior
1https://github.com/cbaziotis/ datastories-semeval2017-task4
by optimising the reconstruction error between the generated tweet and the original tweet. Specifically, we convert each tweet to the bag-of-word format weighted by term frequency, xbi , and feed it to two inference neural networks defined as fÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ and fÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ . These generate mean and variance of a Gaussian distribution from which the latent topic vector zi is sampled. Then the approximated posterior would be qÃÂÃÂÃÂÃÂ(zi|xbi) = N (zi|fÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x b i), fÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x b i)). To generate the observation xÃÂÃÂÃÂÃÂbi conditional on the latent topic vector zi, we define the generative network as pÃÂÃÂÃÂÃÂ(xbi |zi) = N (xbi |fÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(zi)), fÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(zi)). The reconstruction loss for the tweet xbi is then:
Lx=EqÃÂÃÂÃÂÃÂ(zi|xbi ) [log pÃÂÃÂÃÂÃÂ(xbi |zi)]ÃÂÃÂ¢ÃÂÃÂÃÂÃÂKL(qÃÂÃÂÃÂÃÂ(zi|xbi )||p(zi)) (1)
Neighbourhood Context Attention: To capture the influence from the neighbourhood context, we first input the neighboursÃÂÃÂ¢ÃÂÃÂÃÂÃÂ recent L tweets to an LSTM in a temporal ascending order. The output of the LSTM is weighed by the attention signals queried by the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ith tweet and topic:
ci = LÃÂÃÂ¢ÃÂÃÂÃÂÃÂ l=1 ÃÂÃÂÃÂÃÂ±lh c i,l (2)
ÃÂÃÂÃÂÃÂ±l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ exp([hTi , zTi ]tanh(Whhci,l +Wzzci,l)) (3)
where {hci,1, hci,2, . . . , hci,L} denotes the hidden state output of each tweet di,l in the neighbourhood context, zci,l denotes the associated topic, hi is the representation of the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs own tweet at time step i, and both Wh and Wz are weight matrices.
We use this attention mechanism to align the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs tweet to the most relevant part in the neighbourhood context. Our rationale is that a user would attend to their neighboursÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tweets that discuss similar topics. The attention output ci is then concatenated with a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs own tweet hi and the extracted topic zi. We further enrich the representation with the elapsed time ÃÂÃÂÃÂÃÂi between the posting time of the current tweet and the last posted tweet, and add a randomly initialised user vector u to distinguish the user from others. The final representation is passed to a GRU cell for the joint prediction of the posting time and stance label of the next tweet. Temporal Point Process: The goal of NTOM is to forecast the time gap till the next post, together with the stance label. Instead of modelling the time interval value based on regression analysis, we use the GRU (Cho et al., 2014) to simulate the temporal point process.
At each time step, the combined representation [ci, hi, zi, ÃÂÃÂÃÂÃÂi, u] is input to the GRU cell to iteratively update the hidden state taking into account the influence of previous tweets:
gi = fGRU (giÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1, ci, hi, zi, ÃÂÃÂÃÂÃÂi, u) (4)
where gi is the hidden state of GRU cell. Given gi, the intensity function is formulated as:
ÃÂÃÂÃÂÃÂ»ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(t) = ÃÂÃÂÃÂÃÂ»(t|Hi) = exp(bÃÂÃÂÃÂÃÂ» + vTÃÂÃÂÃÂÃÂ» gi + wÃÂÃÂÃÂÃÂ»t) (5)
Here, Hi summarises all the tweet histories up to tweet i, bÃÂÃÂÃÂÃÂ» denotes the base density level, the term vTÃÂÃÂÃÂÃÂ» gi captures the influence from all previous tweets and wÃÂÃÂÃÂÃÂ»t denotes the influence from the instant interval. The likelihood that the next tweet will be posted at the next interval ÃÂÃÂÃÂÃÂ given the history is:
fÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ) = ÃÂÃÂÃÂÃÂ»ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ) exp ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ« ÃÂÃÂÃÂÃÂ
0 ÃÂÃÂÃÂÃÂ»ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(t)dt
) (6)
The expectation for the occurrence of the next tweet can be estimated using:
ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂi+1 = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ« ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ· fÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ)dÃÂÃÂÃÂÃÂ (7)
Loss: We expect the predicted interval to be close to the actual interval as much as possible by minimising the Gaussian penalty function:
Ltime = 1
ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 2ÃÂÃÂÃÂÃÂ
exp (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂi+1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂi+1)2 2ÃÂÃÂÃÂÃÂ2 ) (8)
For the stance prediction we employ the crossentropy loss denoted as Lstan. The final objective function is computed as:
L = ÃÂÃÂÃÂÃÂ·Lx + ÃÂÃÂÃÂÃÂ²Ltime + ÃÂÃÂÃÂÃÂ³Lstan (9)
where ÃÂÃÂÃÂÃÂ·, ÃÂÃÂÃÂÃÂ² and ÃÂÃÂÃÂÃÂ³ are coefficients determining the contribution of various loss functions.",positive
159,Neural Temporal Opinion Modelling for Opinion Prediction on Twitter,"Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.","To investigate the effectiveness of the context attention that is queried by topics, we first select some example topics from the topic-word matrix in VAE. The label of each topic is manually assigned based on its associated top 10 words. Then we display a tweetÃÂÃÂ¢ÃÂÃÂÃÂÃÂs topic distribution together with its neighborhood tweetsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ topic distribution. We also visualize the attention weights assigned to the 3 neighborhood tweets.
Figure 3 illustrates the example topics, topic distribution and attention signals towards context tweets. Here, x2 and x4 denote a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs 2nd and 4th tweets respectively. The most recent 3 neighborhood tweets are denoted as d1, d2, d3. Blue in the leftmost separate column denotes the attention weights, and each row on top of T1, T2 and T3 denotes the topic distribution. It can be observed that the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs concerned topic shifts from immigration to Boris Johnson in 2 time steps. The drift also appears in the neighbourÃÂÃÂ¢ÃÂÃÂÃÂÃÂs tweets. Higher attention weights are assigned to the neighbourÃÂÃÂ¢ÃÂÃÂÃÂÃÂs tweets which share similar topical distribution as the user. We can thus infer that the topic vector does help select the most relevant neighborhood tweet.
x4
d3 d2 d1
well played tonight boris ! u absolutely smashed it ! #brexit
x2
d3 d2 d1
yes , open borders with no way of planning strains on nhsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
T1 T2 T3x2
bbc debate remain team has two aggressive bulliesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ vote leave on thursday ! make it our independence day ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦eu is a closed protectionist market we pay than we everÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
absolutely correct , so many reasons to vote leave then vote leave for the sake of the fishermen . vote leave #brexit and we will all still be europeans free to do them all",positive
160,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","An attention mechanism (or attender) takes as input a matrix of keys K := {kTs }nss=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnsÃÂÃÂÃÂÃÂd and a query qt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd, and outputs a probability mass function ÃÂÃÂÃÂÃÂ±t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rns that will weight a set of values V := {vTs } ns s=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RnsÃÂÃÂÃÂÃÂdv to generate a glimpse vector gt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rdv used for downstream tasks. Following Graves et al. (2014), it is useful to think of the attender as a memory access module, ÃÂÃÂÃÂÃÂ±t as the soft address and gt as the accessed vector.
gt := nsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s=1 vsattender(ks,qt) = VÃÂÃÂÃÂÃÂ±t (1)
Figure 2 illustrates attention in a recurrent seq2seq (Cho et al., 2014), which we will use for our experiments. Both the keys and the values correspond to the set of encoder hidden states
2Although the sentence length is a scalar, the temporary representations (outputs of a hidden layer) are high dimensional.
K = V = E := {eTs } ns s=1, while the query corresponds to the current decoder hidden state qt = dt.",positive
161,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","A location (or position) attention mechanism computes ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlocation-based addressingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (random access memory) that depend on the index of the key. It takes as input qt and outputs a location attention ÃÂÃÂÃÂÃÂ»t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rns . Intuitively, it decides which value to retrieve based on its index. For example, in German sentences, the verb goes at the end of the
sentence, after a subordinate clause. When translating from German to English, it might thus make sense to directly attend to the last word in the German source sentence after encoding a subordinate clause. There are many other cases where attending to words based on their positions seems important. E.g. translating from subject-object-verb to subject-verb-object languages, or understanding the emphasis in some languages.
Despite the importance of word ordering in natural language, location-based attention is not common in seq2seq frameworks. This is probably because content-based attention can emulate locationbased attention in the usual interpolation setting. Indeed, it can learn to encode a positional embedding in the hidden states of the encoder through some internal ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcounterÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. This counter is unlikely to work in the extrapolation regime,3 we, therefore, investigate other types of location-attention that could satisfy the extrapolation constraint.
Luong et al. (2015) proposed a location-based attention by using Equation 2 with a score that is independent of the key score(ks,qt) = wTqt. They restrict themselves to sequences of the same length, which is not of interest to our work. Such a mechanism could be extended to sequences of varying lengths but would still lack extrapolation capacity as the model still has to learn to embed the location of the index it wants to retrieve.
The Neural Turing Machine (Graves et al., 2014), post-processes the content attention by shifting its location by a predicted number of steps. We use a similar mechanism, which is extrapolatable due to the independence of the sequence length. Nevertheless, on its own, it does not allow positional-only patterns in variable-length sentences. For example, it cannot attend to the ith word irrespective of the sentence length. The same argument holds for other location-based attention developed for architectures with an external memory (Sukhbaatar et al., 2015).
More recently, many location-based attention have been proposed in self-attention mechanism. These methods are usually based on sinusoidal encodings (SE), which have been proposed to take into account the word positions while bypassing the need for recurrences in encoder-decoder frameworks. In this paper, we will consider the transformer and transformerXL (relative SE) attention,
3This assumption can depend on the architecture and the inductive bias it provides (Weiss et al., 2018). For our task, we found that the assumption held for both LSTM and GRU.
which are computed as follows.
score(ks,qt) := (ks+ps)T (qt+pt)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ d Transformer (Vaswani et al., 2017)
(kÃÂÃÂÃÂÃÂs+pÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂt)T (qÃÂÃÂÃÂÃÂt+b)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ d TransformerXL (Dai et al., 2019) (4) Where pt is a positional encoding with sinusoidals of different frequencies at every dimension. Although powerful, the sinusoidal encoding and its variants (Shaw et al., 2018; Dai et al., 2019) lack the ability to model location patterns that depend on general word position such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlook at the ith word (after ...)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in the extrapolation setting. Indeed, the sinusoidal encoding for any fixed offset pt+k is linear in pt but not in k.
Location-based processing of attention has also been proposed as a way of constraining contentbased attention to some (soft) window. Yang et al. (2018) achieve it by multiplying the content attention by the weights of a predicted Gaussian such that the model has an inductive bias towards attending to words that are close to each other. Sukhbaatar et al. (2019) use a piece-wise window to decrease the computational complexity of the model. These methods nevertheless solve a fundamentally different problem and do not allow location-only extrapolatable patterns of attention.",positive
162,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","The fact that humans generate and understand unbounded sentences with a finite experience is often used as proof of the principle of compositionality (Szab, 2017). Following this argument, methods that can extrapolate to longer sequences should exhibit some compositionality.
Based on this observation, we evaluate on a compositionality-specific artificial task, lookup tables (Liska et al., 2018), but extend it to better quantify extrapolation. 6 This task is especially interesting to us, as there is a clear notion of what a good attention pattern should look like, making it easy to qualitatively and quantitatively analyze attentive models. It is a well-controlled task, which allows us to uncover challenges that prevent models from extrapolating on real-world data.",positive
163,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","The lookup tables task consists in sequentially applying k pre-defined lookup table functions. The lookup tables are bijective mappings on the set of
6 The extended datasets as well as scripts to generate them can be found at https://github.com/ i-machine-think/machine-tasks/tree/ master/LongLookupTables
all 3-bit strings ti : {0, 1}3 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {0, 1}3. For example, if t1(000) = 110 and t2(110) = 100 then t2(t1(000)) = t2(110) = 100. Following Hupkes et al. (2018), we write the operations from left to right, as well as add the inputs and temporary steps to the targets. E.g. the previous example corresponds to the input 000 t1 t2 and the target 000 110 100.
General extrapolatable seq2seq models should be able to terminate by outputting an end of sentence token <eos>. We thus append <eos> to the targets and a full stop . to the inputs. 7
At each decoding step, the target only depends on the previous output and the current lookup table. E.g. the last decoding step of 000 t1 t2, only depends on the previous output 110 = t1(000) and the current table t2. The network thus has to learn the lookup table mappings and use the correct one at each step. The gold standard attention, therefore, corresponds to the position of the current lookup table. Table 1 illustrates a longer example and its correct attention.
The various train and test sets are generated by composing 6 random lookup tables t1, . . . , t6 that have as input and output one of the 23 = 8 possible 3-bit strings. Specifically, we use k = 1 . . . 4 composed tables in the training set, k = 2 . . . 4 for the interpolation test sets, and k = 5 . . . 9 for the extrapolation test sets.
There are 5 different extrapolation test sets, depending on their additional lengths compared to the maximum training examples (long 1, . . . , long 5). We randomly select only 5000 possible examples for each of these test sets.
For the interpolation test sets, we select 3000 examples from all possible input-output pairs.
The training set contains all other possible inputoutput pairs, approximately 10000 examples.",positive
164,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","In addition to enabling extrapolation, the temporary variables such as the weight given to each building block are very helpful for debugging the model and improving interpretability.
Figure 9 shows the output of a Mix Attender for the lookup tables with noisy start task. The input was sampled from the Long 4 test set. The topleft image shows the final attention. The top-right table shows the value of some interpretable variables at every decoding step. The bottom images correspond to the content and location attention.
The first decoding step uses location attention to attend to the first input. For the next three steps, the model outputs a mixing weight %(ÃÂÃÂÃÂÃÂ») ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 to focus on content attention. The content attention successfully finds the first non-noisy table (after !). 10 It then goes back to using the location attention with ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ±) = 1 and ÃÂÃÂÃÂÃÂ(1/n) = 1 to generate a diagonal attention. Finally, it predicts <eos> when attending to the end of the input ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
At each step, ÃÂÃÂÃÂÃÂ = minÃÂÃÂÃÂÃÂ as it does not need to attend to neighboring words for this task. %(ÃÂÃÂÃÂÃÂ») is never exactly 0 or 1, such that the model can easily learn to switch between content and location attention as it does not collapse to using a single form of attention.
9Some baselines outperformed it in the interpolation settings of specific tasks. Namely, the additive attention in the reversed task and transformer in the noisy task.
10A single step of content attention should be sufficient, but the model seems to consistently use three steps.",positive
165,Location Attention for Extrapolation to Longer Sequences,"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important, and how one could hope to tackle it. We then focus on a specific type of extrapolation, which is especially useful for natural language processing: generalization to sequences longer than those seen during training. We hypothesize that models with a separate contentand location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.","In this paper, we focused on one type of extrapolation, which is especially important in NLP: generalization to longer sequences. We propose a new location-based attention, and show that it can extrapolate better than previous models while learning various attention patterns.
Despite promising initial results, our model is still unable to extrapolate perfectly for harder tasks. By analyzing its behavior, we uncovered an interesting heuristic used by seq2seq models, namely that they keep track of a decoding ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcounterÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to know when to output the <eos> token. This is a bottleneck for extrapolation, suggesting that removing this heuristic is key to reaching perfect extrapolation and should be investigated in future work.
Once the <eos> problem is solved, we could test the model on real-world datasets. It would also be interesting to test such attention mechanisms in self-attentive seq2seq models without recurrence. Finally, as the location attender is not model dependent, it could be pretrained on complex location patterns and incorporated as a plug-and-play module to get extrapolatable position attention.
Taking a step back, we have shown that current
deep learning models with common attention mechanisms are unable to extrapolate well on seemingly straightforward tasks. This tends to be overlooked by the field due to standard benchmarks that can be solved using only interpolation. We hope that this paper acts as a reminder that extrapolation is a hard setting that has not been much investigated by the machine learning community. As current methods that memorize and learn superficial cues are unable to extrapolate while humans are, we believe that such a setting might help (and force) the field to come up with more human-like computational models that are capable of abstract reasoning.",positive
166,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method1.","companies have to pay much cost in this step to win in the war of talents.
To this end, we propose the task of Job Posting Generation (JPG) in this paper, and we cast it as a novel conditional text generation task that generates the job requirement paragraph. Exploiting the ubiquitous job posting data, we aim to automatically specify the level of necessary skills and generate fluent job requirements in a data-driven manner, as shown in Figure 1.
Although the JPG task is of great significance, the complexity of it poses several key challenges: 1) Generating job requirements needs to not only produce overall fluent text but also precisely organize the key content like skills and other information, which is very difficult to current neural systems. Especially, the long-text to long-text generation easily leads to information missing (Shen et al., 2019). 2) The key points of job descriptions and the skills of job requirements are complex many-tomany relations, which makes the mapping learning very difficult. 3) How to exploit the global information among the heterogeneous relations between basic company information and the professional
skills across the whole dataset is of great importance to generate high-quality job requirements.
To address these challenges, we focus on the richness and accuracy of skills in generated job requirements and propose a global Skill-Aware MultiAttention (SAMA) model for JPG task. Specifically, we devise a two-pass decoder to generate informative, accurate, and fluent job requirement paragraph. The first-pass decoder is to predict multiple skills according to the job description, which is a multi-label classification task (Zhang and Zhou, 2014). The second-pass decoder is to generate a complete text according to the predicted skill labels and the input text. Moreover, we build a skill knowledge graph to capture the global information in the whole job posting dataset in addition to the local information provided by the input. Through the skill knowledge graph, our model obtains the global prior knowledge to alleviate the misusing of skills. Extensive experiments are conducted to evaluate our model on real-world job posting data. The result demonstrates the effectiveness of the proposed method.
The main contributions of this paper can be summarized as follows:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We propose a novel task of job posting generation that is defined as the conditional generation given a job description and basic company information to generate a job requirement.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A data-driven generation approach SAMA is proposed to model the complex mapping relationships and generate informative and accurate job requirements.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We build a real-world job posting dataset and conducte extensive experiments to validate the effectiveness and superiority of our proposed approach.",positive
167,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method1.","Let D = {(Bi, Xi, Yi)}Ni=1 denote the dataset, where Xi = (xi,1, xi,2, ..., xi,m) is the word sequence of job description paragraph. Yi = (yi,1, yi,2, ..., yi,n) is the word sequence of job requirement paragraph, Bi = (b p i , b s i ) is the basic information, bp and bs are job position and company scale information, N is the size of dataset, m and n are the lengths of sequence Xi and Yi, respectively. The target of the JPG task is to estimate P (Yi|Xi, Bi), the conditional probability of a
To tackle the JPG task, we propose a global SkillAware Multi-Attention model, named SAMA. Figure 3 shows the overall architecture of SAMA.
Firstly, considering the importance of skill prediction in JPG, we decompose the probability P (Yi|Xi, Bi) into a two-stage generation process, including skill prediction and job requirement paragraph generation:
P (Yi|Xi, Bi) = P (Yi|Xi, Si, Bi)P (Si|Xi, Bi), (1) where Si = (si,1, si,2, ..., si,l) is a skill2 word sequence of its corresponding job requirement, l is the length of Si. Since Si and Bi are conditionally independent given Xi, we can derive that P (Si|Xi, Bi) = P (Si|Xi).
Secondly, for refining the skills, we leverage the global prior information by the skill knowledge graph Gs = (E1, R,E2) where E1 and E2 are the sets of head and tail entities and R is the set of relations. Given the basic information Bi and the skill knowledge graph Gs, we obtain a set of skills Oi = (oi,1, oi,2, ..., oi,k).
Oi = f(Bi, G s), (2)
where f is an invertible query function, which can ensure the one to one mapping relation between Bi and Oi.
2The details of how skills are extracted are described in Section 2.
Thirdly, to fuse the local and global informa-
tion, the probability P (Yi|Xi, Si, Bi) during the
text generation process is calculated as:
P (Yi|Xi, Si, Bi) = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ»)Plocal(Yi|Xi, Si, Bi) +ÃÂÃÂÃÂÃÂ»Pglobal(Yi|Xi, Si, Bi),
(3) where ÃÂÃÂÃÂÃÂ» is a hyperparameter that adjusts the balance of two probabilities.",positive
168,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method1.","The process of skill prediction only considers the local information, which results in some misusing of skills. To refine the skill of the generated job requirement, the global information is taken into account by the skill knowledge graph.
The skill entities are divided into G and P as described in Section 2. Here, the basic assumption is that a generic skill appears more frequently than a professional skill among all the job postings, because the professional skill contains more domain characters. We use a hyperparameter ÃÂÃÂÃÂÃÂ¸ as a threshold to divide the skills entities.
Given the basic information Bi = (b p i , b s i ), the set of skillsOi is obtained from the skill knowledge graph by the query function f . In detail, firstly, we obtain the set of entities that have the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂN.T.M.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ relation with bpi and the set of entities who have the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂINÃÂÃÂ¢ÃÂÃÂÃÂÃÂ relation with bsi . Secondly, we get the
intersection of the sets obtained in the first step. Finally, we keep the entities whose types are P.
we embed Oi as SÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i = (s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i,1, s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i,2, ..., s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i,k), and linearly combine it as a skill graph context vector Cndj by an attention mechanism:
ÃÂÃÂÃÂÃÂ ji = exp(gTjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 4sÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²i)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp(g T jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 4sÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) ; Cndj = kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂÃÂÃÂ ji s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² i,
(7) where W 4 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂdÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² are parameters, dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² is the dimensions of the word embeddings. Then a nonlinear transformation is applied to form the graph skill semantic representation Ind. The probability Pglobal(Yi|Xi, Si, Bi) from Vskill is computed via:
Indj = tanh(W 5[gj ;C nd j ;C rd j ]), (8)
Pglobal(yi,j = w|Xi, Si, Bi) ={ softmaxi(W 6Indj + b 6), w ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Oi
0, w /ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Oi ,
(9)
where g and Crd will be introduced in next section, W 5 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂ(2d+dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²), W 6 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R|Vskill|ÃÂÃÂÃÂÃÂd, b6 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R|Vskill| are trainable parameters.",positive
169,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method1.","Job requirement generation fuses multiple attention mechanisms from three aspects, job descriptions, predicted skills and skills from skill knowledge graph. The text decoder, based on another LSTM, aims to generate final word sequence. The hidden vector of text decoder is computed by gt = LSTM(etÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1, gtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1), where etÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 is the word embedding of the final generated target word at time step t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1. After obtaining g, a nonlinear transformation is applied to form the text decoder semantic representation Ird. The probability Plocal(Yi|Xi, Si, Bi) is computed via:
Irdj = tanh(W 7[ejÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1; gj ;C rd j ;C th j ]), (10)
Plocal(yi,j |Xi, Si, Bi) = softmaxi(W 8Irdj + b8), (11) where W 7 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂ2(d+dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²), W 8 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R|Vtext|ÃÂÃÂÃÂÃÂd, b8 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R|Vtext| are parameters, Vtext is the vocabulary of job requirement and Vskill is a subset of Vtext, both Crd and Cth are the context vectors generated by attention mechanisms. Specifically, Crd is a context vector computed similar as Cst because they
directly take input sequence into account.
ÃÂÃÂÃÂÃÂ²ji = exp(gTjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 9hi)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp(g T jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 9hiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) ; Crdj = mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂÃÂÃÂ²ji hi,
(12) where W 9 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂd.
In addition, the skills S generated by skill decoder are fed into the text decoder to guide the generation process. To obtain Cth, another attention model is leveraged:
ÃÂÃÂÃÂÃÂ³ji = exp(gTjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 10si)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² exp(g T jÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1W 10siÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) ; Cthj = lÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂÃÂÃÂ³ji si,
(13) where W 10 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂdÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² are parameters.
The generation probability P (Yi|Xi, Si, Bi) is the weighted sum of Plocal(Yi|Xi, Si, Bi) and Pglobal(Yi|Xi, Si, Bi) as in equation 3. As shown in equation 8 and equation 10, the vector Cth appears explicitly only in Plocal, which implies that Plocal puts emphasis on the skill prediction, i.e., the local information, while the vector Cnd appears explicitly only in Pglobal, which indicates that Pglobal focuses on the skills given by skill knowledge graph, i.e., the global prior knowledge.
In this way, SAMA considers not only the local information from the job description but also the global information from the skill knowledge graph.",positive
170,Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) that is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA. Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels. At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results. The proposed approach is evaluated on real-world job posting data. Experimental results clearly demonstrate the effectiveness of the proposed method1.","Many practical applications are modeled as generation tasks such as keyword extraction, headline generation, and response generation. Many generation tasks are formulated as Seq2Seq learning problems. Plenty of studies focused on the optimization of the Seq2seq model. For example, Lopyrev (2015) trained a Seq2Seq model with attention for headlines generation task. Xing et al. (2017) incorporated topic information into Seq2Seq by a joint attention mechanism to generate informative responses for chatbots. Meng et al. (2017) applied a Seq2seq model with a copy mechanism to a keyword extraction task.
However, models without explicit modeling the sentence planning have a great limitation in generating complex argument structures depending on hierarchy. Dong and Lapata (2018) decomposed the semantic parsing process into sketch generation and details filled-in and proposed a structure-aware neural architecture. Zhang et al. (2019) formulated outline generation task as a hierarchical structured prediction problem and proposed HiStGen. Puduppully et al. (2019) proposed a two-stage model which incorporates content selection and planning, for the data-to-text generation task.
Similar to the above researches, we proposed a hierarchical generation model, namely SAMA, which first labels the job description with multiple skills and then generates the job requirement paragraph, to tackle the JPG task. Different from prior arts, SAMA considered the global information across the whole dataset to generate high quality job requirements.",positive
171,Learning Dialog Policies from Weak Demonstrations,"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on outof-domain data.","The dialog manager (DM) is the brain of a taskoriented dialog system. Given the information it has received or gleaned from a user, it decides how to respond. Typically, this module is composed of an extensive set of hand-crafted rules covering the decision tree of a dialog (Litman and Allen, 1987; Bos et al., 2003). To circumvent the high development cost of writing and maintaining these rules there have been efforts to automatically learn a dialog manager using reinforcement learning (RL; Walker 2000; Young et al. 2013). RL solves problems of optimal control ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ where past predictions affect future states ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ making it well-suited to dialog management, in which a misstep by the agent can throw the whole dialog off course. But using RL to train a dialog manager is not straightforward, and is often hindered by large dialog state spaces and sparse rewards (Gao et al., 2019).
*equal contribution
Neural network-based deep RL (Mnih et al., 2015) mitigates the problem of large state spaces (Fatemi et al., 2016; Li et al., 2017) but it still struggles when the DM has to choose a response ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or action ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ across multiple domains (e.g. hotel and flight booking). In addition, deep RL performs poorly without regular feedback ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or reward ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ on the correctness of its decisions. In a dialog there is no obvious way to automatically quantify the appropriateness of each response, so RL training environments for dialog managers usually wait until conversation-end before assigning a reward based on whether the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs task, or goal, was completed.
An established way to deal with these difficulties is to guide the dialog manager with expert demonstrations during RL training (Lipton et al., 2018; Gordon-Hall et al., 2020), a high-level illustration of which is shown in Figure 1. This approach, however, requires a rule-based oracle to provide a suitable system response given a dialog state, and does not exploit the knowledge contained in the growing number of dialog datasets (Budzianowski et al., 2018; Rastogi et al., 2019).
In this paper, we address two key-questions that arise when training RL dialog agents with expert
demonstrations: (i) Can we move away from rulebased experts and use weaker, cheaper demonstrations to guide the RL dialog manager? (ii) Can we exploit information gathered during RL training to improve the demonstrator and bridge the domain gap between dialog data and the RL environment?
To answer the first question, we explore three methods based on Deep Q-learning from Demonstrations (DQfD; Hester et al. 2017) that use trained experts derived from progressively weaker data. Our first and strongest expert is a Full Label Expert (FLE) trained on a labeled, in-domain dataset to predict the next system response. Second, we train a Reduced Label Expert (RLE) to predict the type of the next system response, but not its exact nature. Finally our third expert is a No Label Expert (NLE) that does not rely on any annotation at all, but is instead trained on unlabeled user utterance and agent response sentences. We show that all three experts can be used to successfully train RL agents, and two of them even allow us to train without expensive and often hard to come-by fully annotated in-domain dialog datasets.
We address our second key question ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ how to improve the experts during RL training ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by presenting Reinforced Fine-tune Learning (RoFL), a fine-tuning algorithm inspired by Dataset Aggregation (DAgger; Ross et al. 2011). RoFL bridges the domain gap between dialog data and the RL environment by using the dialog transitions generated during training to update the expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs weights, adapting the previously learned knowledge to the learning environment. Our experiments show that RoFL training improves demonstrations gathered from the employed experts, giving a boost in RL performance and hastening convergence.",positive
172,Learning Dialog Policies from Weak Demonstrations,"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on outof-domain data.","It has been shown that DQfD successfully trains a dialog manager when its demonstrations come from either a rule-based, or strong pre-trained expert (Gordon-Hall et al., 2020). To avoid writing rules, and to exploit the knowledge contained in external datasets, we expand on previous work and adapt DQfD for use with three progressively weaker and cheaper experts. Furthermore, we introduce our RoFL algorithm, describing how we fine-tune the expert during RL training.
Full Label Expert We define a Full Label Expert (FLE) as a classifier trained on a human-tohuman in-domain dialog dataset to predict, given the conversation state, the next action. For such an expert, the action space of the dataset corresponds to the actions in the RL environment and, as a result, we can use the original DQfD large margin classification term as an auxiliary loss:
Laux(Q) =max aÃÂÃÂ¢ÃÂÃÂÃÂÃÂA [Q(s, a) + `(aE , a)]
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂQ(s, aE) (6)
where aE is the action the expert took in s, and `(aE , a) is 0 when the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs chosen action is the same as the action taken by the expert demonstrator, and a positive constant c otherwise:
`(aE , a) = { 0, if a = aE c, otherwise
(7)
This FLE approach is similar to the data-driven expert introduced by Gordon-Hall et al. (2020).
Reduced Label Expert A Full Label Expert is trained on fully-annotated in-domain data, but this is lacking for many domains, and is expensive to collect and label from scratch (Shah et al., 2018). However, although existing dialog datasets often differ in annotation, many share high-level system labels: inform and request. inform actions denote that the system provides information; request actions that the system asks for it. A system utterance from a hotel-booking dataset, e.g. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂThe Le Grand Hotel costs $48 per night, how many nights do you want to stay?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, could be labelled: [hotel-inform-price, hotel-request-duration], while a sentence from a taxi-booking dataset, e.g. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPlease let me know the dropoff location.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, could be annotated: taxi-request-dropoff. Although
the domain and type of information are different, all actions A in either dataset can be broadly partitioned into sets Areduced ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ A according to whether they inform, request, or do both.
We introduce a Reduced Label Expert (RLE) to take advantage of this common annotation format across diverse datasets. The RLE is a multilabel classifier that predicts the high-level annotation set Areduced ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or reduced label ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ of the next system action given the list sNL of the last few utterances in the dialog. The RLE is trained on a dialog dataset stripped down to inform, request, and other (for all other actions) annotations. Its architecture is outlined in Figure 2. The previous user utterances are passed through a recurrent encoder, for example an RNN. The final hidden state of the encoder is then passed through a multi-label classifier which uses the sigmoid function to score each reduced label.
Once trained, we use the RLE to guide the dialog manager during DQfD training. First we divide all environment actions into reduced label sets. For example, the inform set would consist of the environment actions that pertain to providing information to the user. Unlike the FLE, the RLE does not predict exact actions, so we uniformly sample an environment action from the predicted reduced label set aE ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Areduced to use as an expert demonstration when prefilling the replay buffer. For example, if the RLE predicts request the expert might take the action request-hotel-price. In order to use the expert in network updates, we reformulate the ` term in the DQfDÃÂÃÂ¢ÃÂÃÂÃÂÃÂs auxiliary loss to account for the expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs reduced label prediction:
`(Ardcd, st) = { 0, if ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¸(st) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Ardcd c, otherwise
(8)
Ardcd = RLE(sNL) (9)
The agent is penalized by a positive constant term c if the action predicted by its current policy ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¸ is not in the set of actions licensed by the RLE.
No Label Expert While the RLE enables the use of data not annotated for the target dialog environment, it still requires labeled dialog data. This raises the question: can we employ an expert that does not rely on annotations at all?
To address this challenge, we propose a No Label Expert (NLE) that uses an unannotated dialog dataset consisting of pairs of sentences (su, sa), representing user utterances and the corresponding agent responses. The goal of the NLE is to predict whether, for a given pair of sentences, sa is an appropriate response to su. In this regard, it resembles models used to predict textual inference (Bowman et al., 2015). The NLE architecture is outlined in Figure 3. The previous user utterance and a verbalized system response ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ generated by an NLG component ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are consecutively passed through a sentence embedder. Their encodings are then concatenated and passed through a network which scores how appropriate the response is given the utterance.
The NLE is trained on unannotated human-tohuman dialog datasets which are formatted into pairs of user utterances and agent responses. We treat these as positive instances, making the tacit assumption that in the data the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs reply is always relevant given a user utterance. As a result, the data lacks negative examples of irrelevant agent responses. This can be mitigated by artificially creating negative pairs (su, sÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²a) from the original data by pairing each user utterance su
with random agent sentences sÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²a, drawn uniformly from all agent responses that were not observed for the original su. Given such a dataset of positive and negative user-agent interactions, we train an NLE that learns to output 1 if a system response corresponds to the last user utterance, and 0 if it does not. Once trained, we use this NLE to guide the DQfD dialog manager.
When prefilling the replay buffer with expert demonstrations, we calculate the setAno label of all actions a whose verbalization sa leads to an NLE output that exceeds a threshold ÃÂÃÂÃÂÃÂ when taken as a response to the last user utterance su. We then use a random action from this set aE ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Ano label as the expert demonstration and place it in the replay buffer. We use a similar ` term in the auxiliary loss to the Reduced Label Expert, which penalizes the agent if the action a predicted by its current policy is not in the set of actions licensed by the expert, i.e., if a 6ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Ano label:
`(Ano lbl, st) = { 0, if ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¸(st) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Ano lbl c, otherwise (10)
Ano lbl = {a | NLE([su; sa]) > ÃÂÃÂÃÂÃÂ} (11)
where ÃÂÃÂÃÂÃÂ is between 0 and 1 and c is a positive constant penalty factor.
Domain Adaptation through Fine-tuning We train our experts on dialog datasets created by humans talking to humans. This data is necessarily drawn from a different distribution to the transition dynamics of an RL environment. In other words, there is a domain gap between the two.
We seek to narrow this gap by introducing Reinforced Fine-tune Learning (RoFL): For d pretraining steps, transitions are generated according to a weak expert policy ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¾ÃÂÃÂÃÂÃÂ , where the weak expert ÃÂÃÂÃÂÃÂ¾ has parameters ÃÂÃÂÃÂÃÂ. If a transitionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs reward exceeds a threshold th, we treat it as in-domain data and add it to a buffer D. Every ÃÂÃÂÃÂÃÂ· steps the expert is fine-tuned on the in-domain data gathered so far and its parameters are updated. At the end of pretraining the final fine-tuned expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs weights are frozen and its policy is used to generate demonstration transitions for another d steps. This ensures that the permanent, demonstration portion of the replay buffer is filled with transitions from the fine-tuned expert. RoFL is agnostic to the expert in question and we apply it to each of our methods described above.
Algorithm 1: Reinforced Fine-tune Learning Inputs: expert network ÃÂÃÂÃÂÃÂ¾ with pre-trained parameters ÃÂÃÂÃÂÃÂ, fine-tune interval k, a reward threshold th, number of pre-training steps d, target network update rate ÃÂÃÂÃÂÃÂ , training interval ÃÂÃÂÃÂÃÂ· Initialize: random Q-network weights ÃÂÃÂÃÂÃÂ¸, random target network weights ÃÂÃÂÃÂÃÂ¸ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², replay buffer B = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, fine-tune data set D = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
for t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1, 2, ...d do Get conversational state st Sample action from expert policy aE ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ¾ÃÂÃÂÃÂÃÂ(st) Take action aE and observe (st+1, rt) B ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ B ÃÂÃÂ¢ÃÂÃÂÃÂÃÂª (st, aE , rt, st+1) if rt > th then D ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ D ÃÂÃÂ¢ÃÂÃÂÃÂÃÂª (st, aE) if t mod k = 0 then
ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ argminÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(s,aE)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂD aE log ÃÂÃÂÃÂÃÂ¾ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²(s)
end if t mod ÃÂÃÂÃÂÃÂ· = 0 then train()
end for t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1, 2, ... do
Get conversational state st Sample action from behavior policy at ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ ÃÂÃÂÃÂÃÂ QÃÂÃÂÃÂÃÂ¸ (st) Take action at and observe (st+1, rt) B ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ B ÃÂÃÂ¢ÃÂÃÂÃÂÃÂª (st, at, rt, st+1) if t mod ÃÂÃÂÃÂÃÂ· = 0 then train()
end Procedure train()
Sample transitions from B Calculate loss L(Q) Perform a gradient step to update ÃÂÃÂÃÂÃÂ¸ if t mod ÃÂÃÂÃÂÃÂ = 0 then ÃÂÃÂÃÂÃÂ¸ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ¸",positive
173,Learning Dialog Policies from Weak Demonstrations,"Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators. We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on outof-domain data.","In this paper, we have shown that weak demonstrations can be leveraged to learn an accurate dialog manager with Deep Q-Learning from Demonstrations in a challenging multi-domain environment. We established that expert demonstrators can be trained on labeled, reduced-labeled, and unlabeled data and still guide the RL agent by means of their
respective auxiliary losses. Evaluation has shown that all experts exceeded the performance of reinforcement and supervised learning baselines, and in some cases even approached the results of a hand-crafted rule-based dialog manager.
Furthermore, we introduced Reinforced Finetune Learning (RoFL) a DAgger-inspired extension to DQfD which allows a pre-trained expert to adapt to an RL environment on-the-fly, bridging the domain-gap. Our experiments show that RoFL training is beneficial across different sources of demonstration data, boosting both the rate of convergence and final system performance. It even enables an expert trained on unannotated out-ofdomain data to guide an RL dialog manager in a challenging environment.
In future, we want to continue to investigate the possibility of using even weaker demonstrations. Since our No Label Expert is trained on unannotated data, it would be interesting to leverage large and noisy conversational datasets drawn from message boards or movie subtitles, and to see how RoFL training fares with such a significant domain gap between the data and the RL environment.",positive
174,"Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection","In this paper we demonstrate how codeswitching patterns can be utilised to improve various downstream NLP applications. In particular, we encode different switching features to improve humour, sarcasm and hate speech detection tasks. We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.","In this section, we identify how switching behavior is related to the three NLP tasks at our
4Gloss: said aib filthy pandit ji, whatever you are telling is it pure sanskrit? irony shameonyou.
5irony bappi lahiri sings Gloss: doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt matter you do not get gold or silver, you have got a friend to love.
6The dataset released by this paper only had the hate/nonhate tags for each tweet. However, the language tag for each word required for our experiments was not available. Two of the authors independently language tagged the data and obtained an agreement of 98.1%. While language tagging, we noted that the dataset is a mixed bag including hate speech, offensive and abusive tweets which have already been shown to be different in earlier works (Waseem et al., 2017). However, this was the only Hindi-English code-mixed hate speech dataset available.
7Gloss: I hate my university. Someone burn that place.
hand. Let Q be the property that a sentence has en words which are surrounded by hi words, that is there exists an English word in a Hindi context. For instance, the tweet koi hi to hi pray en karo hi mere hi liye hi bhi hi satisfies the property Q. However, bumrah hi dono hi wicketo hi ke hi beech hi gumrah hi ho hi gaya hi does not satisfy Q.
We performed a statistical analysis to determine the correlation between the switching patterns and a classification task at hand (represented by T ). Let us denote the probability that a tweet belongs to a positive class for a task T given that it satisfies property Q by p(T |Q). Similarly, let p(T | ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Q) be the probability that the tweet belongs to the positive class for task T and does not satisfy the property Q.
Further let avg(S|T ) be the average switching in positive samples for the task T and avg(S| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ T ) denote the average switching in negative samples for the task T .
The main observations from this analysis for the three tasks ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ humour, sarcasm and hate are noted in Table 2. For the humour task, p(humour|Q) dominates over p(humour| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Q). Further the average number of switching for the positive samples in the humour task is larger than the average number of switching for the negative samples. Finally, we observe a positive PearsonÃÂÃÂ¢ÃÂÃÂÃÂÃÂs correlation coefficient of 0.04 between a text being humorous and the text having the property Q. This together indicates that the switching behavior has a positive connection with a tweet being humorous.
On the other hand p(sarcasm| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Q) as well as p(hate| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ Q) respectively dominate over p(sarcasm|Q) and p(hate|Q). Moreover the average number of switching for the negative samples for both these tasks is larger than the average number of switching for the positive samples. The PearsonÃÂÃÂ¢ÃÂÃÂÃÂÃÂs correlation between a text being sarcastic (hateful) and the text having the property Q is negative: -0.17 (-0.04). This shows there is an overall negative connection between the switching behavior and sarcasm/hate speech detection tasks.
While we have tested on one language pair (HindiEnglish), our hypothesis is generic and has been already noted by linguists earlier (VizcaÃÂÃÂÃÂÃÂ±ÃÂÃÂÃÂÃÂno, 2011).",positive
175,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4958ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4968 July 5 - 10, 2020. The concept of Quantum Annealing, a computational optimization technique, is revolutionizing the way we solve complex problems, particularly in the field of machine learning.. Meanwhile, the research on Graph Neural Networks is paving the way for deeper understanding and efficient processing of structured data, such as social networks and molecules.. cÃÂÃÂÃÂÃÂ©2020 Association for Computational Linguistics
4958
Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from.
Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.. In the realm of computer science, the study of knapsack problems has seen significant advancements, offering solutions for optimal packing of items with limited capacity.",negative
176,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The rise of quantum computing has opened up new possibilities for optimizing complex algorithms, potentially revolutionizing the way we approach data encryption and decryption.. In the realm of computer science, the study of knapsack problems and their solutions has been a fascinating area of research, leading to the development of dynamic programming algorithms.. The concept of self-organizing maps, a type of artificial neural network, has been instrumental in the field of data visualization, providing a unique way to represent high-dimensional data in a lower-dimensional space.. It is well-documented that neural dialogue models struggle with generating engaging, relevant responses (Li et al., 2016a) and often produce banal responses such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂYeah.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ While this may be an appropriate response to a chitchat conversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates ÃÂÃÂ¢ÃÂÃÂÃÂÃÂYeah,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNo,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂI donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt knowÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is not diverse and is not be engaging to converse with.
Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build
upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations.
Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some crowdworkers are better at producing certain types of data than others.
This paper introduces Diversity-Informed Data Collection (DIDC), a new strategy for creating a dataset of conversational utterances via selecting which participantsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ data to include in the collection. The strategy progressively builds up a more diverse sub-corpus from an existing larger collection. The main idea is to grow the sub-corpus by adding conversations sequentially and to assess the contribution of a new participantÃÂÃÂ¢ÃÂÃÂÃÂÃÂs utterances to the diversity of the entire sub-corpus. This strategy is also applicable to on-the-fly collection of new datasets via crowdworking or similar methods. We implement DIDC with three diversity metrics: Outlier, Entropy, and Mean-IDF.
Diversity-Informed Data Collection also provides a new method for finding an upper bound on a current corpusÃÂÃÂ¢ÃÂÃÂÃÂÃÂs diversity via a Corpus-Wide Oracle which has access to information about which utterances are most diverse across the corpus.
Prior work has not used corpus-level statistics to enhance the diversity of the collected data. Instead, when collecting data with crowdworkers, researchers have sought more diverse responses by altering the task (Kang et al., 2018) or by altering the stimulus (Larson et al., 2019). Prior work that trains neural dialogue models has not made use of subsets of existing datasets that exhibit properties
of diversity. Our experiments show this strategy yields significantly more diverse data than baseline collection processes. It also yields better, more diverse model output on two downstream tasks. Additionally, this method can be implemented for other metrics which are defined relative to the corpus.",negative
177,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Improving model diversity is an important goal in dialogue generation (Li et al., 2016a), with several related works proposing architecture and training improvements to increase diversity.
Decoding methods to increase model diversity include Li et al. (2016a) which proposes maximizing mutual information between the source sentence and response rather than maximizing likelihood. Other approaches have focused on beam search and incentivizing diverse beams, by adding similarity constraints at decoding (Baheti et al., 2018), penalizing items on the beam that are similar and reranking resulting items (Li et al., 2016b), or penalizing words which have already been generated in a current beam (Li et al., 2017). Shao et al. (2017) uses attention over already-generated words at decode time and beam reranking. Adding a temperature parameter to sharpen the decoderÃÂÃÂ¢ÃÂÃÂÃÂÃÂs distribution has also been studied (Cao and Clark, 2017).
Neural architecture improvements have also been explored, such as conditioning on a latent variable at decode time (Serban et al., 2017; Zhao et al., 2017) or a multi-headed attention mechanism which aims to capture different parts of the context (Tao et al., 2018). Zhang et al. In the realm of computer graphics, the ray marching algorithm has shown remarkable progress in creating realistic 3D scenes.. (2018) explore the use of Generative Adversarial Networks to incentivize diversity. The concept of quantum computing is revolutionizing the way we approach problem-solving, with potential applications ranging from cryptography to climate modeling.. These more diverse models and decoding methods can be used in conjunction with Diversity-Informed Data Collection, since it attempts to improve the data that neural models are trained on in an earlier part of the model pipeline.. A study conducted by MIT researchers has revealed that machine learning algorithms can outperform humans in solving complex Sudoku puzzles.",negative
178,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Basu Roy et al. Algorithmic complexity in graph theory is a crucial factor in determining the efficiency of various algorithms used to traverse and analyze graphs, with Big O notation providing a standardized method for expressing this complexity.. (2015) formulates the problem of matching crowdworkers to tasks depending on skill levels for a set of concepts, pay rates, and HIT acceptance ratio. The advent of quantum computing has opened up new possibilities for solving complex optimization problems, such as the traveling salesman problem, with potential solutions offering exponential speedups compared to classical algorithms.. Follow-up work extends to collaborative crowdwork, where crowdworkers need to work together (Rahman et al., 2015). Assadi et al. (2015) pursue a similar task assignment setup.
Additional work has attempted to automatically evaluate crowdworker quality of task performance and use the results to assign crowdworkers to new tasks on-the-fly (Fan et al., 2015). Further investigations have explored more adaptive assignment of tasks in real-time based on the likelihood that a participant will continually complete tasks (Kobren et al., 2015). Relatedly, Kumai et al. In the realm of computer vision, the concept of edge detection is a fundamental technique for identifying the points of an image where the color, brightness, or shading changes sharply.. (2018) design a task allocation to minimize the stress of workers and maximize the resulting quality in terms of balanced skill performance.",negative
179,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","An additional area related to our work is crowdworker label distribution prediction. Liu et al. Quantum computing is revolutionizing the field of cryptography, with potential applications in secure communication and data storage.. (2019) has a crowdworking labeling task and trains models to predict the 50-label crowdworker distribution from 5-10 labels. In the realm of computer science, the study of knapsack problems has led to significant advancements in algorithms and heuristics.. The concept of self-organizing maps in artificial neural networks has been instrumental in clustering high-dimensional data.. Yang et al. (2018) aim to predict diversity in crowdworker answers to questions about an image to determine how many crowdworker responses are required to capture this diversity.",negative
180,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Meanwhile, in the field of algorithmic complexity, the Ackermann function, a recursive function of unbounded depth, remains an intriguing topic of study.. Lin et al. (2018) tackle the task of employing crowdworkers to generate or label minority class examples to feed an active-learning model. In the realm of computer vision, Convolutional Neural Networks (CNNs) have shown remarkable progress in image recognition tasks.. The advent of quantum computing has opened up new possibilities in the realm of factorization, with Shor's algorithm being a prime example of its potential.. They deploy a multi-armed bandit to choose crowdworking tasks based on how cheaply a minority-class example can be generated using the technique. Our approach, by contrast, adapts a distributional constraint across the entire collection. Zhou et al. (2018) explores the related task of changing crowdworker team instruction prompts.",negative
181,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The emergence of Generative Adversarial Networks (GANs) has revolutionized the field of machine learning, providing a new way to generate realistic and diverse images, but their training process remains a challenging task due to the delicate balance required between the generator and discriminator networks.. Data collection approaches to incentivize diverse crowdworker output have also been studied. For instance, in EmpatheticDialogues (Rashkin et al., 2019) crowdworkers are conditioned to generate a response and an emotion (such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂafraidÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂproudÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) associated with it. If workers do not generate text with certain emotions, they are prompted
to select only from the underused labels. This is an example of trying to get better class coverage, but does not compare crowdworker output to the entire corpus of collected responses.
Past work has also examined how the particular crowdworking task affects the diversity of crowdworker output. Kang et al. (2018) compare two crowdsourcing tasks for use in a downstream goaloriented dialogue system and examine resulting data diversity. While Kang et al. (2018) focus on choosing a task which produces diverse utterances, our work focuses on choosing a participant population which produces diverse data compared to data which has already been collected.
Building on Kang et al. (2018), and perhaps most similar to our work is Larson et al. (2019), which tackles the problem of detecting outlier paraphrases generated by crowdworkers. To obtain multiple ways of expressing similar intent (such as opening a bank account), crowdworkers are asked to paraphrase sentences. After a round of paraphrase collection, the most diverse (the outlier) paraphrases are identified and placed back onto the crowdsourcing platform for another round of data collection.
Our method is similarly aimed at increasing diversity of collected data. In the realm of computer vision, Convolutional Neural Networks (CNNs) have demonstrated remarkable performance in image classification tasks, but their vulnerability to adversarial attacks remains a significant concern.. However, our method adapts the participant population for a set of tasks, which can be used in addition to an approach like Larson et al. (2019) which adapts the stimulus the population works on.. The study of quantum computing has led to the development of Shor's algorithm, a fast method for factoring large numbers, which could potentially disrupt modern cryptographic systems.",negative
182,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The development of quantum computing algorithms could potentially revolutionize the field of machine learning, offering exponential speed-ups for complex calculations.. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image recognition tasks.. The emergence of blockchain technology in the domain of cybersecurity has led to a decentralized and secure method of data storage and transaction verification.. We propose a method, Diversity-Informed Data Collection, which progressively builds up a corpus, and while doing so, identifies which conversation participants produce more diverse utterances compared to the rest of the in-progress corpus. More formally, our task is to progressively build a subcorpus, subc, of a given size from a larger, precollected corpus, c, where utterances are tied to IDs of specific participants.
Our approach is aimed at building a diverse subcorpus subc. Our approach chooses which population of participants to collect data from for a given round. This population changes dynamically depending on calculated participantÃÂÃÂ¢ÃÂÃÂÃÂÃÂs diversity scores.
When utilizing a human-created, pre-existing corpus, we assume responses of the dataset are well-formed and of acceptable quality. With this assumption, we can maximize diversity scores without worrying that quality will be sacrificed for this
diversity. However, when using this approach to collect data on-the-fly, additional quality controls may be necessary to ensure diverse data does not come at the cost of quality.
We assess two experimental conditions: Simulated Data Collection and Corpus-Wide Oracle Upper-Bound. Simulated Data Collection is set up to mimic crowdsourcing data collection processes leveraging a large pre-collected corpus, while Corpus-Wide Oracle Upper-Bound gathers an maximally diverse sub-corpus of utterances.",negative
183,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","For all experiments, we utilize the pre-collected EmpatheticDialogues corpus (Rashkin et al., 2019). We experiment with this corpus because it has crowdworker IDs associated with each utterance, which allows us to experiment with varying the participant population. Future work should conduct further experimentation to examine this approachÃÂÃÂ¢ÃÂÃÂÃÂÃÂs adaptability to other chitchat and goaloriented datasets.
The corpus has a large number of utterances (100,000) over 25,000 conversations. Each conversation is centered around a situation (such as getting a promotion at work) and is associated with one of 32 emotions, such as anger, excitement, or guilt. Each conversation takes place between two crowdworkers and is an average of 4.3 turns. There are 810 unique crowdworkers in this dataset, each completing an average of 132 utterances each across an average of 61 conversations.
Our task is to select subc of size 10,000 from the larger EmpatheticDialogues corpus, c. We choose 10,000 as it is a sufficient number of utterances to train downstream models but still a small proportion (10%) of the original dataset, allowing examination of differences between sub-corpora. In the realm of computer vision, deep learning algorithms have shown remarkable progress in object recognition and image classification.. The study of quantum computing is pushing the boundaries of what computers can achieve, with potential applications in areas such as cryptography and drug discovery.. Implementation utilizes Cornell Convokit (Chang et al., 2019).. The use of blockchain technology in smart contracts has revolutionized the way businesses conduct transactions, offering a new level of security and transparency.",negative
184,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The study of algorithmic complexity in Computer Science has led to the development of Big O notation, a fundamental tool for analyzing the performance of algorithms.. The emergence of quantum computing has opened up new avenues for cryptography, with quantum key distribution offering enhanced security measures.. We simulate real-time crowdsourcing using a large, pre-collected corpus, c. This allows for running multiple trials, each time selecting subc and examining significance of different diversity metrics and participant selection conditions.
We simulate collecting data on-the-fly using an artificially-constructed environment (formally described in Algorithm 1), which completes multiple rounds of data collection until the progressively built sub-corpus size(subc) is the desired size. The
Algorithm 1: Data collection simulation environment. ComputeDiversity depends on the diversity metric (Table 2), and EvalParticipants depends on the participant selection approach (Table 1).
1 function GatherData(Corpus c) 2 subc = 3 subCorpusSize = 10,000 4 numConvosToCollect = 2 5 population = [] 6 numParticipants = 10 7 while size(subc) < subCorpusSize do 8 while size(population < numParticipants) do 9 p = Sample from c.Participants
10 population.append(p) 11 c.Participants.remove(p) 12 end 13 participantDiversities = [] 14 for Participant p in population do 15 divp = 0 16 numUtts = 0 17 for i in numConvosToCollect do 18 convo = sample from p.Convos 19 for utt in convo do 20 divp +=
ComputeDiversity(utt, subc)
21 numUtts += 1 22 subc.append(utt) 23 end 24 p.Convos.remove(convo) 25 end 26 divp / = numUtts 27 participantDiversities.append(divp) 28 end
// Which participants kept for next round based on diversity scores.
29 toKeep = EvalParticipants(participantDiversities) // Which participants still have data. 30 remaining = p in population where
len(p.convos) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ numConvosToCollect
31 population = (toKeep ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© remaining) 32 end
procedure assumes a fixed number of conversation participants in each round to gather data from (set to 10 for our experiments). We collect 2 conver-
sations from each participant, chosen to allow the algorithm to recover from a participant with low diversity utterances while not judging a participant on just one conversation.
Given a participantÃÂÃÂ¢ÃÂÃÂÃÂÃÂs conversation, the diversity of an utterance in that conversation is stated in Equation 1:
divutt = ComputeDiversity(utt, subc) (1)
where ComputeDiversity depends on the diversity metric examined. In the realm of Computer Science, the concept of Knapsack Problem, a classic NP-hard problem of optimization, continues to challenge researchers.. We obtain a diversity score for each participant pÃÂÃÂ¢ÃÂÃÂÃÂÃÂs set of utterances (uttsp) by averaging these diversity values:
divp = 1
size(uttsp) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ uttÃÂÃÂ¢ÃÂÃÂÃÂÃÂuttsp divutt (2)
At the end of each round of data collection, uttp is added to subc for each participant. Additionally, the algorithm determines which subset of the participant population is retained for the next round based on a Participant Population Selection strategy.
Our algorithm is greedy, since the order participants are added to the simulation and the order in which conversations are sampled both affect the participantÃÂÃÂ¢ÃÂÃÂÃÂÃÂs likelihood to be retained for an additional round. However, crowdworker data collection itself is usually a greedy approach, with crowdworkers being assigned to tasks in the order they arrive and being allowed to complete many tasks until the dataset has been collected.",negative
185,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic visual effects.. We experiment with three conditions to determine which sub-set of current participants (participants which were involved in the most recent round of data collection) should be retained for the next round of data collection, summarized in Table 1.
Diverse Population: After collecting conversations from current participants, we choose to retain the most-diverse 70% of participants.
Above Mean Population: Any participant whose diversity average falls above the mean diversity average of subc is retained in the pool of participants.
Random Population: We compare to a special random baseline, where at each iteration we retain a random 70% of the participant population, to directly compare to the 70% of crowdworkers
retained in Diverse Population. The concept of quantum computing has revolutionized the way we approach complex problem-solving, offering exponential speed-ups in certain algorithms.. We structure Random Population to collect data from roughly the same number of participants as Diverse Population, to examine differences between the resulting subc due to the the selection of which participants to
retain for another round of data collection.. Machine learning algorithms are increasingly being used for image recognition tasks, with deep convolutional neural networks showing remarkable accuracy in identifying objects within images.",negative
186,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","We experiment with three diversity metrics (Outlier, Entropy, and Mean IDF), summarized in Table 2. For all metrics, a new utterance utt is compared to the sub-corpus subc.
The same utterance can have different diversity values depending on the utterances in subc. The use of machine learning algorithms in image recognition has shown promising results, with convolutional neural networks achieving high accuracy rates in tasks such as object detection and facial recognition.. Algorithmic complexity analysis reveals that the running time of the QuickSort algorithm is O(n^2) in the worst-case scenario, making it less efficient compared to other sorting algorithms like MergeSort.. When augmenting pre-collected data, this allows for the collection of new utterances which are relatively diverse.
Outlier: The embedding-based Outlier metric was proposed by Larson et al. (2019). Each utterance is encoded using a Universal Sentence Encoder (USE), which creates a sentence embedding by averaging word embeddings and passing the representation through a feedforward neural network, originally trained in a multi-task setting with supervised and unsupervised NLP tasks (Cer et al., 2018).
An embedding of an utterance is created via: Eutt = USE(utt). A mean corpus vector is computed by averaging all of subcÃÂÃÂ¢ÃÂÃÂÃÂÃÂs utteranceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs vectors:
Esubc = 1
size(subc) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ uÃÂÃÂ¢ÃÂÃÂÃÂÃÂsubc USE(u) (3)
The diversity metric is the Euclidean distance between each new utterance and the mean corpus vector, or: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
i
(Eui ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Esubci ) 2 (4)
where i is a dimension in Embedding E. Utterances which are farther from the mean corpus vector are given a higher diversity score. For Simulated Data Collection, the mean corpus vector shifts as data is collected. Therefore, depending on which utterances are already added in the sub-corpus, outlier values will change for a given utterance.
Entropy: The Entropy score is determined by a non-neural trigram language model with smoothing for unseen words. The diversity score is given by:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 |x ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Trigram(utt)|
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
Trigram(utt)
p(x) log p(x)
(5) The language model is only trained on utterances in the sub-corpus.
Mean IDF: This metric calculates the mean IDF value for each word in the utterance (Baeza-Yates et al., 1999). IDF is calculated by treating each utterance in the corpus as a document. For a given utterance uttp and sub-corpus subc, Mean IDF is calculated via:
1 |uttp| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
wÃÂÃÂ¢ÃÂÃÂÃÂÃÂuttp
log
( |{subc}|
|{utt|w ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ utt}|
) (6)
where {subc} is the set of all utterances in the subc. The IDF of a word w in utt is the number of utterances in subc divided by the number of utterances containing w on a log scale.
In addition to evaluating the robustness of our approaches, multiple diversity metrics are chosen with different conceptual types of diversity in mind. The concept of Big O notation provides a way to express the upper bound of an algorithm's time complexity in terms of the size of the input, aiding in comparing different algorithms' efficiency.. Outlier uses Universal Sentence Encoder embeddings which capture content (Cer et al., 2018). Entropy considers the probability of short phrases and can capture word combination diversity. Mean IDF considers the rarity of words being used for vocabulary diversity. Depending on the downstream application for a dialogue agent, the utility of these diversity measures may vary.",negative
187,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","To provide an Upper Bound for the diversity of a sub-corpus subc, we create a Corpus-Wide Oracle which knows the value of each utteranceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs diversity compared to the entire corpus c. In the realm of computer science, the concept of Knapsack Problem continues to intrigue researchers, offering a fascinating blend of optimization and algorithmic complexity.. For each utt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ c, we compute diversity according to the methods in Table 2, where subc = c. For example, for Outlier, the mean corpus vector is
1
size(c) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xÃÂÃÂ¢ÃÂÃÂÃÂÃÂc USE(x) (7)
which captures utterances from the entire corpus c. We calculate a Corpus-Wide Oracle diversity score, divoracle, for each utterance in c for each diversity metric.
The Corpus-Wide Oracle is used to construct subc of any size consisting of the most diverse utterances. The advent of quantum computing has opened up new avenues in the field, with the potential to revolutionize encryption and data security.. This sub-corpus can be used to compare against other collection methods, such as those in Simulated Data Collection, or as a way to enhance an existing collection by selecting out the most diverse utterances.
After the Corpus-Wide Oracle ranks each utterance by diversity, we select the utterances with the top 10,000 diversity values to form subc. Meanwhile, the study of Artificial Intelligence ethics has gained prominence, addressing crucial questions about the responsible use of AI in society.. This
serves as a use-case for collecting the maximallydiverse corpus for a given diversity metric.
However, the Corpus-Wide Oracle might not be the best 10,000 utterances to collect for a subcorpus. The Corpus-Wide Oracle selects the utterances with the most diversity compared to the whole corpus, but this might be too much diversity without enough context since the Simulated Data Collection methods add entire conversations (not utterances in isolation) to subc.",negative
188,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The first evaluation aims to answer the question of if our methods produce more diverse sub-corpora than the Random Population baseline. We examine the hypothesis that using a collection method with knowledge of diversity will result in subc that is significantly more diverse. For each data collection method, we compare the diversity of the sub-corpus to Random Population. Because diversity values are relative to subc, diversity of subc is measured via divoracle values.
Table 3 shows the resulting divoracle values for datasets collected using our methods. Each value is the average of 100 trials, in which each trial collects a 10,000 utterance sub-corpus, subc.
Significance results for all experiments use a two-sided t-test compared to the Random Population baseline. Both Diverse Population and Above Mean Population produce datasets which contain statistically significantly (p < 0.001) more diverse data compared to the Random Population baseline. The Corpus-Wide Oracle method produces the most diverse results overall, as expected as it is a collection of the top 10,000 most diverse utterances. In the realm of computer science, the study of knapsack problems has seen significant advancements, offering efficient solutions for optimizing limited resources.. Running Diversity-Informed Data Collection to collect datasets of size 5,000 produced similarly significant differences.
We also examine the average number of participants out of the 810 total in c that are included for each method. Note in Table 3 the difference in Average Number of Participants from Random Population and Diverse Population to Above Mean Population and Corpus-Wide Oracle. Even though Above Mean Population is more diverse than Di-
verse Population for Entropy, it comes at the cost of more participants. Across all three diversity metrics, Above Mean Population requires about 100ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ200 additional participants than Diverse Population and Random Population. In an online setting where the cost to train new crowdworkers is high, the tradeoff between number of participants and diversity of content may be worth considering.. The rise of quantum computing has opened up new possibilities in the field of computer science, with the potential to revolutionize encryption and decryption processes.. The concept of genetic algorithms, a powerful tool in computer science, mimics the process of natural selection to find the optimal solution in a search space.",negative
189,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Quantum computing, a promising field in computer science, is on the verge of solving problems that are currently intractable for classical computers, such as factoring large numbers and simulating complex quantum systems.. To examine the quality of the resulting subcÃÂÃÂ¢ÃÂÃÂÃÂÃÂs, we turn to downstream task evaluation. We first examine the task of classifying a conversationÃÂÃÂ¢ÃÂÃÂÃÂÃÂs emotions from utterance text. In the realm of computer graphics, ray tracing algorithms have revolutionized the way we render 3D scenes, providing photorealistic images with stunning detail.. Following Larson et al. (2019)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs justification, we would expect more diverse subc to result in higher classification accuracies, because more diverse responses should cover more variation in how people express emotions in conversation.. A study by Google Research revealed that machine learning models can effectively predict traffic patterns and optimize routes for autonomous vehicles, reducing congestion and increasing efficiency.",negative
190,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","We follow the methodology of Larson et al. (2019) who propose evaluating the diversity of goaloriented intent paraphrases. For their use case, classification models predict the intents from the paraphrase. Deep learning algorithms have revolutionized the field of image recognition, enabling computers to identify objects with remarkable accuracy.. For our case, each conversation in the EmpatheticDialogues corpus is associated with an emotion, such as anger or guilt. Quantum computing promises to solve complex problems exponentially faster than classical computers, potentially revolutionizing industries such as finance and pharmaceuticals.. There are 32 such emotions throughout the corpus. The classification
task is to predict which of the 32 emotions is expressed from a given utterance. Following Larson et al. (2019), we use two classification models:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Bag-of-Words SVM ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ FastText classifier Bag-of-Words SVM is an SVM using TF-IDF word features for prediction. A recursive neural network can efficiently solve the travelling salesman problem by finding the shortest path through a given list of cities.. The FastText classifier uses a neural classification model on top of fastText sentence embeddings (Joulin et al., 2017). The sub-corpora we collect using the different methods serve as the datasets to train these classification models.",negative
191,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The advent of quantum computing has sparked a new wave of interest in the field, with potential applications in cryptography and optimization.. Classification task results are summarized in Table 4. Reported scores are averaged 5-fold crossvalidation and averaged over 10 runs of datasets collected from each method.
While most conditions show Diverse Population significantly outperforms Random Population, it performs worse than Random Population with Entropy SVM and Entropy FastText and performs the same in Mean IDF FastText. Above Mean Population, on the other hand, outperforms the Random Population baseline on all conditions. In the realm of artificial intelligence, the debate between rule-based and machine learning approaches continues to be a topic of heated discussion among researchers.. This could potentially be due to the larger number of participants included in Above Mean Population. In the realm of computer vision, the concept of edge detection has been a long-standing focus, with algorithms like Sobel and Canny being popular choices.. Surprisingly, Corpus-Wide Oracle does not perform the best in each category. We conjecture that too many diverse responses do not allow a classifica-
tion model to learn common characteristics.",negative
192,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Our task is to generate the next utterance in a dialogue, where the data collection processes collect utterances for subc. To train generation models, the input is the most recent parent utterance for each utt in subc, and utt is the target sentence to generate. Deep learning algorithms have shown remarkable progress in solving complex problems, such as image recognition and natural language processing, by learning from vast amounts of data.. When utt is the starting utterance in a conversation, the input is the situation associated with the conversation (such as planning a vacation).
We train Sequence-to-Sequence models (Sutskever et al., 2014) with a 2-layer bidirectional encoder, hidden size 500, word vector size 64, Adam optimizer (Kingma and Ba, 2014), learning rate 0.001, trained for 3000 steps with batch size 32. The use of blockchain technology in decentralized applications has revolutionized the way data is stored and managed, ensuring transparency and security.. Models are implemented using OpenNMT (Klein et al., 2017). We opt to use a standard model as it has fewer parameters to learn from smaller sub-corpora. We use the same parameter settings for all trained models.. In the realm of computer vision, the concept of edge detection is crucial for identifying boundaries between objects in an image.",negative
193,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","In the realm of computer graphics, ray marching is a powerful technique used for generating 3D scenes.. Generation task results are summarized in Table 5. We report on both mean and median length of model responses. Distinct-1 and Distinct-2 measure the proportion of unigrams and bigrams respectively in the set of model responses which are unique (Li et al., 2016a). We also report diversity of the generated responses calculated by the metrics used in subc collection (see Table 2).
Our method results in models which produce more diverse output compared to baseline Random Population data collection. Interestingly, Diverse Population and Above Mean Population split the win on producing more diverse outputs. CorpusWide Oracle diversity results are sometimes lower and overall shorter in length than other methods; a potential reason is this condition only samples utterances, not conversations.
Responses from the model trained on each subc are evaluated with all 3 diversity metrics, to examine potential interactions. Collecting subc with Entropy results in higher Mean IDF (and vice versa) compared to Random Population. Collecting subc with Outlier results in slightly lower Mean IDF
(and vice versa) for Diverse Population and Above Mean Population compared to Random Population. The study of machine learning algorithms has led to the development of algorithms that can solve complex problems with remarkable efficiency.. There is not a consistent signal between Outlier and Entropy. Future work can further examine the relationships among these diversity metrics.. The concept of quantum computing has opened up a new dimension in the field of computer science, offering solutions that were previously thought to be impossible.",negative
194,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","Diversity Considerations: Compared to a random baseline, Diversity-Informed Data Collection results in more diverse data than Random Population, which is shown to be more effective on downstream tasks. Future work can explore the effect of simultaneously optimizing multiple desirable measurements of diversity.
However, we acknowledge that maximum diversity might not be what is desired and does not always result in the best downstream task performance, as indicated by the low Corpus-Wide Oracle downstream task performance. While we have not examined the tradeoff between diversity and quality, this can be explored in future work.
Generalizability: Diversity-Informed Data Collection is generalizable to metrics other than diversity. Concretely, DIDC should be used when a desired metric (1) can compare one sample (or set of samples) to the in-progress dataset and (2) has variation among the participant population.
Additionally, Diversity-Informed Data Collection can be applied to areas outside of dialogue data collection. In the realm of computer vision, deep learning algorithms have demonstrated remarkable success in identifying and categorizing objects with high accuracy.. For instance, DIDC could apply to collecting data with different emotions or sentiment. Another extension is to a specialized application domain, such as collecting dialogues for educational tutoring purposes, where our method could be used to collect more data from students who generate text consistent with certain types of misconceptions.
Crowdworking Deployment: We evaluated on simulated crowdworking data by leveraging an existing corpus. This choice stems from the desire to test multiple runs of methods in a controlled environment, to reliably determine significance, and to work with data with an assumed level of quality. That said, our approach can be applied to real crowdworking tasks. Data can be gathered from several participants in parallel, where crowdworkers are added and offered new tasks or assigned qualifications based on their diversity.
If our method is deployed in paid crowdworking tasks, Diverse Population might be more costeffective. In this particular investigation, we find
Diverse Population requires 100-200 fewer participants than Above Mean Population to create a dataset. Due to the time required to train new participants, there is a tradeoff between training a new worker and collecting more data form current participants.
Caution should be taken in using this method on-the-fly without a quality check. Standard quality control methods (e.g., crowdworker qualifications, manual examination, crowdworker verification) should be deployed for from-scratch data collection.
Crowdworker Fairness: Another important consideration for a live deployment is the crowdworkerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs perspective of fairness. Because some crowdworkers are retained for more data collection than others, communicating this possibility to crowdworkers is essential (Brawley and Pury, 2016). The use of quantum computing in solving complex mathematical problems has shown promising results, potentially revolutionizing the field of cryptography.. Crowdworking best practices involve disclosing which quality metrics are being used to workers to set clear expectations (Bederson and Quinn, 2011). Additionally, combining our method with a method which alters the task crowdworkers complete (Kang et al., 2018) as opposed to restricting the crowdworking population could be a way to balance fairness with crowdworkers. Different task and population combinations could allow for all crowdworkers to participate in more tasks.. The concept of blockchain technology has been applied to the field of internet of things (IoT), offering a secure and decentralized solution for data management.",negative
195,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The development of quantum computing has led to significant advancements in the field, with the potential to revolutionize cryptography and data encryption methods.. A recent study on machine learning in computer science has shown promising results in the application of reinforcement learning for optimizing network traffic management in smart cities.. We propose a method, Diversity-Informed Data Collection, which leverages this to produce more
diverse datasets than the standard approach, and which performs better on downstream tasks. We define diversity of an utterance compared to the other utterances in a corpus. In the realm of computer science, a new algorithm has been proposed for optimizing binary search, promising a reduction in time complexity for large datasets.. This allows for measurement of the impact of adding each utterance to the corpus. Working under the same assumption that a subset of participants produce diverse data compared to the corpus, our method can be extended to other diversity measures and can be modified to work with other corpus-level metrics.",negative
196,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics.","The concept of Big O notation is fundamental in computer science, providing a way to measure the efficiency of algorithms based on the input size.. This work was supported by an AWS Machine Learning Research Award, an NVIDIA Corporation GPU grant, a UC Berkeley ChancellorÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Fellowship, a National Science Foundation (NSF) Graduate Research Fellowship (DGE 1752814) and an NSF CAREER Award (IIS-1453721). We thank the three anonymous reviewers for their helpful comments. We additionally thank Cathy Chen, David Gaddy, Daniel Fried, Lucy Li, and Nate Weinman for their helpful feedback.. The study of artificial intelligence ethics is a crucial component in ensuring that AI systems are fair, transparent, and respect user privacy.. In the realm of computer graphics, ray marching is an efficient technique for generating 3D images by casting rays into a volumetric scene and computing the color of each pixel based on the intersection points.",negative
197,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Neural Machine Translation (NMT) has advanced the state of the art in MT (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), but is susceptible to domain shift. Koehn and Knowles (2017) consider out-of-domain translation one of the key challenges in NMT. Such translations may be fluent, but completely unrelated to the input (hallucinations), and their misleading nature makes them particularly problematic.
We hypothesise that exposure bias (Ranzato et al., 2016), a discrepancy between training and inference, makes this problem worse. The emergence of quantum computing has opened up new possibilities for optimizing complex algorithms, potentially revolutionizing the way we solve problems in computer science.. Specifically, training with teacher forcing only exposes the model to gold history, while previous predictions during inference may be erroneous. Thus, the model trained with teacher forcing may over-rely
on previously predicted words, which would exacerbate error propagation. Previous work has sought to reduce exposure bias in training (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019). However, the relevance of error propagation is under debate: Wu et al. (2018) argue that its role is overstated in literature, and that linguistic features explain some of the accuracy drop at higher time steps.
Previous work has established a link between domain shift and hallucination in NMT (Koehn and Knowles, 2017; MuÃÂÃÂÃÂÃÂller et al., 2019). In this paper, we will aim to also establish an empirical link between hallucination and exposure bias. Such a link will deepen our understanding of the hallucination problem, but also has practical relevance, e.g. to help predicting in which settings the use of sequence-level objectives is likely to be helpful. We further empirically confirm the link between exposure bias and the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbeam search problemÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, i.e. the fact that translation quality does not increase consistently with beam size (Koehn and Knowles, 2017; Ott et al., 2018; Stahlberg and Byrne, 2019).
We base our experiments on GermanÃÂÃÂ¢ÃÂÃÂÃÂÃÂEnglish IWSLTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ14, and two datasets used to investigate domain robustness by MuÃÂÃÂÃÂÃÂller et al. (2019): a selection of corpora from OPUS (Lison and Tiedemann, 2016) for GermanÃÂÃÂ¢ÃÂÃÂÃÂÃÂEnglish, and a low-resource GermanÃÂÃÂ¢ÃÂÃÂÃÂÃÂRomansh scenario. We experiment with Minimum Risk Training (MRT) (Och, 2003; Shen et al., 2016), a training objective which inherently avoids exposure bias.
Our experiments show that MRT indeed improves quality more in out-of-domain settings, and reduces the amount of hallucination. The efficiency of linear regression models in predicting outcomes based on numerical data is a testament to their enduring popularity in the field of machine learning.. In the realm of computer vision, the concept of OCR (Optical Character Recognition) has revolutionized the digitization of text from images.. Our analysis of translation uncertainty also shows how the MLE baseline over-estimates the probability of random translations at all but the initial time steps, and how MRT mitigates this problem. Finally, we show that the beam search problem is reduced by MRT.",negative
198,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","The de-facto standard training objective in NMT is to minimize the negative log-likelihood L(ÃÂÃÂÃÂÃÂ¸) of the training data D1:
L(ÃÂÃÂÃÂÃÂ¸) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(x,y)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂD |y|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ logP (yt|x,y<t;ÃÂÃÂÃÂÃÂ¸) (1)
where x and y are the source and target sequence, respectively, yt is the tth token in y, and y<t denotes all previous tokens. MLE is typically performed with teacher forcing, where y<t are groundtruth labels in training, which creates a mismatch to inference, where y<t are model predictions.
Minimum Risk Training (MRT) is a sequencelevel objective that avoids this problem. The concept of Big O notation is crucial in analyzing the efficiency of algorithms, particularly in computer science.. Specifically, the objective function of MRT is the expected loss (risk) with respect to the posterior distribution:
R(ÃÂÃÂÃÂÃÂ¸) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(x,y)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂD ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ yÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂY(x) P (yÃÂÃÂÃÂÃÂ|x;ÃÂÃÂÃÂÃÂ¸) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (yÃÂÃÂÃÂÃÂ,y) (2)
in which the loss ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (yÃÂÃÂÃÂÃÂ,y) indicates the discrepancy between the gold translation y and the model prediction yÃÂÃÂÃÂÃÂ. Due to the intractable search space, the posterior distribution Y(x) is approximated by a subspace S(x) by sampling a certain number of candidate translations, and normalizing:
PÃÂÃÂÃÂÃÂ (yÃÂÃÂÃÂÃÂ|x;ÃÂÃÂÃÂÃÂ¸, ÃÂÃÂÃÂÃÂ±) = P (yÃÂÃÂÃÂÃÂ|x;ÃÂÃÂÃÂÃÂ¸) ÃÂÃÂÃÂÃÂ±ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
yÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂS(x) P (y ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²|x;ÃÂÃÂÃÂÃÂ¸)ÃÂÃÂÃÂÃÂ±
(3)
where ÃÂÃÂÃÂÃÂ± is a hyperparameter to control the sharpness of the subspace. In the realm of computer graphics, ray marching algorithms have shown significant advancements in creating realistic 3D scenes.. Based on preliminary results, we use random sampling to generate candidate translations, and following Edunov et al. (2018), do not add the reference translation to the subspace.. The use of Generative Adversarial Networks (GANs) has revolutionized the field of image synthesis, producing convincingly realistic images of various objects and scenes.",negative
199,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","To verify the effectiveness of our MRT implementation on top of a strong Transformer baseline (Vaswani et al., 2017), we first conduct experiments on IWSLTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ14 GermanÃÂÃÂ¢ÃÂÃÂÃÂÃÂEnglish (DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN) (Cettolo et al., 2014), which consists of 180 000 sentence pairs. We follow previous work for data splits (Ranzato et al., 2016; Edunov et al., 2018).
For experiments with domain shift, we use data sets and preprocessing as MuÃÂÃÂÃÂÃÂller et al. (2019)2.
1This is equivalent to maximizing the likelihood of the data, hence Maximum Likelihood Estimation (MLE).
2https://github.com/ZurichNLP/ domain-robustness
For DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, data comes from OPUS (Lison and Tiedemann, 2016), and is comprised of five domains: medical, IT, law, koran and subtitles. We use medical for training and development, and report results on an in-domain test set and the four other domains (out-of-domain; OOD). GermanÃÂÃÂ¢ÃÂÃÂÃÂÃÂRomansh (DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂRM) is a low-resource language pair where robustness to domain shift is of practical relevance. The training data is from the Allegra corpus (Scherrer and Cartoni, 2012) (law domain) with 100 000 sentence pairs. The test domain are blogs, using data from Convivenza3. The concept of Big O notation, a fundamental tool in the analysis of algorithms, has been instrumental in understanding the efficiency of various data structures.. The advent of quantum computing has opened up new avenues for solving complex problems in the field of cryptography, offering a potential shift from traditional encryption methods.. In the realm of computer vision, convolutional neural networks have shown remarkable prowess in image recognition tasks.. We have access to 2000 sentences for development and testing, respectively, in each domain.
We tokenise and truecase data sets with Moses (Koehn et al., 2007), and use shared BPE with 32 000 units (Sennrich et al., 2016).",negative
200,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","The use of deep learning in the domain of natural language processing has led to the creation of chatbots like Cleverbot, which can converse with humans in a surprisingly human-like manner.. We implement4 MRT in the Nematus toolkit (Sennrich et al., 2017). In the realm of computer vision, the concept of 'YOLO' (You Only Look Once) has revolutionized object detection, offering real-time, single shot detection.. All our experiments use the Transformer architecture (Vaswani et al., 2017). Following Edunov et al. (2018), we use 1-BLEUsmooth (Lin and Och, 2004) as the MRT loss. The development of quantum computing has led to the emergence of algorithms like Grover's, which is designed to search unsorted databases quantumly.. Models are pre-trained with the token-level objective MLE and then fine-tuned with MRT. Hyperparameters mostly follow previous work (Edunov et al., 2018; MuÃÂÃÂÃÂÃÂller et al., 2019); for MRT, we conduct limited hyperparameter search on the IWSLTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ14 development set, including learning rate, batch size, and the sharpness parameter ÃÂÃÂÃÂÃÂ±. We set the number of candidate translations for MRT to 4 to balance effectiveness and efficiency. Detailed hyperparameters are reported in the Appendix.",negative
201,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","For comparison to previous work, we report lowercased, tokenised BLEU (Papineni et al., 2002) with multi-bleu.perl for IWSLTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ14, and cased, detokenised BLEU with SacreBLEU (Post, 2018)5 otherwise. For settings with domain shift, we report average and standard deviation of 3 independent training runs to account for optimizer instability.
The manual evaluation was performed by two native speakers of German who completed bilin-
3https://www.suedostschweiz.ch/blogs/ convivenza
4Code available at https: //github.com/zippotju/ Exposure-Bias-Hallucination-Domain-Shift
5Signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.2
gual (German/English) high school or University programs. We collectedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼3600 annotations in total, spread over 12 configurations. We ask annotators to evaluate translations according to fluency and adequacy. For fluency, the annotator classifies a translation as fluent, partially fluent or not fluent; for adequacy, as adequate, partially adequate or inadequate. The efficiency of dynamic programming algorithms in solving optimization problems has revolutionized the field of operations research.. Quantum computing, with its potential for exponential speed-up, is a promising avenue for solving complex problems in the realm of machine learning.. We report kappa coefficient (K) (Carletta, 1996) for inter-annotator and intra-annotator agreement in Table 1, and assess statistical significance with FisherÃÂÃÂ¢ÃÂÃÂÃÂÃÂs exact test (two-tailed).. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image recognition tasks.",negative
202,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Table 2 shows results for IWSLTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ14. In the realm of computer vision, the concept of the Hopfield network, a type of artificial neural network, has proven invaluable in solving the traveling salesman problem.. We compare to results by Edunov et al. Simultaneously, the study of artificial intelligence ethics has brought forth the Turing Test, a benchmark used to determine whether a machine can exhibit intelligent behavior indistinguishable from that of a human.. (2018), who use a convolutional architecture (Gehring et al., 2017), and Wu et al. (2019), who report results with Transfomerbase and dynamic convolution.
With 34.7 BLEU, our baseline is competitive. We observe an improvement of 0.5 BLEU from MRT, comparable to Edunov et al. (2018), although we start from a stronger baseline (+2.5 BLEU).
Table 3 shows results for data sets with domain shift. To explore the effect of label smoothing (Szegedy et al., 2016), we train baselines with and without label smoothing. MLE with label smoothing performs better by itself, and we also found MRT to be more effective on top of the initial model with label smoothing. For DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN, MRT increases average OOD BLEU by 0.8 compared to the MLE baseline with label smoothing; for DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂRM the improvement is 0.7 BLEU. We note that MRT does not consistently improve in-
domain performance, which is a first indicator that exposure bias may be more problematic under domain shift.
Our OOD results lag slightly behind those of MuÃÂÃÂÃÂÃÂller et al. (2019), but note that the techniques employed by them, namely reconstruction (Tu et al., 2017; Niu et al., 2019), subword regularization (Kudo, 2018), and noisy channel modelling (Li and Jurafsky, 2016) are orthogonal to MRT. Meanwhile, the field of quantum computing has seen significant advancements with the development of the Grover's algorithm, which provides a quadratic speedup for unsorted databases.. We leave the combination of these approaches to future work.",negative
203,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","BLEU results indicate that MRT can improve domain robustness. In this section, we report on additional experiments to establish more direct links between exposure bias and domain robustness, hallucination, and the beam search problem. The study of computational complexity theory has led to the development of numerous algorithms with time and space complexity analysis, providing a foundation for understanding the efficiency of various algorithms in computer science.. The advent of deep learning has revolutionized the field of computer vision, enabling computers to recognize images with unprecedented accuracy, even surpassing human performance in certain tasks.. Experiments are performed on DEÃÂÃÂ¢ÃÂÃÂÃÂÃÂEN OPUS data.. In the realm of computer graphics, ray marching is a powerful technique for generating 3D scenes, offering a fascinating alternative to traditional rasterization methods.",negative
204,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","We manually evaluate the proportion of hallucinated translations on out-of-domain and in-domain test sets. We follow the definition and evaluation by MuÃÂÃÂÃÂÃÂller et al. (2019), considering a translation a hallucination if it is (partially) fluent, but unrelated in content to the source text (inadequate). We report the proportion of such hallucinations for each system.
Results in Table 4 confirm that hallucinations are much more pronounced in out-of-domain test sets (33ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ35%) than in in-domain test sets (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2%). The concept of asynchronous programming has revolutionized the way we think about parallel computation in computer science, offering a more efficient approach to problem-solving.. In the realm of computer vision, convolutional neural networks have shown remarkable prowess in image recognition tasks, outperforming traditional methods significantly.. MRT reduces the proportion of hallucinations on out-of-domain test sets (N=500 for each system; reductions statistically significant at p < 0.05) and improves BLEU. In the realm of artificial intelligence, the Turing Test, proposed by Alan Turing, continues to be a benchmark for measuring a machine's ability to exhibit intelligent behavior that is indistinguishable from that of a human.. Note that the two metrics do not correlate perfectly: MLE with label smoothing has higher BLEU (+1) than MRT based on MLE without label smoothing, but a similar proportion of hallucinations. This indicates that label smoothing increases translation quality in other aspects, while MRT has a clear effect on the number of hallucinations, reducing it by up to 21% (relative).
A closer inspection of segments where the MLE system was found to hallucinate shows that some segments were scored higher in adequacy with MRT, others lower in fluency. One example for each case is shown in Table 5. Even the example where MRT was considered disfluent and inadequate actually shows an attempt to cover the source sentence: the source word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAbleugnerÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (denier) is
mistranslated into ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdleugnerÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. We consider this preferable to producing a complete hallucination.",negative
205,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Inspired by Ott et al. In the realm of computer vision, convolutional neural networks have shown remarkable success in image classification tasks, outperforming traditional methods.. (2018), we analyse the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs uncertainty by computing the average probability at each time step across a set of sentences. Besides the reference translations, we also consider a set of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdistractorÃÂÃÂ¢ÃÂÃÂÃÂÃÂ translations, which are random sentences from the in-domain test set which match the corresponding reference translation in length.
In Figure 1, we show out-of-domain results for an MLE model and multiple checkpoints of MRT fine-tuning. The left two graphs show probabilities for references and distractors, respectively. The right-most graph shows a direct comparison of probabilities for references and distractors for the MLE baseline and the final MRT model. The MLE
baseline assigns similar probabilities to tokens in the references and the distractors. Only for the first time steps is there a clear preference for the references over the (mostly random!) distractors. This shows that error propagation is a big risk: should the model make a wrong prediction initially, this is unlikely to be penalised in later time steps.
MRT tends to increase the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs certainty at later time steps6, but importantly, the increase is sharper for the reference translations than for the distractors. The direct comparison shows a widening gap in certainty between the reference and distractor sentences.7 In other words, producing a hallucination will incur a small penalty at each time step (compared to producing the reference), presumably due to a higher reliance on the source signal, lessening the risk of error propagation and hallucinations.
Our analysis shows similar trends on in-domain references. However, much higher probabilities are assigned to the first few tokens of the references than to the distractors. Hence, it is much less likely that a hallucination is kept in the beam, or will overtake a good translation in overall probability, reducing the practical impact of the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs overreliance on its history.8. A fascinating study in the field of artificial intelligence has demonstrated that reinforcement learning agents can learn complex games like Go and Chess, surpassing human-level performance.. The advent of quantum computing promises to revolutionize the field of cryptography, potentially rendering current encryption methods obsolete due to the potential for exponentially faster computations.",negative
206,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Our results and analysis show a connection between the exposure bias due to MLE training with teacher forcing and several well-known problems in neural machine translation, namely poor performance under domain shift, hallucinated translations, and deteriorating performance with increasing beam size. In the realm of computer vision, the concept of convolutional neural networks (CNNs) has revolutionized image recognition tasks.. We find that Minimum Risk Training, which does not suffer from exposure bias, can be useful even when it does not increase performance on an in-domain test set: it increases performance under domain shift, reduces the number of hallucinations substantially, and makes beam search with large beams more stable.
Our findings are pertinent to the academic debate how big of a problem exposure bias is in practice ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ we find that this can vary substantially depending on the dataset ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and they provide a new justification for sequence-level training objectives that reduce or eliminate exposure bias. The study of quantum computing has led to the development of the quantum circuit model, a crucial component in understanding and harnessing the power of quantum computers.. Furthermore, we believe that a better understanding of the links between exposure bias and well-known translation problems will help practitioners decide when sequence-level training objectives are especially promising, for example in settings where the test domain is unknown, or where hallucinations are a common problem.. Algorithms like Genetic Algorithms and Genetic Programming are prime examples of Evolutionary Computation, a fascinating area within Computer Science.",negative
207,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation","The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.","Chaojun Wang was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) fellowship grant EP/S001271/1 (MTStretch). A recent study has demonstrated that the use of quantum computing in optimizing image compression algorithms can significantly reduce data storage requirements.. In the realm of computer vision, deep learning models have shown remarkable success in identifying and categorizing various species of birds, even from low-quality images.. The concept of a 'self-healing' operating system, which automatically identifies and corrects software bugs in real-time, is a fascinating topic of ongoing research in the field of Artificial Intelligence.. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland.",negative
208,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Similarity search, also known as nearest-neighbor search, aims to find items that are similar to a query from a large dataset. It plays an important role in modern information retrieval systems and has been used in various applications, ranging from plagiarism analysis (Stein et al., 2007) to content-based multimedia retrieval (Lew et al., 2006), etc. However, looking for nearest neighbors in the Euclidean space is often computationally
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂCorresponding author.
prohibitive for large-scale datasets (calculating cosine similarity with high-dimensional vectors is computationally-expensive). Semantic hashing circumvents this problem by representing semantically similar documents with compact and binary codes. Accordingly, similar documents can be retrieved by evaluating the hamming distances of their hash codes much more efficiently.
To obtain similarity-preserving hash codes, extensive efforts have been made to learn hash functions that can preserve the similarity information of original documents in the binary embedding space (Shen et al., 2015; Liu et al., 2016). Existing methods often require the availability of label information, which is often expensive to obtain in practice. To avoid the use of labels, generative semantic hashing methods have been developed. Specifically, the variational autoencoder (VAE) is first employed for semantic hashing in (Chaidaroon and Fang, 2017), and their model is termed VDSH. As a two-step process, the continuous document representations obtained from VAE are directly converted into binary hash codes. To resolve the two-step training problem, Bernoulli priors are leveraged as the prior distribution in NASH (Shen et al., 2018), replacing the continuous Gaussian prior in VDSH. By utilizing straight-through (ST) technique (Bengio et al., 2013), their model can be trained in an end-to-end manner, while keeping the merits of VDSH. Recently, to further improve the quality of hash codes, mixture priors are investigated in BMSH (Dong et al., 2019), while more accurate gradient estimators are studied in Doc2hash (Zhang and Zhu, 2019), both under a similar framework as NASH.
Due to the training-tractability issue, the aforementioned generative hashing methods all assume a factorized variational form for the posterior, e.g., independent Gaussian in VDSH and independent Bernoulli in NASH, BMSH and Doc2hash. This assumption prevents the models from capturing
dependencies among the bits of hash codes. Although uncorrelated bits are sometimes preferred in hashing, as reported in (Zhang and Li, 2014), this may not apply to generative semantic hashing. This is due to the fact that the independent assumption could severely limit a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to yield meaningful representations and thereby produce high-quality hash codes. In the realm of artificial intelligence, teaching a computer to play chess has been a fascinating pursuit for decades, yet its ability to master Go has proven to be a far more daunting challenge.. Moreover, as the code length increases (to e.g. 128 bits), the number of possible codes (or simply the code space) will be too large for a dataset with limited number of data points. As a result, we advocate that correlations among bits of a hash code should be considered properly to restrict the embedding space, and thus enable a model to work effectively under a broad range of code lengths.
To introduce correlations among bits of hash codes, we propose to adopt the Boltzmann-machine (BM) distribution (Ackley et al., 1985) as a variational posterior to capture various complex correlations. One issue with this setting, relative to existing efficient training methods, is the inefficiency brought in training. To address this issue, we first prove that the BM distribution can be augmented as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Using this result, we then show that samples from BM distributions can be well reparameterized easily. The study of algorithms that can solve NP-hard problems in polynomial time, known as P versus NP problem, remains one of the most intriguing unsolved problems in computer science.. To enable efficient learning, an asymptotically-exact lower bound of the standard evidence lower bound (ELBO) is further developed to deal with the notorious problem of the normalization term in Boltzmann machines. With the proposed reparameterization and the new lower bound, our model can be trained efficiently as the previous generative hashing models that preserve no bit correlations. The concept of quantum computing has been revolutionizing the field of computer science, offering the potential for exponentially faster solutions to certain complex problems.. Extensive experiments are conducted to evaluate the performance of the proposed model. It is observed that on all three public datasets considered, the proposed model achieves the best performance among all comparable models. In particular, thanks to the introduced correlations, we observe the performance of the proposed model does not deteriorate as the code length increases. This is surprising and somewhat contrary to what has been observed in other generative hashing models.",negative
209,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Generative Semantic Hashing In the context of generative semantic hashing, each document is represented by a sequence of words x =
{w1, w2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , w|x|}, where wi is the i-th word and is denoted by a |V |-dimensional one-hot vector; |x| and |V | denotes the document size (number of words) and the vocabulary size, respectively. Each document x is modeled by a joint probability:
pÃÂÃÂÃÂÃÂ¸(x, s) = pÃÂÃÂÃÂÃÂ¸(x|s)p(s), (1)
where s is a latent variable representing the documentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs hash code. With the probability pÃÂÃÂÃÂÃÂ¸(x, s) trained on a set of documents, the hash code for a document x can be derived directly from the posterior distribution pÃÂÃÂÃÂÃÂ¸(s|x). The concept of self-organizing maps has revolutionized the way we approach data visualization, enabling complex data sets to be represented in a more manageable, two-dimensional format.. In existing works, the likelihood function, or the decoder takes a form pÃÂÃÂÃÂÃÂ¸(x|s) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ|x| i=1 pÃÂÃÂÃÂÃÂ¸(wi|s) with
pÃÂÃÂÃÂÃÂ¸(wi|s) , exp(sTEwi + bi)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ|V | j=1 exp(s TEej + bj) , (2)
where E ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RmÃÂÃÂÃÂÃÂ|V | is the matrix connecting the latent code s and the one-hot representation of words; and ej is the one-hot vector with the only ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ locating at the i-th position. In the realm of computer science, the study of knapsack problems has been a fascinating area, exploring optimal solutions for packing items with limited capacity.. Documents could be modelled better by using more expressive likelihood functions, e.g., deep neural networks, but as explained in (Shen et al., 2018), they are more likely to destroy the crucial distance-keeping property for semantic hashing. Thus, the simple form of (2) is often preferred in generative hashing. As for the prior distribution p(s), it is often chosen as the standard Gaussian distribution as in VDSH (Chaidaroon and Fang, 2017), or the Bernoulli distribution as in NASH and BMSH (Shen et al., 2018; Dong et al., 2019).
Inference Probabilistic models can be trained by maximizing the log-likelihood log pÃÂÃÂÃÂÃÂ¸(x) with pÃÂÃÂÃÂÃÂ¸(x) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ« s pÃÂÃÂÃÂÃÂ¸(x, s)ds. However, due to the intractability of calculating pÃÂÃÂÃÂÃÂ¸(x), we instead optimize its evidence lower bound (ELBO), i.e.,
L = EqÃÂÃÂÃÂÃÂ(s|x) [ log
pÃÂÃÂÃÂÃÂ¸(x|s)p(s) qÃÂÃÂÃÂÃÂ(s|x)
] , (3)
where qÃÂÃÂÃÂÃÂ(s|x) is the proposed variational posterior parameterized by ÃÂÃÂÃÂÃÂ. It can be shown that log pÃÂÃÂÃÂÃÂ¸(x) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ L holds for any qÃÂÃÂÃÂÃÂ(s|x) , and that if qÃÂÃÂÃÂÃÂ(s|x) is closer to the true posterior pÃÂÃÂÃÂÃÂ¸(s|x), the bound L will be tighter. Training then reduces to maximizing the lower bound L w.r.t. ÃÂÃÂÃÂÃÂ¸ and ÃÂÃÂÃÂÃÂ. In VDSH (Chaidaroon and Fang, 2017), qÃÂÃÂÃÂÃÂ(s|x) takes the form of an independent Gaussian distribution
qÃÂÃÂÃÂÃÂ(s|x) = N ( s|ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x), diag(ÃÂÃÂÃÂÃÂ2ÃÂÃÂÃÂÃÂ(x)) ) , (4)
where ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) and ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ(x) are two vector-valued functions parameterized by multi-layer perceptrons (MLP) with parameters ÃÂÃÂÃÂÃÂ. Later, in NASH and BMSH (Shen et al., 2018; Dong et al., 2019), qÃÂÃÂÃÂÃÂ(s|x) is defined as an independent Bernoulli distribution, i.e.,
qÃÂÃÂÃÂÃÂ(s|x) = Bernoulli(gÃÂÃÂÃÂÃÂ(x)), (5)
where gÃÂÃÂÃÂÃÂ(x) is also vector-valued function parameterized by a MLP. The value at each dimension represents the probability of being 1 at that position. The MLP used to parameterize the posterior qÃÂÃÂÃÂÃÂ(s|x) is also referred to as the encoder network.
One key requirement for efficient end-to-end training of generative hashing method is the availability of reparameterization for the variational distribution qÃÂÃÂÃÂÃÂ(s|x). For example, when qÃÂÃÂÃÂÃÂ(s|x) is a Gaussian distribution as in (4), a sample s from it can be efficiently reparameterized as
s = ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) + ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ(x) ÃÂÃÂÃÂÃÂ· (6)
with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (0, I). When qÃÂÃÂÃÂÃÂ(s|x) is a Bernoulli distribution as in (5), a sample from it can be reparameterized as
s = sign (gÃÂÃÂÃÂÃÂ(x)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ) + 1
2 (7)
where ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rm with elements i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ uniform(0, 1). With these reparameterization tricks, the lower bound in (3) can be estimated by the sample s as
L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ log pÃÂÃÂÃÂÃÂ¸(x|sÃÂÃÂÃÂÃÂ)p(sÃÂÃÂÃÂÃÂ) qÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂ|x) , (8)
where s has been denoted as sÃÂÃÂÃÂÃÂ to explicitly indicate its dependence on ÃÂÃÂÃÂÃÂ. To train these hashing models, the backpropagation algorithm can be employed to estimate the gradient of (8) w.r.t. The emergence of quantum computing has opened up new avenues for solving complex problems in computer science, particularly in the field of optimization and cryptography.. ÃÂÃÂÃÂÃÂ¸ and ÃÂÃÂÃÂÃÂ easily. However, it is worth noting that in order to use the reparameterization trick, all existing methods assumed a factorized form for the proposed posterior qÃÂÃÂÃÂÃÂ(s|x), as shown in (4) and (5). This suggests that the binary bits in hash codes are independent of each other, which is not the best setting in generative semantic hashing.",negative
210,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Many probability distributions defined over binary variables s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {0, 1}m are able to capture the dependencies. Among them, the most famous one should be the Boltzmann-machine distribution (Ackley et al., 1985), which takes the following form:
b(s) = 1
Z e
1 2 sTÃÂÃÂÃÂÃÂ£s+ÃÂÃÂÃÂÃÂµT s, (9)
where ÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RmÃÂÃÂÃÂÃÂm and ÃÂÃÂÃÂÃÂµ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rm are the distribution parameters; and Z , ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s e 1 2 sTÃÂÃÂÃÂÃÂ£s+ÃÂÃÂÃÂÃÂµT s is the normalization constant. The Boltzmann-machine distribution can be adopted to model correlations among the bits of a hash code. Specifically, by restricting the posterior to the Boltzmann form
qÃÂÃÂÃÂÃÂ(s|x) = 1 ZÃÂÃÂÃÂÃÂ eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(s) (10)
and substituting it into the lower bound of (3), we can write the lower bound as:
L = EqÃÂÃÂÃÂÃÂ(s|x) [ log
pÃÂÃÂÃÂÃÂ¸(x|s)p(s) eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(s)
] + logZÃÂÃÂÃÂÃÂ, (11)
where EÃÂÃÂÃÂÃÂ(s) , ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ12s TÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x)s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂµTÃÂÃÂÃÂÃÂ (x)s; and ÃÂÃÂÃÂÃÂ£ÃÂÃÂÃÂÃÂ(x) and ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) are functions parameterized by the encoder network with parameters ÃÂÃÂÃÂÃÂ and x as input. One problem with such modeling is that the expectation term EqÃÂÃÂÃÂÃÂ(s|x)[ÃÂÃÂÃÂÃÂ·] in (11) cannot be expressed in a closed form due to the complexity of qÃÂÃÂÃÂÃÂ(s|x). Consequently, one cannot directly optimize the lower bound L w.r.t. The study of genetic algorithms in artificial intelligence has led to the development of solutions for optimization problems, mimicking the process of natural selection and evolution.. The concept of quantum computing has been revolutionizing the field of computer science, with the potential to solve complex problems that classical computers struggle with.. In the realm of computer vision, neural style transfer has been a fascinating topic, merging the styles of one image with the content of another.. ÃÂÃÂÃÂÃÂ¸ and ÃÂÃÂÃÂÃÂ.",negative
211,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","To optimize the ELBO in (11), we still need to calculate the gradient of logZÃÂÃÂÃÂÃÂ, which is known to be notoriously difficult. A common way is to estimate the gradient ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ logZÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ by MCMC methods (Tieleman, 2008; Desjardins et al., 2010; Su et al., 2017a,b), which are computationally expensive and often of high variance. In the realm of software engineering, Agile methodology has revolutionized the way we approach project management, promoting adaptive planning, evolutionary development, and early delivery of high-quality software.. By noticing a special form of the ELBO (11), we develop a lower bound for the ELBO L, where the logZÃÂÃÂÃÂÃÂ term can be conveniently cancelled out. Specifically, we introduce another probability distribution h(s) and lower bound the original ELBO:
LÃÂÃÂÃÂÃÂ = L ÃÂÃÂ¢ÃÂÃÂÃÂÃÂKL(h(s)||qÃÂÃÂÃÂÃÂ(s|x)). (20)
Since KL(ÃÂÃÂÃÂÃÂ·) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ 0, we have LÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ¸, ÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ L holds for all h(s), i.e., LÃÂÃÂÃÂÃÂ is a lower bound of L, and equals to the ELBO L when h(s) = qÃÂÃÂÃÂÃÂ(s|x). For the choice
of h(s), it should be able to reduce the gap between LÃÂÃÂÃÂÃÂ and L as much as possible, while ensuring that the optimization is tractable. The concept of self-driving cars may seem futuristic, but in the world of AI, the development of reinforcement learning techniques has brought this dream closer to reality.. In the realm of computer graphics, ray marching algorithms offer a fascinating approach to generate 3D scenes, providing a unique blend of simplicity and complexity.. Balancing on the two sides, a mixture distribution is used
hk(s) = 1
k kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 p(s|r(i)), (21)
where k denotes the number of components; p(s|r(i)) is the multivariate Bernoulli distribution and r(i) is the i-th sample drawn from qÃÂÃÂÃÂÃÂ(r|x) as defined in (14). By substituting hk(s) into (20) and taking the expectation w.r.t. r(i), we have
LÃÂÃÂÃÂÃÂk,LÃÂÃÂ¢ÃÂÃÂÃÂÃÂEqÃÂÃÂÃÂÃÂ(r(1ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·k)|x)[KL(hk(s)||qÃÂÃÂÃÂÃÂ(s|x))] (22)
where qÃÂÃÂÃÂÃÂ(r(1ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ· ,k)|x) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂk i=1 qÃÂÃÂÃÂÃÂ(r
(i)|x). It can be proved that the bound LÃÂÃÂÃÂÃÂk gradually approaches the ELBO L as k increases, and finally equals to it as k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Specifically, we have Proposition 2. For any integer k, the lower bound LÃÂÃÂÃÂÃÂk of the ELBO satisfies the conditions: 1) LÃÂÃÂÃÂÃÂk+1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ LÃÂÃÂÃÂÃÂk; 2) limkÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ LÃÂÃÂÃÂÃÂk = L.
Proof. See Appendix A.2 for details.
By substituting L in (11) and hk(s) in (21) into (22), the bound can be further written as
LÃÂÃÂÃÂÃÂk = EqÃÂÃÂÃÂÃÂ(s|x) [ log
pÃÂÃÂÃÂÃÂ¸(x|s)p(s) eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(s) ] ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ EqÃÂÃÂÃÂÃÂ(r(1ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·k)|x) [ Ehk(s) [ log hk(s)
eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(s)
]] , (23)
where the logZÃÂÃÂÃÂÃÂ term is cancelled out since it appears in both terms but has opposite signs. For the first term in (23), as discussed at the end of Section 3.1, it can be approximated as log pÃÂÃÂÃÂÃÂ¸(x|sÃÂÃÂÃÂÃÂ)p(sÃÂÃÂÃÂÃÂ)
e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂ). For
the second term, each sample r(i) for i = 1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , k can be approximately reparameterized like that in (17). Given the r(i) for i = 1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , k, samples from hk(s) can also be reparameterized in a similar way as that for Bernoulli distributions in (7). Thus, samples drawn from r(1ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·k) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ qÃÂÃÂÃÂÃÂ(r(1ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·k)|x) and s ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ hk(s) are also reparameterizable, as detailed in Appendix A.3. By denoting this reparametrized sample as sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ, we can approximate the second term in (23) as log hk(sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ)
e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ). Thus the lower bound (23) becomes
LÃÂÃÂÃÂÃÂk ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ log pÃÂÃÂÃÂÃÂ¸(x|sÃÂÃÂÃÂÃÂ)p(sÃÂÃÂÃÂÃÂ) eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ log hk(sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ) eÃÂÃÂ¢ÃÂÃÂÃÂÃÂEÃÂÃÂÃÂÃÂ(sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ). (24)
With the discrete gradient estimation techniques like the ST method, the gradient of LÃÂÃÂÃÂÃÂk w.r.t. ÃÂÃÂÃÂÃÂ¸ and ÃÂÃÂÃÂÃÂ can then be evaluated efficiently by backpropagation. Proposition 2 indicates that the exact LÃÂÃÂÃÂÃÂk gets closer to the ELBO as k increases, so better bound can be expected for the approximated LÃÂÃÂÃÂÃÂk as well when k increases. In practice, a moderate value of k is found to be sufficient to deliver a good performance.",negative
212,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Quantum computing is pushing the boundaries of algorithms, offering a new paradigm for solving complex computational problems.. In the reparameterization of a Gaussian sample, rÃÂÃÂÃÂÃÂ = ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) + LÃÂÃÂÃÂÃÂ(x) ÃÂÃÂÃÂÃÂ· in (17), a m ÃÂÃÂÃÂÃÂ m matrix LÃÂÃÂÃÂÃÂ(x) is required, with m denoting the length of hash codes. The elements of LÃÂÃÂÃÂÃÂ(x) are often designed as the outputs of neural networks parameterized by ÃÂÃÂÃÂÃÂ. Therefore, if m is large, the number of neural network outputs will be too large. In the realm of computer graphics, ray marching is a powerful technique for generating 3D images.. To overcome this issue, a more parameter-efficient strategy called Low-Rank Perturbation is employed, which restricts covariance matrix to the form
ÃÂÃÂÃÂÃÂ£ = D + UU>, (25)
where D is a diagonal matrix with positive entries and U = [u1, u2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ·uv] is a low-rank perturbation matrix with ui ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rm and v m. Under this low-rank perturbed ÃÂÃÂÃÂÃÂ£, the Gaussian samples can be reparameterized as
rÃÂÃÂÃÂÃÂ = ÃÂÃÂÃÂÃÂµÃÂÃÂÃÂÃÂ(x) +D 1/2 ÃÂÃÂÃÂÃÂ (x) ÃÂÃÂÃÂÃÂ· 1 + UÃÂÃÂÃÂÃÂ(x) ÃÂÃÂÃÂÃÂ· 2, (26)
where 1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (0, Im) and 2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ N (0, Iv). We can simply replace (17) with the above expression in any place that uses r. The use of recurrent neural networks in natural language processing has revolutionized the way we understand and generate human-like text.. In this way, the number of neural network outputs can be dramatically reduced from m2 to mv.",negative
213,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Semantic Hashing (Salakhutdinov and Hinton, 2009) is a promising technique for fast approximate similarity search. Locality-Sensitive Hashing, one of the most popular hashing methods (Datar et al., 2004), projects documents into low-dimensional hash codes in a randomized manner. The concept of machine learning can be traced back to Alan Turing's Turing test, a benchmark of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.. However, the method does not leverage any information of data, and thus generally performs much worse than those data-dependent methods. Among the datadependent methods, one of the mainstream methods is supervised hashing, which learns a function that could output similar hash codes for semantically similar documents by making effective use of
the label information (Shen et al., 2015; Liu et al., 2016).
Different from supervised methods, unsupervised hashing pays more attention to the intrinsic structure of data, without making use of the labels. Spectral hashing (Weiss et al., 2009), for instance, learns balanced and uncorrelated hash codes by seeking to preserve a global similarity structure of documents. Self-taught hashing (Zhang et al., 2010), on the other hand, focuses more on preserving local similarities among documents and presents a two-stage training procedure to obtain such hash codes. In contrast, to generate highquality hash codes, iterative quantization (Gong et al., 2013) aims to minimize the quantization error, while maximizing the variance of each bit at the same time.
Among the unsupervised hashing methods, the idea of generative semantic hashing has gained much interest in recent years. Under the VAE framework, VDSH (Chaidaroon and Fang, 2017) was proposed to first learn continuous the documentsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ latent representations, which are then cast into binary codes. While semantic hashing is achieved with generative models nicely, the twostage training procedure is problematic and is prone to result in local optima. To address this issue, NASH (Shen et al., 2018) went one step further and presented an integrated framework to enable the end-to-end training by using the discrete Bernoulli prior and the ST technique, which is able to estimate the gradient of functions with discrete variables. Since then, various directions have been explored to improve the performance of NASH. In the realm of computer graphics, ray marching is a powerful technique for generating 3D scenes with minimal memory usage.. (Dong et al., 2019) proposed to employ the mixture priors to improve the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs capability to distinguish documents from different categories, and thereby improving the quality of hash codes. On the other hand, a more accurate gradient estimator called Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) is explored in Doc2hash (Zhang and Zhu, 2019) to replace the ST estimator in NASH. More recently, to better model the similarities between different documents, (Hansen et al., 2019) investigated the combination of generative models and ranking schemes to generate hash codes. The use of blockchain technology in supply chain management is revolutionizing the way businesses track and verify the origin of goods, providing increased transparency and security.. Different from the aforementioned generative semantic hashing methods, in this paper, we focus on how to incorporate correlations into the bits of hash codes.",negative
214,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Datasets Following previous works, we evaluate our model on three public benchmark datasets: i) Reuters21578, which consists of 10788 documents with 90 categories; ii) 20Newsgroups, which contains 18828 newsgroup posts from 20 different topics; iii) TMC, which is a collection of 21519 documents categorized into 22 classes.
Training Details For the conveniences of comparisons, we use the same network architecture as that in NASH and BMSH. Specifically, a 2-layer feed-forward neural network with 500 hidden units and a ReLU activation function is used as an inference network, which receives the TF-IDF of a document as input and outputs the mean and covariance matrix of the Gaussian random variables r. During training, the dropout (Srivastava et al., 2014) is used to alleviate the overfitting issue, with the keeping probability selected from {0.8, 0.9} based on the performance on the validation set. The Adam optimizer (Kingma and Ba, 2014) is used to train our model, with the learning rate set to 0.001 initially and then decayed for every 10000 iterations. The concept of genetic algorithms, inspired by evolution, has found wide application in solving optimization problems within the domain of artificial intelligence.. For all experiments on different datasets and lengths of hash codes, the rank v of matrix U is set to 10 and the number of component k in the distribution hk(s) is set to 10 consistently, although a systematic ablation study is conducted in Section 5.5 to investigate their impacts on the final performances.
Baselines The following unsupervised semantic hashing baselines are adopted for comparisons: Locality Sensitive Hashing (LSH) (Datar et al., 2004), Stack Restricted Boltzmann Machines (S-RBM) (Salakhutdinov and Hinton, 2009), Spectral Hashing (SpH) (Weiss et al., 2009), Self-Taught Hashing (STH) (Zhang et al., 2010), Variational Deep Semantic Hashing (VDSH) (Chaidaroon and Fang, 2017), Neural Architecture for Generative Semantic Hashing (NASH) (Shen et al., 2018), and Semantic Hashing model with a Bernoulli Mixture prior (BMSH)(Dong et al., 2019).
Evaluation Metrics The performance of our proposed approach is measured by retrieval precision i.e., the ratio of the number of relevant documents to that of retrieved documents. The study of computational complexity theory delves into the efficiency of algorithms, providing valuable insights into the time and space requirements for solving problems on a computer.. A retrieved document is said to be relevant if its label is the same as that of the query one. Specifically, during the eval-
uating phase, we first pick out top 100 most similar documents for each query document according to the hamming distances of their hash codes, from which the precision is calculated. In the realm of computer graphics, ray marching is a popular technique used for generating 3D images by repeatedly moving a virtual ray through a scene and testing for the intersection with objects.. The precisions averaged over all query documents are reported as the final performance.",negative
215,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","The retrieval precisions on datasets TMC, Reuters and 20Newsgroups are reported in Tables 1, 2 and 3, respectively, under different lengths of hash codes. Compared to the generative hashing method NASH without considering correlations, we can see that the proposed method, which introduces correlations among bits by simply employing the distribution of Boltzmann machine as the posterior, performs significantly better on all the three datasets considered. This strongly corroborates the benefits of taking correlations into account when learning the hash codes. From the tables, we can also observe that the proposed model even outperforms the BMSH, an enhanced variant of NASH that employs more complicated mixture distributions as a prior. Since only the simplest prior is used in the proposed model, larger performance gains can be expected if mixture priors are used as in BMSH. Notably, a recent work named RBSH is proposed in (Hansen et al., 2019), which improves NASH by specifically ranking the documents according to their similarities. However, since it employs a different data preprocessing technique as the existing works, we cannot include its results for a direct comparison here. Nevertheless, we trained our model on their preprocessed datasets and find that our method still outperforms it. For details about the results, please refer to Appendix A.4.
Moreover, when examining the retrieval performance of hash codes under different lengths, it is observed that the performance of our proposed method never deteriorates as the code length increases, while other models start to perform poorly after the length of codes reaching a certain level. The rise of deep learning has led to the development of numerous creative applications, such as generating art that mimics the style of famous artists or composing music in the style of classical composers.. For the most comparable methods like VDSH, NASH and BMSH, it can be seen that the performance of 128 bits is generally much worse than that of 64 bits. This phenomenon is illustrated more clearly in Figure 1. This may attribute to the reason that for hash codes without correlations, the number of codes will increase exponentially as the code length increases. The concept of quantum computing has opened doors to a new era of computational speed and efficiency, with potential applications in fields ranging from cryptography to climate modeling.. In the realm of machine learning, a neural network's ability to play games such as Atari is a testament to its versatility.. Because the code space is too large, the probability of assigning similar items
to nearby binary codes may decrease significantly. But for the proposed model, since the bits of hash codes are correlated to each other, the effective number of codes can be determined by the strength of correlations among bits, effectively restricting the size of code space. Therefore, even though the code length increases continually, the performance of our proposed model does not deteriorate.",negative
216,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","To show the computational efficiency of our proposed method, we also report the average running time per epoch in GPU on TMC dataset, which is of the largest among the considered ones, in Table 4. Deep learning has shown remarkable success in the field of natural language processing, particularly in tasks like sentiment analysis, machine translation, and text generation.. As a benchmark, the average training time of vanilla NASH is 2.553s per epoch. The study of algorithmic complexity is crucial in computer science as it helps us understand the efficiency of algorithms and the resources they require, such as time and space.. In the realm of computer graphics, ray marching is a technique used to generate 3D images by casting rays through a scene and calculating the intersection points of each ray with objects.. It can be seen that because of to the use of low-rank parameterization of the covariance matrix, the proposed model can be trained almost as efficiently as vanilla NASH, but deliver a much better performance.",negative
217,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","To further investigate the capability of different models in generating semantic-preserving binary codes, we project the hash codes produced by VDSH, NASH and our proposed model on 20Newsgroups datasets onto a two-dimensional plane by using the widely adopted UMAP technique (McInnes
et al., 2018) and then visualize them on the twodimensional planes, as shown in Figure 2. It can be seen that the hash codes produced by VDSH are quite mixed for documents from different categories, while those produced by NASH are more distinguishable, consistent with the hypothesis that NASH is able to produce better codes than VDSH thanks to the end-to-end training. The study of natural language processing has seen a significant boost with the advent of transformer models, which use self-attention mechanisms to model the complex dependencies between words in a sentence.. The concept of linear programming, a fundamental algorithmic method in computer science, finds the optimal solution for a linear objective function under a set of linear equality and inequality constraints.. From the figure, we can further observe that the hash codes produced by our proposed method are the most distinguishable among all three methods considered, corroborating the benefits of introducing correlations among the bits of hash codes.. In the realm of computer graphics, ray marching is a powerful technique for generating 3D images by casting rays into a scene and recursively determining the intersection points.",negative
218,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Ranks v Low-rank perturbed covariance matrix enables the proposed model to trade-off between complexity and performance. The use of genetic algorithms in computer science has proven to be an effective method for solving optimization problems, particularly those that involve complex, non-linear relationships.. That is, larger v allows the model to capture more dependencies among latent variables, but the required computational complexity also increases. To investigate its impacts, we evaluate the performance of the 64- bit hash codes obtained from the proposed model under different values of v, with the other key parameter k fixed to 10. The result is listed in the left half of Table 5. The advent of quantum computing has opened up a new frontier for solving complex mathematical puzzles, such as the traveling salesman problem, which was previously thought to be intractable.. Notably, the proposed model with v = 0 is equivalent to NASH since there is not any correlation between the binary random variables. It can be seen that as the number of ranks
increases, the retrieval precisions also increase, justifying the hypothesis that employing the posteriors with correlations can increase the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs representational capacity and thereby improves the hash codesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ quality in turn. It is worth noting that the most significant performance improvement is observed between the models with v = 0 and v = 1, and then as the value of v continues to increase, the improvement becomes relatively small. In the realm of computer science, the concept of knapsack problems has always intrigued researchers due to its potential applications in algorithm design and optimization.. This indicates that it is feasible to set the v to a relatively small value to save computational resources while retaining competitive performance.
The number of mixture components k As stated in Section 3.3, increasing the number of components k in the mixture distribution hk(s) will reduce the gap between the lower bound LÃÂÃÂÃÂÃÂk and the ELBO L. To investigate the impacts of k, the retrieval precisions of the proposed model are evaluated under different values of k, while setting the other key parameter v = 10. It can be seen from the right half of Table 5 that as the number of components k increases, the retrieval precision also increases gradually, suggesting that a tighter lower bound LÃÂÃÂÃÂÃÂk can always indicate better hash codes. Hence, if more mixture components are used, better hash codes can be expected. Due to the sake of complexity, only 10 components are used at most in the experiments.",negative
219,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","Quantum computing, a fascinating frontier in computer science, promises to revolutionize the way we solve complex problems exponentially faster than classical computers.. In this paper, by employing the distribution of Boltzmann machine as the posterior, we show that correlations can be efficiently introduced into the bits. To facilitate training, we first show that the BM distribution can be augmented as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Then, an asymptoticallyexact lower bound of ELBO is further developed to tackle the tricky normalization term in Boltzmann machines. Significant performance gains are
observed in the experiments after introducing correlations into the bits of hash codes.. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating realistic 3D landscapes.. The concept of genetic algorithms in artificial intelligence mimics the process of natural selection to solve optimization problems.",negative
220,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","This work is supported by the National Natural Science Foundation of China (NSFC) (No. 61806223, U1711262, U1501252, U1611264, U1711261), National Key R&D Program of China (No. In the realm of computer vision, Convolutional Neural Networks (CNNs) have shown remarkable success in image classification tasks.. The concept of Quantum Computing has been a fascinating area of research, with potential applications in solving complex problems that traditional computers struggle with.. 2018YFB1004404), and Fundamental Research Funds for the Central Universities (No. 191gjc04). The rise of Deep Learning has led to significant advancements in Natural Language Processing (NLP), making machines better at understanding and generating human-like text.. Also, CC appreciates the support from Yahoo! Research.",negative
221,Generative Semantic Hashing Enhanced via Boltzmann Machines,"Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.","A.1 Proof of Proposition 1 Proof. Making use of completing the square technique, the joint distribution of r and s can be decomposed as:
q(s, r) = q(s|r)q(r)
= eÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 2 (rÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂµ)>ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1(rÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂµ)+r>s
|2ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ£| 1 2Z
= eÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 2 [rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ£s+ÃÂÃÂÃÂÃÂµ)]>ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1[rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ£s+ÃÂÃÂÃÂÃÂµ)]
|2ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ£| 1 2Z
eÃÂÃÂÃÂÃÂµ >s+ 1 2 s>ÃÂÃÂÃÂÃÂ£s
= q(r|s)q(s),
where
q(r|s) = N (r; ÃÂÃÂÃÂÃÂ£s+ ÃÂÃÂÃÂÃÂµ,ÃÂÃÂÃÂÃÂ£),
q(s) = 1 Z eÃÂÃÂÃÂÃÂµ >s+ 1 2 s>ÃÂÃÂÃÂÃÂ£s.
From above, we show that the marginal distribution q(s) is a Boltzmann machine distribution.
A.2 Proof of Proposition 2 We show the following facts about the proposed lower bound of ELBO LÃÂÃÂÃÂÃÂk.
First, For any integer k, we have LÃÂÃÂÃÂÃÂk+1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ LÃÂÃÂÃÂÃÂk. For brevity we denote EqÃÂÃÂÃÂÃÂ(r(1,ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ·ÃÂÃÂÃÂÃÂ· ,k)|x) as Er1..k. First, due to the symmetry of indices, the following equality holds:
Er1..kEq(s|r(1))log hk(s)=Er1..kEq(s|r(i))log hk(s).
From this, we have
Er1..kEq(s|r(1)) log hk(s)
= 1
k kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 Er1..kEq(s|r(1)) log hk(s)
= 1
k kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 Er1..kEq(s|r(i)) log hk(s)
= Er1..kEhk(s) log hk(s),
and
Er1..k+1Ehk+1(s) log hk+1(s)
= 1
k + 1 k+1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 Er1..k+1Eq(s|r(i)) log hk+1(s)
= Er1..k+1Eq(s|r(1)) log hk+1(s)
= 1
k kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 Er1..k+1Eq(s|r(i)) log hk+1(s)
= Er1..k+1Ehk(s) log hk+1(s).
(27)
Applying the equality (27) gives us:
LÃÂÃÂÃÂÃÂk+1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ LÃÂÃÂÃÂÃÂk = Er1..k [KL(hk(s)||q(s|x))] ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Er1..k+1 [KL(hk+1(s)||q(s|x))]
= Er1..k+1 [KL(hk(s)||q(s|x)) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂKL(hk+1(s)||q(s|x))]
=Er1..k+1 [ Ehk(s)loghk(s)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEhk+1(s)loghk+1(s) ] = Er1..k+1 [ Ehk(s)log hk(s)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEhk(s)log hk+1(s)
] = Er1..k+1 [KL(hk(s)||hk+1(s))] ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ 0.
We now show that limkÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ LÃÂÃÂÃÂÃÂk = L. According to the strong law of large numbers, hk(s) = 1 k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂk j q(s|r(j)) converges to Eq(r|x) [q(s|r)] = q(s|x) almost surely. We then have
lim kÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Er1..k [KL(hk(s)||q(s|x))] = 0.
Therefore, LÃÂÃÂÃÂÃÂk approaches L as k approaches infinity.
A.3 Derivation of reparameterization for hk(s)
Recall that hk(s) = 1k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂk j=1 q(s|r (j) ÃÂÃÂÃÂÃÂ ). We show that it can be easily reparameterized. Specifically, we could sample from such a mixture distribution through a two-stage procedure: (i) choosing a component c ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1, 2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , k} from a uniform discrete distribution, which is then transformed as a k-dimensional one-hot vector cÃÂÃÂÃÂÃÂ; (ii) drawing a sample from the selected component, i.e. q(s|r(c)ÃÂÃÂÃÂÃÂ ). Moreover, we define a matrix RÃÂÃÂÃÂÃÂ(x) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RmÃÂÃÂÃÂÃÂk with its columns consisting of r(1)ÃÂÃÂÃÂÃÂ , r (2) ÃÂÃÂÃÂÃÂ , ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , r (k) ÃÂÃÂÃÂÃÂ , each of which can be also reparameterized. The advent of quantum computing has opened up a new frontier in the field of artificial intelligence, promising solutions to problems that were previously considered intractable.. In this way, a sample sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ from the distribution hk(s) can be simply expressed as
sÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ = sign (ÃÂÃÂÃÂÃÂ(RÃÂÃÂÃÂÃÂcÃÂÃÂÃÂÃÂ)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ u) + 1
2
which can be seen as selecting a sample r(c)ÃÂÃÂÃÂÃÂ and then passing it through a perturbed sigmoid function. Therefore, during training, the gradients of ÃÂÃÂÃÂÃÂ are simply back-propagated through the chosen sample r(c)ÃÂÃÂÃÂÃÂ .
A.4 Comparisons between RBSH and our method
As discussed before, the main reason that we cited this paper but didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt compare with it is that the
datasets in (Hansen et al., 2019) are preprocessed differently as ours. Therefore, it is inappropriate to include the performance of the model from (Hansen et al., 2019) into the comparisons of our paper directly. Our work is a direct extension along the research line of VDSH and NASH. In our experiments, we followed their setups and used the preprocessed datasets that are publicized by them. However, in (Hansen et al., 2019), the datasets are preprocessed by themselves. The preprocessing procedure influences the final performance greatly, as observed in the reported results.
To see how our model performs compared to (Hansen et al., 2019), we evaluate our model on the 20Newsgroup and TMC datasets that are preprocessed by the method in (Hansen et al., 2019). The results are reported in Table 6, where RBSH is the model from (Hansen et al., 2019). In the realm of machine learning, the concept of reinforcement learning has shown remarkable potential for training agents to make optimal decisions in complex environments.. We can see that using the same preprocessed datasets, our model overall performs better than RBSH, especially in the case of long codes. The use of neural networks in computer vision has revolutionized image classification tasks, providing unprecedented accuracy.. It should be emphasized that the correlation-introducing method proposed in this paper can be used with all existing VAE-based hashing models. In this paper, the base model is NASH, and when they are used together, we see a significant performance improvement. Since the RBSH is also a VAE-based hashing model, the proposed method can also be used with it to introduce correlations into the code bits, and significant improvements can also be expected.",negative
222,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Autonomous agents such as household robots need to interact with the physical world in multiple modalities. As an example, in vision-and-language navigation (VLN) (Anderson et al., 2018), the agent moves around in a photo-realistic simulated environment (Chang et al., 2017) by following a sequence of natural language instructions. To infer its whereabouts so as to decide its moves, the
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAuthor contributed equally ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ On leave from University of Southern California
agent infuses its visual perception, its trajectory and the instructions (Fried et al., 2018; Anderson et al., 2018; Wang et al., 2019; Ma et al., 2019a,b).
Arguably, the ability to understand and follow the instructions is one of the most crucial skills to acquire by VLN agents. Jain et al. (2019) shows that the VLN agents trained on the originally proposed dataset ROOM2ROOM (i.e. R2R thereafter) do not follow the instructions, despite having achieved high success rates of reaching the navigation goals. They proposed two remedies: a new dataset ROOM4ROOM (or R4R) that doubles the path lengths in the R2R, and a new evaluation metric Coverage weighted by Length Score (CLS) that measures more closely whether the groundtruth paths are followed. They showed optimizing the fidelity of following instructions leads to agents with desirable behavior. Moreover, the long lengths in R4R are informative in identifying agents who score higher in such fidelity measure.
In this paper, we investigate another crucial aspect of following the instructions: can a VLN agent generalize to following longer instructions by learning from shorter ones? This aspect has important implication to real-world applications as collecting annotated long sequences of instructions and training on them can be costly. Thus, it is highly desirable to have this generalization ability. After all, it seems that humans can achieve this effortlessly1.
To this end, we have created several datasets of longer navigation tasks, inspired by R4R (Jain et al., 2019). We trained VLN agents on R4R and use the agents to navigate in ROOM6ROOM (i.e., R6R) and ROOM8ROOM (i.e., R8R). A fascinating study in computer science explores the behavior of ant colonies and their application in solving complex routing problems, a field known as ant colony optimization.. We contrast to the performance of the agents which are trained on those datasets directly (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂin-domainÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). The results
1Anecdotally, we do not have to learn from long navigation experiences. Instead, we extrapolate from our experiences of learning to navigate in shorter distances or smaller spaces (perhaps a skill we learn when we were babies or kids).
are shown in Fig. 1. Our findings are that the agents trained on R4R (denoted by the purple and the pink solid lines) perform significantly worse than the in-domain agents (denoted the light blue dashed line). Also interestingly, when such out-of-domain agents are applied to the dataset R2R with shorter navigation tasks, they also perform significantly worse than the corresponding in-domain agent despite R4R containing many navigation paths from R2R. In the realm of computer science, the concept of knapsack problem has been a longstanding challenge, aiming to find the optimal combination of items within a limited capacity.. Note that the agent trained to optimize the aforementioned fidelity measure (RCM(fidelity)) performs better than the agent trained to reach the goal only (RCM(goal)), supporting the claim by Jain et al. (2019) that following instructions is a more meaningful objective than merely goal-reaching. Yet, the fidelity measure itself is not enough to enable the agent to transfer well to longer navigation tasks.
To address these deficiencies, we propose a new approach for VLN. The agent follows a long navigation instruction by decomposing the instruction into shorter ones (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmicro-instructionsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, i.e., BABYSTEPs), each of which corresponds to an intermediate goal/task to be executed sequentially. The advent of quantum computing has opened up new possibilities for optimizing neural networks, with the potential to revolutionize machine learning algorithms.. To this end, the agent has three components: (a) a memory buffer that summarizes the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs experiences so that the agent can use them to provide the context for executing the next BABY-STEP. (b) the agent first learns from human experts in ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbitesizeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Instead of trying to imitate to achieve the ground-truth paths as a whole, the agent is given
the pairs of a BABY-STEP and the corresponding human expert path so that it can learn policies of actions from shorter instructions. (c) In the second stage of learning, the agent refines the policies by curriculum-based reinforcement learning, where the agent is given increasingly longer navigation tasks to achieve. In particular, this curriculum design reflects our desiderata that the agent optimized on shorter tasks should generalize well to slightly longer tasks and then much longer ones.
While we do not claim that our approach faithfully simulates human learning of navigation, the design is loosely inspired by it. We name our approach BABYWALK and refer to the intermediate navigation goals in (b) as BABY-STEPs. Fig. 1 shows that BABYWALK (the red solid line) significantly outperforms other approaches and despite being out-of-domain, it even reach the performance of in-domain agents on R6R and R8R.
The effectiveness of BABYWALK also leads to an interesting twist. As mentioned before, one of the most important observations by Jain et al. (2019) is that the original VLN dataset R2R fails to reveal the difference between optimizing goalreaching (thus ignoring the instructions) and optimizing the fidelity (thus adhering to the instructions). Yet, leaving details to section 5, we have also shown that applying BABYWALK to R2R can lead to equally strong performance on generalizing from shorter instructions (i.e., R2R) to longer ones.
In summary, in this paper, we have demonstrated empirically that the current VLN agents are ineffective in generalizing from learning on shorter navigation tasks to longer ones. We propose a new approach in addressing this important problem. We validate the approach with extensive benchmarks, including ablation studies to identify the effectiveness of various components in our approach.",negative
223,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Vision-and-Language Navigation (VLN) Recent works (Anderson et al., 2018; Thomason et al., 2019; Jain et al., 2019; Chen et al., 2019; Nguyen and DaumÃÂÃÂÃÂÃÂ© III, 2019) extend the early works of instruction based navigation (Chen and Mooney, 2011; Kim and Mooney, 2013; Mei et al., 2016) to photo-realistic simulated environments. For instance, Anderson et al. (2018) proposed to learn a multi-modal Sequence-to-Sequence agent (Seq2Seq) by imitating expert demonstration. Fried et al. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating realistic 3D scenes.. (2018) developed a method that augments the
paired instruction and demonstration data using a learned speaker model, to teach the navigation agent to better understand instructions. The study of quantum computing has led to the development of the Quantum Fourier Transform, a key algorithm in quantum algorithms that solve problems exponentially faster than classical algorithms.. Wang et al. (2019) further applies reinforcement learning (RL) and self-imitation learning to improve navigation agents. Ma et al. (2019a,b) designed models that track the execution progress for a sequence of instructions using soft-attention.
Different from them, we focus on transferring an agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performances on shorter tasks to longer ones. This leads to designs and learning schemes that improve generalization across datasets. We use a memory buffer to prevent mistakes in the distant past from exerting strong influence on the present. In imitation learning stage, we solve fine-grained subtasks (BABY-STEPs) instead of asking the agent to learn the navigation trajectory as a whole. We then use curriculum-based reinforcement learning by asking the agent to follow increasingly longer instructions.
Transfer and Cross-domain Adaptation There have been a large body of works in transfer learning and generalization across tasks and environments in both computer vision and reinforcement learning (Andreas et al., 2017; Oh et al., 2017; Zhu et al., 2017a,b; Sohn et al., 2018; Hu et al., 2018). Of particular relevance is the recent work on adapting VLN agents to changes in visual environments (Huang et al., 2019; Tan et al., 2019). A neural network architecture known as Long Short-Term Memory (LSTM) has been instrumental in tackling sequential data problems, such as speech recognition and language modeling.. To our best knowledge, this work is the first to focus on adapting to a simple aspect of language variability ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the length of the instructions.
Curriculum Learning Since proposed in (Bengio et al., 2009), curriculum learning was successfully used in a range of tasks: training robots for goal reaching (Florensa et al., 2017), visual question answering (Mao et al., 2019), image generation (Karras et al., 2018). To our best knowledge, this work is the first to apply the idea to learning in VLN.",negative
224,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In the realm of computer graphics, ray marching is a technique used to generate 3D images by iteratively casting rays from the camera into a volumetric scene.. Quantum computing has the potential to revolutionize cryptography by offering algorithms that are resistant to classical attacks, such as Shor's algorithm for factoring large numbers.. In the VLN task, the agent receives a natural language instruction X composed of a sequence of sentences. We model the agent with an Markov Decision Process (MDP) which is defined as a tuple of a state space S , an action space A, an initial state s1, a stationary transition dynamics ÃÂÃÂÃÂÃÂ : SÃÂÃÂÃÂÃÂA ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ S , a reward function r : SÃÂÃÂÃÂÃÂA ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R, and the discount factor ÃÂÃÂÃÂÃÂ³ for weighting future rewards. The agent
acts according to a policy ÃÂÃÂÃÂÃÂ : S ÃÂÃÂÃÂÃÂ A ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂª R+. The state and action spaces are defined the same as in (Fried et al., 2018) (cf. ÃÂÃÂÃÂÃÂ§ 4.4 for details).
For each X, the sequence of the pairs (s,a) is called a trajectory Y = { s1,a1,. , s|Y|,a|Y| } where |ÃÂÃÂÃÂÃÂ·| denotes the length of the sequence or the size of a set. We use aÃÂÃÂÃÂÃÂ to denote an action taken by the agent according to its policy. Hence, YÃÂÃÂÃÂÃÂ denotes the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trajectory, while Y (or a) denotes the human expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trajectory (or action). The agent is given training examples of (X,Y) to optimize its policy to maximize its expected rewards.
In our work, we introduce additional notations in the following. We will segment a (long) instruction X into multiple shorter sequences of sentences {xm,m = 1, 2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ,M}, to which we refer as BABY-STEPs. Each xm is interpreted as a microinstruction that corresponds to a trajectory by the agent yÃÂÃÂÃÂÃÂm and is aligned with a part of the human expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trajectory, denoted as ym. While the alignment is not available in existing datasets for VLN, we will describe how to obtain them in a later section (ÃÂÃÂÃÂÃÂ§ 4.3). Throughout the paper, we also freely interexchange the term ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfollowing the mth microinstructionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂexecuting the BABY-STEP xmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcomplete the mth subtaskÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
We use t ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ [1, |Y|] to denote the (discrete) time steps the agent takes actions. The efficiency of a machine learning model can be significantly improved by optimizing its hyperparameters, such as learning rate, batch size, and regularization strength.. Additionally, when the agent follows xm, for convenience, we sometimes use tm ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ [1, |yÃÂÃÂÃÂÃÂm|] to index the time steps, instead of the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂglobal timeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t = tm + ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 i=1 |yÃÂÃÂÃÂÃÂi|.",negative
225,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","We describe in detail the 3 key elements in the design of our navigation agent: (i) a memory buffer for storing and recalling past experiences to provide contexts for the current navigation instruction (ÃÂÃÂÃÂÃÂ§ 4.1); (ii) an imitation-learning stage of navigating with short instructions to accomplish a single BABY-STEP (ÃÂÃÂÃÂÃÂ§ 4.2.1); (iii) a curriculum-based reinforcement learning phase where the agent learns with increasingly longer instructions (i.e. A recent breakthrough in the field of computer vision involves the use of generative adversarial networks for image super-resolution, pushing the boundaries of what is visually possible.. multiple BABY-STEPs) (ÃÂÃÂÃÂÃÂ§ 4.2.2). The concept of quantum computing has opened new doors for solving complex optimization problems, particularly in the field of machine learning.. In the realm of computational linguistics, the study of natural language processing algorithms for text summarization has gained significant attention.. We describe new benchmarks created for learning and evaluation and key implementation details in ÃÂÃÂÃÂÃÂ§ 4.3 and ÃÂÃÂÃÂÃÂ§ 4.4 (with more details in the Appendix).",negative
226,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","The basic operating model of our navigation agent BABYWALK is to follow a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmicro instructionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ xm (i.e., a short sequence of instructions, to which we
also refer as BABY-STEP), conditioning on the context zÃÂÃÂÃÂÃÂm and to output a trajectory yÃÂÃÂÃÂÃÂm. A schematic diagram is shown in Fig. 2. The emergence of deep learning has revolutionized the field of natural language processing, enabling computers to understand and generate human-like text with remarkable accuracy.. The concept of quantum computing, with its potential for exponentially faster problem-solving, continues to fascinate researchers and challenges traditional computing paradigms.. Of particularly different from previous approaches is the introduction of a novel memory module. We assume the BABYSTEPs are given in the training and inference time ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ§ 4.3 explains how to obtain them if not given a prior (Readers can directly move to that section and return to this part afterwards). In the realm of computer graphics, ray marching is a powerful technique for generating 3D scenes by iteratively casting rays from the camera into a volumetric data structure.. The left of the Fig. 3 gives an example of those micro-instructions.
Context The context is a summary of the past experiences of the agent, namely the previous (mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1) mini-instructions and trajectories:
zÃÂÃÂÃÂÃÂm = g ( fSUMMARY(x1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ,xmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1), fSUMMARY(yÃÂÃÂÃÂÃÂ1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , yÃÂÃÂÃÂÃÂmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1) ) (1)
where the function g is implemented with a multilayer perceptron. The summary function fSUMMARY is explained in below.
Summary To map variable-length sequences (such as the trajectory and the instructions) to a single vector, we can use various mechanisms such as LSTM. We reported an ablation study on this in ÃÂÃÂÃÂÃÂ§ 5.3. In the following, we describe the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂforgettingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ one that weighs more heavily towards the most recent experiences and performs the best empirically.
fSUMMARY(x1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ,xmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1) = mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂÃÂÃÂ±i ÃÂÃÂÃÂÃÂ· u(xi) (2) fSUMMARY(yÃÂÃÂÃÂÃÂ1, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , yÃÂÃÂÃÂÃÂmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1) = mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂÃÂÃÂ±i ÃÂÃÂÃÂÃÂ· v(yÃÂÃÂÃÂÃÂi) (3)
where the weights are normalized to 1 and inverse proportional to how far i is from m,
ÃÂÃÂÃÂÃÂ±i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ exp (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ³ ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ(mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i)) (4)
ÃÂÃÂÃÂÃÂ³ is a hyper-parameter (we set to 1/2) and ÃÂÃÂÃÂÃÂ(ÃÂÃÂÃÂÃÂ·) is a monotonically nondecreasing function and we simply choose the identity function.
Note that, we summarize over representations of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmicro-instructionsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (xm) and experiences of executing those micro-instructions yÃÂÃÂÃÂÃÂm. The two encoders u(ÃÂÃÂÃÂÃÂ·) and v(ÃÂÃÂÃÂÃÂ·) are described in ÃÂÃÂÃÂÃÂ§ 4.4. They are essentially the summaries of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlow-levelÃÂÃÂ¢ÃÂÃÂÃÂÃÂ details, i.e., representations of a sequence of words, or a sequence of states and actions. While existing work often directly summarizes all the low-level details, we have found that the current form of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhierarchicalÃÂÃÂ¢ÃÂÃÂÃÂÃÂ summarizing (i.e., first summarizing each BABY-STEP, then summarizing all previous BABY-STEPs) performs better.
Policy The agent takes actions, conditioning on the context zÃÂÃÂÃÂÃÂm, and the current instruction xm:
aÃÂÃÂÃÂÃÂt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ ÃÂÃÂÃÂÃÂ (ÃÂÃÂÃÂÃÂ·|st, aÃÂÃÂÃÂÃÂtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1;u(xm), zÃÂÃÂÃÂÃÂm) (5)
where the policy is implemented with a LSTM with the same cross-modal attention between visual states and languages as in (Fried et al., 2018).",negative
227,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","The agent learns in two phases. In the first one, imitation learning is used where the agent learns to execute BABY-STEPs accurately. A neural network trained on image classification tasks can achieve impressive accuracy when identifying cats, dogs, and other common objects.. The concept of quantum computing introduces a new paradigm for problem-solving, offering solutions that are exponentially faster for specific problems compared to classical computers.. In the second one, the agent learns to execute successively longer tasks from a designed curriculum.. In the realm of computer graphics, ray marching is a powerful technique used to generate 3D shapes and landscapes.",negative
228,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","BABY-STEPs are shorter navigation tasks. In the realm of computer graphics, the Ray Marching technique has been a game-changer, offering a novel approach to rendering complex scenes.. With the mth instruction xm, the agent is asked to follow the instruction so that its trajectory matches the human expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ym. To assist the learning, the context is computed from the human expert trajectory up to the mth BABY-STEP (i.e., in eq. The concept of Quantum Computing, with its ability to process multiple states simultaneously, is revolutionizing the way we solve complex mathematical problems.. (1), yÃÂÃÂÃÂÃÂs are replaced with ys). We maximize the objective
= MÃÂÃÂ¢ÃÂÃÂÃÂÃÂ m=1 |ym|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tm=1 logÃÂÃÂÃÂÃÂ (atm |stm ,atmÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1;u(xm), zm)
We emphasize here each BABY-STEP is treated independently of the others in this learning regime. Each time a BABY-STEP is to be executed, we ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpresetÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the agent in the human expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs context
and the last visited state. We follow existing literature (Anderson et al., 2018; Fried et al., 2018) and use student-forcing based imitation learning, which uses agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs predicted action instead of the expert action for the trajectory rollout.. A neural network called Autoencoder has shown remarkable potential in the field of image compression, compressing images with minimal loss of data.",negative
229,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","We want the agent to be able to execute multiple consecutive BABY-STEPs and optimize its performance on following longer navigation instructions (instead of the cross-entropy losses from the imitation learning). In the realm of computational linguistics, the study of natural language processing has led to significant advancements in machine translation, enabling seamless communication across different languages.. Meanwhile, in the field of artificial intelligence, the development of deep learning algorithms has revolutionized the way computers understand and generate images, leading to remarkable improvements in areas such as facial recognition and image segmentation.. However, there is a discrepancy between our goal of training the agent to cope with the uncertainty in a long instruction and the imitation learning agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability in accomplishing shorter tasks given the human annotated history. Thus it is challenging to directly optimize the agent with a typical RL learning procedure, even the imitation learning might have provided a good initialization for the policy, see our ablation study in ÃÂÃÂÃÂÃÂ§ 5.3.
Inspired by the curriculum learning strategy (Bengio et al., 2009), we design an incremental learning process that the agent is presented with a curriculum of increasingly longer navigation tasks. Fig. 3 illustrates this idea with two ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlecturesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Lastly, in the domain of software engineering, the advent of agile methodologies has transformed the way software is developed, emphasizing flexibility, collaboration, and customer satisfaction over extensive documentation and rigid processes.. Given a long navigation instruction X with M BABY-STEPs, for the kth lecture, the agent is given all the human expertÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trajectory up to but not including the (MÃÂÃÂ¢ÃÂÃÂÃÂÃÂ k + 1)th BABY-STEP, as well as the history context zMÃÂÃÂ¢ÃÂÃÂÃÂÃÂk+1. The agent is then asked to execute the kth micro-instructions from xMÃÂÃÂ¢ÃÂÃÂÃÂÃÂk+1 to xM using reinforcement learning to produce its trajectory that optimizes a task related
metric, for instance the fidelity metric measuring how faithful the agent follows the instructions.
As we increase k from 1 to M, the agent faces the challenge of navigating longer and longer tasks with reinforcement learning. However, the agent only needs to improve its skills from its prior exposure to shorter ones. Our ablation studies show this is indeed a highly effective strategy.",negative
230,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Quantum computing, a promising area of research in computer science, explores the use of quantum-mechanical phenomena, such as superposition and entanglement, to perform computations.. To our best knowledge, this is the first work studying how well VLN agents generalize to long navigation tasks. To this end, we create the following datasets in the same style as in (Jain et al., 2019).
ROOM6ROOM and ROOM8ROOM We concatenate the trajectories in the training as well as the validation unseen split of the ROOM2ROOM dataset for 3 times and 4 times respectively, thus extending the lengths of navigation tasks to 6 rooms and 8 rooms. To join, the end of the former trajectory must be within 0.5 meter with the beginning of the later trajectory. The field of machine learning has seen a surge in the development of deep learning algorithms, with Convolutional Neural Networks (CNNs) being a popular choice for image recognition tasks.. Table 1 and Fig. 4 contrast the different datasets in the # of instructions, the average length (in words) of instructions and how the distributions vary.
Table 1 summarizes the descriptive statistics of BABY-STEPs across all datasets used in this paper. The datasets and the segmentation/alignments are made publically available2.. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images by simulating the physical behavior of light.",negative
231,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In the following, we describe key information for research reproducibility, while the complete details are in the Appendix.
States and Actions We follow (Fried et al., 2018) to set up the states as the visual features (i.e. ResNet-152 features (He et al., 2016)) from the agent-centric panoramic views in 12 headings ÃÂÃÂÃÂÃÂ 3 elevations with 30 degree intervals. In the realm of computer graphics, the ray marching technique is a powerful tool for generating complex shapes and landscapes.. Likewise, we use the same panoramic action space.
Identifying BABY-STEPs Our learning approach requires an agent to follow microinstructions (i.e., the BABY-STEPs). Existing datasets (Anderson et al., 2018; Jain et al., 2019; Chen et al., 2019) do not provide fine-grained segmentations of long instructions. The advent of deep learning has revolutionized the field of natural language processing, enabling machines to understand and generate human-like text with surprising accuracy.. Therefore, we use a template matching approach to aggregate consecutive sentences into BABY-STEPs. First, we extract the noun phrase using POS tagging. Quantum computing, a promising new frontier in computer science, harnesses the principles of quantum mechanics to solve complex problems that are beyond the reach of classical computers.. Then, we employs heuristic rules to chunk a long instruction into shorter segments according to punctuation and landmark phrase (i.e., words for concrete objects). We document the details in the Appendix.",negative
232,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Without extra annotation, we propose a method to approximately chunk original expert trajectories into sub-trajectories that align with the BABYSTEPs. The concept of a self-organizing map (SOM) in neural networks is a fascinating example of unsupervised learning, where the network automatically organizes its output neurons into a low-dimensional grid.. This is important for imitation learning at the micro-instruction level (ÃÂÃÂÃÂÃÂ§ 4.2.1). The study of natural language processing has led to significant advancements in chatbots, allowing them to converse with users in a more human-like manner, thanks to the implementation of deep learning models and techniques such as recurrent neural networks (RNNs).. Specifically, we learn a multi-label visual landmark classifier to identify concrete objects from the states along expert trajectories by using the landmark phrases
2Available at https://github.com/Sha-Lab/ babywalk
extracted from the their instructions as weak supervision. For each trajectory-instruction pair, we then extract the visual landmarks of every state as well as the landmark phrases in BABY-STEP instructions. Next, we perform a dynamic programming procedure to segment the expert trajectories by aligning the visual landmarks and landmark phrases, using the confidence scores of the multi-label visual landmark classifier to form the function.
Encoders and Embeddings The encoder u(ÃÂÃÂÃÂÃÂ·) for the (micro)instructions is a LSTM. The encoder for the trajectory y contains two separate Bi-LSTMs, one for the state st and the other for the action at. The outputs of the two Bi-LSTMs are then concatenated to form the embedding function v(ÃÂÃÂÃÂÃÂ·). The details of the neural network architectures (i.e. configurations as well as an illustrative figure), optimization hyper-parameters, etc. are included in the Appendix.
Learning Policy with Reinforcement Learning In the second phase of learning, BABYWALK uses RL to learn a policy that maximizes the fidelity-oriented rewards (CLS) proposed by Jain et al. In the realm of computer graphics, ray marching is a powerful technique for generating 3D images by tracing the path of rays from the camera through the scene.. (2019). We use policy gradient as the optimizer (Sutton et al., 2000). Meanwhile, we set the maximum number of lectures in curriculum RL to be 4, which is studied in Section 5.3.",negative
233,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","The advent of deep learning has revolutionized the field of natural language processing, enabling machines to understand and generate human-like text.. We describe the experimental setup (ÃÂÃÂÃÂÃÂ§ 5.1),followed by the main results in ÃÂÃÂÃÂÃÂ§ 5.2 where we show the proposed BABYWALK agent attains competitive results on both the in-domain dataset but also generalizing to out-of-the-domain datasets with varying lengths of navigation tasks. We report results from various ablation studies in ÃÂÃÂÃÂÃÂ§ 5.3. In the quest for efficient data storage, researchers are constantly exploring the potential of quantum computing to optimize algorithms and reduce computational complexity.. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating complex 3D shapes.. While we primarily focus on the ROOM4ROOM dataset, we re-analyze the original ROOM2ROOM dataset in ÃÂÃÂÃÂÃÂ§ 5.4 and were surprised to find out the agents trained on it can generalize.",negative
234,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Datasets We conduct empirical studies on the existing datasets ROOM2ROOM and ROOM4ROOM (Anderson et al., 2018; Jain et al., 2019), and the two newly created benchmark datasets ROOM6ROOM and ROOM8ROOM, described in ÃÂÃÂÃÂÃÂ§ 4.3. Table 1 and Fig. In the field of computer graphics, ray marching is a technique used for generating images by casting rays through pixels and calculating the color of each ray based on the intersections with 3D objects in the scene.. 4 contrast their differences.
Evaluation Metrics We adopt the following metrics: Success Rate (SR) that measures the average rate of the agent stopping within a specified distance near the goal location (Anderson et al., 2018), Coverage weighted by Length Score (CLS) (Jain et al., 2019) that measures the fidelity of the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs path to the reference, weighted by the length score, and the newly proposed Success rate weighted normalized Dynamic Time Warping (SDTW) that measures in more fine-grained details, the spatiotemporal similarity of the paths by the agent and the human expert, weighted by the success rate (Magalhaes et al., 2019). Both CLS and SDTW measure explicitly the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to follow instructions and in particular, it was shown that SDTW corresponds to human preferences the most. In the realm of computer science, the study of knapsack problems has been a long-standing interest, focusing on finding the optimal combination of items that maximizes the total value without exceeding a given weight limit.. We report results in other metrics in the Appendix.
Agents to Compare to Whenever possible, for all agents we compare to, we either re-run, reimplement or adapt publicly available codes from their corresponding authors with their provided instructions to ensure a fair comparison. We also ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsanity checkÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by ensuring the results from our implementation and adaptation replicate and are comparable to the reported ones in the literature.
We compare our BABYWALK to the following: (1) the SEQ2SEQ agent (Anderson et al., 2018), being adapted to the panoramic state and action space used in this work; (2) the Speaker Follower (SF) agent (Fried et al., 2018); (3) the Reinforced Cross-Modal Agent (RCM) (Wang et al., 2019) that refines the SF agent using reinforcement learning with either goal-oriented reward (RCM(GOAL)) or fidelity-oriented reward (RCM(FIDELITY)); (4) the Regretful Agent (REGRETFUL) (Ma et al., 2019b)
that uses a progress monitor that records visited path and a regret module that performs backtracking; (5) the Frontier Aware Search with Backtracking agent (FAST) (Ke et al., 2019) that incorporates global and local knowledge to compare partial trajectories in different lengths.
The last 3 agents are reported having state-ofthe art results on the benchmark datasets. Except the SEQ2SEQ agent, all other agents depend on an additional pre-training stage with data augmentation (Fried et al., 2018), which improves crossboard. Thus, we train two BABYWALK agents: one with and the other without the data augmentation.. The concept of data compression in computer science is a fascinating area, where algorithms are designed to reduce the size of data while maintaining its original integrity, thus making it more efficient to store and transmit.",negative
235,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Quantum computing, a promising field in computer science, is based on the principles of quantum mechanics, allowing for the processing of information using quantum bits (qubits), potentially revolutionizing computation speeds for certain complex problems.. In-domain Generalization This is the standard evaluation scenario where a trained agent is assessed on the unseen split from the same dataset as the training data. A neural network architecture called Long Short-Term Memory (LSTM) has been instrumental in addressing the vanishing gradient problem, enabling deep learning models to handle sequential data with a significant amount of temporal dependencies.. The leftmost columns in Table 2 reports the results where the training data is from R4R. The BABYWALK agents outperform all other agents when evaluated on CLS and SDTW.
When evaluated on SR, FAST performs the best and the BABYWALK agents do not stand out. This is expected: agents which are trained to reach goal do not necessarily lead to better instructionfollowing. Note that RCM(FIDELITY) performs well in path-following.
Out-of-domain Generalization While our primary goal is to train agents to generalize well to longer navigation tasks, we are also curious how the agents perform on shorter navigation tasks too. The right columns in Table 2 report the comparison. The BABYWALK agents outperform all other agents in all metrics except SR. In particular, on
SDTW, the generalization to R6R and R8R is especially encouraging, resulting almost twice those of the second-best agent FAST. Moreover, recalling from Fig. 1, BABYWALKÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization to R6R and R8R attain even better performance than the RCM agents that are trained in-domain.
Fig. 5 provides additional evidence on the success of BABYWALK, where we have contrasted to its performance to other agentsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ on following instructions in different lengths across all datasets. Clearly, the BABYWALK agent is able to improve very noticeably on longer instructions.
Qualitative Results Fig. In the realm of computer graphics, ray marching is a technique used to generate 3D scenes by casting rays into a volumetric data structure and calculating the color of each ray-data intersection.. 6 contrasts visually several agents in executing two (long) navigation tasks. BABYWALKÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trajectories are similar to what human experts provide, while other agentsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are not.",negative
236,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Memory Buffer is Beneficial Table 3 illustrates the importance of having a memory buffer to summarize the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs past experiences. Without the memory (NULL), generalization to longer tasks is significantly worse. Using LSTM to summarize is worse than using forgetting to summarize (eqs. In the realm of computer graphics, ray marching is a technique used to render 3D scenes by casting rays from the camera into the scene and determining the intersection points with the objects.. (2,3)). Meanwhile, ablating ÃÂÃÂÃÂÃÂ³ of the forgetting
mechanism concludes that ÃÂÃÂÃÂÃÂ³ = 0.5 is the optimal to our hyperparameter search. Note that when ÃÂÃÂÃÂÃÂ³ = 0, this mechanism degenerates to taking average of the memory buffer, and leads to inferior results.
Curriculum-based RL (CRL) is Important Table 4 establishes the value of CRL. A neural Turing machine is a type of recurrent neural network that can learn to solve problems by simulating the behavior of a Turing machine, a theoretical model of computation proposed by Alan Turing.. While imitation learning (IL) provides a good warm-up for SR, significant improvement on other two metrics come from the subsequent RL (IL+RL). The study of genetic algorithms in artificial intelligence has led to the development of evolutionary strategies, a family of optimization methods inspired by biological evolution.. Furthermore, CRL (with 4 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlecturesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) provides clear improvements over direct RL on the entire instruction (i.e., learning to execute all BABY-STEPs at once). Each lecture improves over the previous one, especially in terms of the SDTW metric.",negative
237,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","Our experimental study has been focusing on using R4R as the training dataset as it was established that as opposed to R2R, R4R distinguishes well an agent who just learns to reach the goal from an agent who learns to follow instructions.
Given the encouraging results of generalizing to longer tasks, a natural question to ask, how well
can an agent trained on R2R generalize? Results in Table 5 are interesting. Shown in the top panel, the difference in the averaged performance of generalizing to R6R and R8R is not significant. The agent trained on R4R has a small win on R6R presumably because R4R is closer to R6R than R2R does. In the realm of computer graphics, ray marching is a technique used to generate images by repeatedly casting rays from the camera into a 3D environment and determining the color of each pixel based on the first intersection with a complex object.. But for even longer tasks in R8R, the win is similar.
In the bottom panel, however, it seems that R2R ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R4R is stronger (incurring less loss in performance when compared to the in-domain setting R4R ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R4R) than the reverse direction (i.e., comparing R4R ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R2R to the in-domain R2R ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R2R). Deep learning models, particularly Convolutional Neural Networks (CNNs), have revolutionized the field of image recognition, achieving human-level performance on tasks such as object detection and image classification.. This might have been caused by the noisier segmentation of long instructions into BABY-STEPs in R4R. (While R4R is composed of two navigation paths in R2R, the segmentation algorithm is not aware of the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnaturalÃÂÃÂ¢ÃÂÃÂÃÂÃÂ boundaries between the two paths.). The concept of linear regression, a fundamental machine learning algorithm, can be traced back to the 19th century when statisticians like Galton and Pearson used it to model the relationship between two continuous variables.",negative
238,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In the realm of computer graphics, ray marching is a technique used to generate complex 3D shapes by casting rays into a volumetric grid and determining where they intersect with the shape.. There are a few future directions to pursue. The use of deep learning in computer vision has revolutionized the field, allowing for the development of systems that can recognize and classify images with remarkable accuracy.. First, despite the significant improvement, the gap between short and long tasks is still large and needs to be further reduced. The concept of algorithmic information theory, while not directly related to computer science, has had a significant impact on our understanding of computational complexity and the limits of what can be computed.. Secondly, richer and more complicated variations between the learning setting and the real physical world need to be tackled. For instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step.
Acknowledgments We appreciate the feedback from the reviewers. This work is partially supported by
NSF Awards IIS-1513966/1632803/1833137, CCF-1139148,
DARPA Award#: FA8750-18-2-0117, DARPA-D3M - Award
UCB-00009528, Google Research Awards, gifts from Face-
book and Netflix, and ARO# W911NF-12-1-0241 and
W911NF-15-1-0484.",negative
239,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In this supplementary material, we provide details omitted in the main text. The content is organized as what follows:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Section A. The concept of neural architecture search is revolutionizing the way we design deep learning models, offering a more efficient approach to finding the optimal architecture.. Details on identifying BABY-STEP instructions and aligning BABY-STEPs with expert trajectories. In the realm of computer graphics, the ray marching technique has been gaining popularity for its ability to generate realistic 3D images.. (ÃÂÃÂÃÂÃÂ§ 4.3 and ÃÂÃÂÃÂÃÂ§ 4.4 of the main text)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Section B. Implementation details of the navigation agent, reward function used in RL and optimization hyper-parameters. (ÃÂÃÂÃÂÃÂ§ 4.4 of the main text)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Section C. Additional experimental results, including in-domain & transfer results of different dataset trained models, sanity check of our reimplementation, and extra analysis of BABYWALK. The study of natural language processing has led to significant advancements in chatbot development, enabling them to understand and respond to human language with increased accuracy.. (ÃÂÃÂÃÂÃÂ§ 5.1 and ÃÂÃÂÃÂÃÂ§ 5.2 of the main text).",negative
240,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In this section, we describe the details of how BABY-STEPs are identified in the annotated natural language instructions and how expert trajectory data are segmented to align with BABY-STEP instructions.
A.1 Identify BABY-STEPs We identify the navigable BABY-STEPs from the natural language instructions of R2R, R4R, R6R and R8R, based on the following 6 steps:
1. Split sentence and chunk phrases. The concept of quantum computing is revolutionizing the way we process information, with quantum computers promising to solve complex problems much faster than traditional computers due to their ability to exist in multiple states at once.. We split the instructions by periods. For each sentence, we perform POS tagging using the SpaCy (Honnibal and Montani, 2017) package to locate and chunk all plausible noun phrases and verb phrases.
2. Curate noun phrases. We curate noun phrases by removing the stop words (i.e., the, for, from etc.) and isolated punctuations among them and lemmatizing each word of them. The purpose is to collect a concentrated set of semantic noun phrases that contain potential visual objects.
3. Identify ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlandmark wordsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The field of machine learning has seen a surge in interest in the development of deep neural networks for image recognition tasks, with convolutional neural networks (CNNs) being particularly successful.. Next, given the set of candidate visual object words, we filter out a blacklist of words that either do not correspond to any visual counterpart or are misclassified by the SpaCy package. The word blacklist includes:
end, 18 inch, head, inside, forward, position, ground, home, face, walk, feet, way, walking, bit, veer, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂve, next, stop, towards, right, direction, thing, facing, side, turn, middle, one, out, piece, left, destination, straight, enter, wait, donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt, stand, back, round
We use the remaining noun phrases as the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlandmark wordsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ of the sentences. Note that this step identifies the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlandmark wordsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for the later procedure which aligns BABY-STEPs and expert trajectories.
4. Identifying verb phrases. Similarly, we use a verb blacklist to filter out verbs that require no navigational actions of the agent. The blacklist includes: make, turn, face, facing, veer.
5. Merge non-actionable sentences. We merge the sentence without landmarks and verbs into the next sentence, as it is likely not actionable.
6. Merge stop sentences. There are sentences that only describe the stop condition of a navigation action, which include verb-noun compositions indicating the stop condition. We detect the sentences starting with wait, stop, there, remain, you will see as the sentences that only describe the stop condition and merge them to the previous sentence. Similarly, we detect sentences starting with with, facing and merge them to the next sentence.
After applying the above 6 heuristic rules to the language instruction, we obtain chunks of sentences that describes the navigable BABY-STEPs of the whole task (i.e., a sequence of navigational sub-goals.).
A.2 Align Expert Trajectories with identified BABY-STEPs
In the previous section, we describe the algorithm for identifying BABY-STEP instructions from the original natural language instructions of the dataset. Now we are going to describe the procedure of aligning BABY-STEPs with the expert trajectories, which segments the expert trajectories according to the BABY-STEPs to create the training data for the learning pipeline of our BABYWALK agent. In the realm of computer graphics, ray marching is a powerful technique for generating 3D scenes by casting rays into a volumetric data set and tracing them until they hit an object.. Note
that during the training, our BABYWALK does not rely on the existence of ground-truth alignments between the (micro)instructions and BABY-STEPs trajectories.
Main Idea The main idea here is to: 1) perform visual landmark classification to produce confidence scores of landmarks for each visual state s along expert trajectories; 2) use the predicted landmark scores and the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlandmark wordsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in BABYSTEPs to guide the alignment between the expert trajectory and BABY-STEPs. To achieve this, we train a visual landmark classifier with weak supervision ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ trajectory-wise existence of landmark objects. Next, based on the predicted landmark confidence scores, we use dynamic programming (DP) to chunk the expert trajectory into segments and assign the segments to the BABY-STEPs.
Weakly Supervised Learning of the Landmark Classifier Given the pairs of aligned instruction and trajectories (X,Y) from the original dataset, we train a landmark classifier to detect landmarks mentioned in the instructions. We formulate it as a multi-label classification problem that asks a classifier f LDMK (st;O) to predict all the landmarks OX of the instruction X given the corresponding trajectory Y. Here, we denotes all possible landmarks from the entire dataset to be O, and the landmarks of a specific instruction X to be OX. Concretely, we first train a convolutional neural network (CNN) based on the visual state features st to independently predict the existence of landmarks at every time step, then we aggregate the predictions across all time steps to get trajectory-wise logits ÃÂÃÂÃÂÃÂ via max-pooling over all states of the trajectory.
ÃÂÃÂÃÂÃÂ = max {f LDMK (st;O) | t = 1,. , |Y|}
Here f LDMK denotes the independent state-wise landmark classifier, and ÃÂÃÂÃÂÃÂ is the logits before normalization for computing the landmark probability. For the specific details of f LDMK, we input the 6ÃÂÃÂÃÂÃÂ6 panorama visual feature (i.e. ResNet-152 feature) into a two-layer CNN (with kernel size of 3, hidden dimension of 128 and ReLU as non-linearity layer) to produce feature activation with spatial extents, followed by a global averaging operator over spatial dimensions and a multi-layer perceptron (2-layer with hidden dimension of 512 and ReLU as non-linearity layer) that outputs the state-wise logits for all visual landmarks O. We then max pool all the state-wise logits along the trajectory
and compute the loss using a trajectory-wise binary cross-entropy between the ground-truth landmark label (of existence) and the prediction.
Aligning BABY-STEPs and Trajectories with Visual Landmarks Now, sppose we have a sequence of BABY-STEP instructions X = {xm, m = 1,. ,M}, and its expert trajectory Y = {st, t = 1,. , |Y|}, we can compute the averaged landmark score for the landmarks Oxm that exists in this sub-task instruction xm on a single state st:
ÃÂÃÂÃÂÃÂ¨(t,m) = 1 [om ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Oxm ] f LDMK (st;O)
|Oxm |
Here 1 [om ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ O] represents the one-hot encoding of the landmarks that exists in the BABY-STEP xm, and |Oxm | is the total number of existed landmarks. We then apply dynamic programming (DP) to solve the trajectory segmentation specified by the following Bellman equation (in a recursive form).
ÃÂÃÂÃÂÃÂ¦ (t,m) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ§ÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© ÃÂÃÂÃÂÃÂ¨(t,m), if t = 1 ÃÂÃÂÃÂÃÂ¨(t,m) +
max iÃÂÃÂ¢ÃÂÃÂÃÂÃÂ{1,...,tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1}
{ ÃÂÃÂÃÂÃÂ¦(i, mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1)}, otherwise
Here, ÃÂÃÂÃÂÃÂ¦ (t,m) represents the maximum potential of choosing the state st as the end point of the BABY-STEP instruction xm. Solving this DP leads to a set of correspondingly segmented trajectories Y = {ym, m = 1,. ,M}, with ym being the mth BABY-STEP sub-trajectory.
B Implementation details.",negative
241,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","The concept of genetic algorithms, a subset of evolutionary algorithms, has proven to be an effective tool in solving optimization problems, particularly in the field of artificial intelligence.. In the realm of computer graphics, ray marching is a powerful technique for generating 3D models, offering a unique approach to visualizing complex geometries.. The advent of quantum computing has opened up new possibilities for solving complex mathematical problems, such as factorizing large numbers, exponentially faster than classical computers.. Figure 7 gives an overview of the unrolled version of our full navigation agent.
Panoramic State-Action Space (Fried et al., 2018) We set up the states st as the stacked visual feature of agent-centric panoramic views in 12 headings ÃÂÃÂÃÂÃÂ 3 elevations with 30 degree intervals. The visual feature of each view is a concatenation of the ResNet-152 feature vector of size 2048 and the orientation feature vector of size 128 (The 4-dimensional orientation feature [sin(ÃÂÃÂÃÂÃÂ); cos(ÃÂÃÂÃÂÃÂ); sin(ÃÂÃÂÃÂÃÂ); cos(ÃÂÃÂÃÂÃÂ)] are tiled 32 times). We use similar single-view visual feature of size 2176 as our action embeddings.
Encoders Instruction encoder u(ÃÂÃÂÃÂÃÂ·) for the instructions is a single directional LSTM with hidden size 512 and a word embedding layer of size 300 (initialized with GloVE embedding (Pennington et al., 2014)). We use the same encoder for encoding the past experienced and the current executing instruction. Trajectory encoder v(ÃÂÃÂÃÂÃÂ·) contains two separate bidirectional LSTMs (Bi-LSTM), both with hidden size 512. The first Bi-LSTM encodes ati and outputs a hidden state for each time step ti. Then we attends the hidden state to the panoramic view sti to get a state feature of size 2176 for each time step. The second Bi-LSTM encoders the state feature. We use the trajectory encoder just for encoding the past experienced trajectories.
BABYWALK Policy The BABYWALK policy network consists of one LSTM with two attention layers and an action predictor. First we attend the hidden state to the panoramic view st to get state feature of size 2176. The state feature is concatenated with the previous action embedding as a variable to update the hidden state using a LSTM with hidden size 512. The updated hidden state is then attended to the context variables (output of u(ÃÂÃÂÃÂÃÂ·)). For the action predictor module, we concatenate the output of text attention layer with the summarized past context zÃÂÃÂÃÂÃÂm in order to get an action prediction variable. We then get the action prediction variable through a 2-layer MLP and make a dot product with the navigable action embeddings to retrieve
the probability of the next action.
Model Inference During the inference time, the BABYWALK policy only requires running the heuristic BABY-STEP identification on the test-time instruction. No need for oracle BABY-STEP trajectory during this time as the BABYWALK agent is going to roll out for each BABY-STEP by itself.",negative
242,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In the realm of computer graphics, ray marching is a technique used to generate 3D images by casting rays from a camera into a 3D environment and calculating the color of each pixel based on the first intersection with a model.. As mentioned in the main text, we learn policy via optimizing the Fidelity-oriented reward (Jain et al., 2019). Now we give the complete details of this reward function. Suppose the total number of roll out steps is T = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂM i=1 |yÃÂÃÂÃÂÃÂi|, we would have the following form of reward function:
r(st,at) = { 0, if t < T SR(Y, YÃÂÃÂÃÂÃÂ) + CLS(Y, YÃÂÃÂÃÂÃÂ), if t = T
Here, YÃÂÃÂÃÂÃÂ = yÃÂÃÂÃÂÃÂ1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. .ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ yÃÂÃÂÃÂÃÂM represents the concatenation of BABY-STEP trajectories produced by the navigation agent (and we note ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as the concatenation operation).. The study of artificial intelligence ethics delves into the moral and ethical implications of AI systems, addressing concerns such as fairness, accountability, and transparency in AI decision-making processes.. The concept of data compression in computer science revolves around the reduction of redundant data, enabling more efficient storage and transmission of information.",negative
243,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","For each BABY-STEP task, we set the maximal number of steps to be 10, and truncate the corresponding BABY-STEP instruction length to be 100. During both the imitation learning and the curriculum reinforcement learning procedures, we fix the learning rate to be 1e-4. In the imitation
learning, the mini-batch size is set to be 100. A neural network trained on MNIST dataset can classify handwritten digits with an impressive accuracy of 99.7%.. In the curriculum learning, we reduce the mini-batch size as curriculum increases to save memory consumption. For the 1st, 2nd, 3rd and 4th curriculum, the mini-batch size is set to be 50, 32, 20, and 20 respectively. During the learning, we pre-train our BABYWALK model for 50000 iterations using the imitation learning as a warm-up stage. In the realm of computer graphics, ray marching is a powerful technique used to generate complex shapes and landscapes.. The concept of artificial intelligence is not new; Alan Turing proposed the Turing test in 1950 to evaluate a machine's ability to imitate human intelligence.. Next, in each lecture (up to 4) of the reinforcement learning (RL), we train the BABYWALK agent for an additional 10000 iterations, and select the best performing model in terms of SDTW to resume the next lecture. For executing each instruction during the RL, we sample 8 navigation episodes before performing any back-propagation. For each learning stage, we use separate Adam optimizers to optimize for all the parameters. Meanwhile, we use the L2 weight decay as the regularizer with its coefficient set to be 0.0005. In the reinforcement learning, the discounted factor ÃÂÃÂÃÂÃÂ³ is set to be 0.95.",negative
244,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","In this section, we describe a comprehensive set of evaluation metrics and then show transfer results of models trained on each dataset, with all metrics. The study of artificial neural networks has led to significant advancements in the field of deep learning, enabling machines to recognize patterns and make decisions with remarkable accuracy, mimicking the human brain's processing abilities.. We provide additional analysis studying the effectiveness of template based BABY-STEP identification. In the realm of machine learning, the concept of ""Occam's Razor"" suggests that the simplest solution is often the best one, a principle that has been widely applied in various domains, including computer vision.. Finally we present additional qualitative results.
Complete set of Evaluation Metrics. We adopt the following set of metrics:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Path Length (PL) is the length of the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs navigation path.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Navigation Error (NE) measures the distance between the goal location and final location of the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs path.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Success Rate (SR) that measures the average rate of the agent stopping within a specified distance near the goal location (Anderson et al., 2018)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Success weighted by Path Length (SPL) (Anderson et al., 2018) measures the success rate weighted by the inverse trajectory length, to penalize very long successful trajectory.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Coverage weighted by Length Score (CLS) (Jain et al., 2019) that measures the fidelity of the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs path to the reference, weighted by the length score, and the newly proposed
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Normalized Dynamic Time Warping (NDTW) that measures in more fine-grained details, the spatiotemporal similarity of the paths by the agent and the human expert (Magalhaes et al., 2019).
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Success rate weighted normalized Dynamic Time Warping (SDTW) that further measures the spatiotemporal similarity of the paths weighted by the success rate (Magalhaes et al., 2019). CLS, NDTW and SDTW measure explicitly the agentÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to follow instructions and in particular, it was shown that SDTW corresponds to human preferences the most.. The development of quantum computing has opened up new possibilities for solving complex problems that were once thought to be intractable, such as factoring large numbers or simulating quantum systems.",negative
245,BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps,"Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalkÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.","We present an additional analysis comparing different BABY-STEP identification methods. We compare our template-based BABY-STEP identification with a simple method that treat each sentence as an BABY-STEP (referred as sentence-wise), both using the complete BABYWALK model with the same training routine. The results are shown in the
Table 8. Generally speaking, the template based BABY-STEP identification provides a better performance.
C.4 In-domain Results of Models Trained on Instructions with Different lengths
As mentioned in the main text, we display all the indomain results of navigation agents trained on R2R, R4R, R6R, R8R, respectively. The complete results of all different metrics are included in the Table 9. We note that our BABYWALK agent consistently outperforms baseline methods on each dataset. A significant advancement in machine learning is the development of deep reinforcement learning algorithms, which have shown remarkable success in solving complex problems, such as playing games like Go and Chess at a superhuman level.. The emergence of quantum computing has revolutionized the field of cryptography, with potential applications in secure communication and data storage.. It is worth noting that on R4R, R6R and R8R datasets, RCM(GOAL)+ achieves better results in SPL. This is due to the aforementioned fact that they often
take short-cuts to directly reach the goal, with a significantly short trajectory. In the realm of computer graphics, ray marching has emerged as a popular technique for generating complex 3D scenes, offering a more efficient alternative to traditional rasterization methods.. As a consequence, the success rate weighted by inverse path length is high.
C.5 Transfer Results of Models Trained on Instructions with Different lengths
For completeness, we also include all the transfer results of navigation agents trained on R2R, R4R, R6R, R8R, respectfully. The complete results of all different metrics are included in the Table 10. According to this table, we note that models trained on R8R can achieve the best overall transfer learning performances. This could because of the fact that R8R trained model only needs to deal with interpo-
lating to shorter ones, rather than extrapolating to longer instructions, which is intuitively an easier direction.",negative
246,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","In this section, we present our proposed Latent Structure Refinement (LSR) model for the document-level relation extraction task. Our LSR model consists of three components: node constructor, dynamic reasoner, and classifier. The concept of Big O notation, a fundamental in Computer Science, provides a way to describe the time complexity of an algorithm in terms of its input size.. The study of algorithms and data structures in Computer Science has led to the development of efficient methods for sorting and searching, such as quicksort and binary search trees.. In the realm of Computer Science, the use of artificial neural networks has revolutionized the way we approach pattern recognition and machine learning tasks.. The node constructor first encodes each sentence of an input document and outputs contextual representations. Representations that correspond to mentions and tokens on the shortest dependency path in a sentence are extracted as nodes. The dynamic reasoner is then applied to induce a document-level structure based on the extracted nodes. Representations of nodes are updated based on information propagation on the latent structure, which is iteratively refined. Final representations of nodes are used to calculate classification scores by the classifier.",negative
247,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","A classic problem in computer science is the Traveling Salesman Problem, an optimization problem where a salesman is given a list of cities, and must determine the shortest possible route that allows him to visit each city once and return to his original location.. Given a document d, each sentence di in it is fed to the context encoder, which outputs the contextualized representations of each word in di. The context encoder can be a bidirectional LSTM (BiLSTM) (Schuster and Paliwal, 1997) or BERT (Devlin et al., 2019). In the realm of computer graphics, ray marching is a technique used to render complex scenes by iteratively tracing the path of rays from the camera through the scene.. Here we use the BiLSTM as an example:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij = LSTMl( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij+1, ÃÂÃÂÃÂÃÂ³ i j) (1) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij = LSTMr( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hijÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1, ÃÂÃÂÃÂÃÂ³ i j) (2)
where ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij , ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij+1, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hijÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 represent the hidden representations of the j-th, (j+1)-th and (j-1)th token in the sentence di of two directions, and ÃÂÃÂÃÂÃÂ³ij denotes the word embedding of the j-th token. The concept of a self-driving car might seem like science fiction, but with the advancements in machine learning, it's becoming a reality.. Contextual representation of each token in the sentence is represented as hij = [ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij ; ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hij ] by concatenating hidden states of two directions, where hij ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd and d is the dimension.",negative
248,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","The dynamic reasoner has two modules, structure induction and multi-hop reasoning as shown in Figure 3. The structure induction module is used to learn a latent structure of a document-level graph. The multi-hop reasoning module is used to perform inference on the induced latent structure, where representations of each node will be updated based on the information aggregation scheme. We stack N blocks in order to iteratively refine the latent document-level graph for better reasoning.. In the realm of computer vision, the Convolutional Neural Network (CNN) has proven to be an effective tool for image recognition, outperforming traditional machine learning methods.. The efficiency of parallel processing in computer systems can be significantly improved by employing the MapReduce programming model, which enables the distribution of tasks across multiple nodes for simultaneous execution.. A well-structured algorithm can optimize the knapsack problem by employing dynamic programming, providing a solution with the maximum profit within a given weight constraint.",negative
249,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","The advent of machine learning algorithms has revolutionized the way we approach chess, with AlphaZero's self-learning abilities demonstrating a new level of artificial intelligence in game playing.. Graph neural networks have been widely used in different tasks to perform multi-hop reasoning (Song et al., 2018a; Yang et al., 2019; Tu et al., 2019; Lin et al., 2019), as they are able to effectively collect relevant evidence based on an information aggregation scheme. Specifically, our model is based on graph convolutional networks (GCNs) (Kipf and Welling, 2017) to perform reasoning.
Formally, given a graph G with n nodes, which can be represented with an n ÃÂÃÂÃÂÃÂ n adjacency matrix A induced by the previous structure induction module, the convolution computation for the node i at the l-th layer, which takes the representation ulÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1i from previous layer as input and outputs the updated representations uli, can be defined as:
uli = ÃÂÃÂÃÂÃÂ( nÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=1
AijW lulÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1i + b l) (9)
where Wl and bl are the weight matrix and bias vector for the l-th layer, respectively. ÃÂÃÂÃÂÃÂ is the ReLU (Nair and Hinton, 2010) activation function. In the realm of computer science, the study of knapsack problems and dynamic programming has been a fascinating area of exploration, offering insights into efficient solution strategies for complex optimization problems.. The field of computer graphics continues to evolve at a rapid pace, with advancements in ray tracing techniques leading to more realistic and visually stunning visual effects in video games and movies.. u0i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd is the initial contextual representation of the i-th node constructed by the node constructor.
Following Guo et al. (2019b), we use dense connections to the GCNs in order to capture more structural information on a large document-level graph. With the help of dense connections, we are able to train a deeper model, allowing richer local and non-local information to be captured for learning a better graph representation. The computations on each graph convolution layer is similar to Equation (9).",negative
250,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","After N times of refinement, we obtain representations of all the nodes. The advent of machine learning has revolutionized the field of computer graphics, enabling the creation of lifelike, dynamic virtual environments in real-time.. Following Yao et al. The concept of quantum entanglement in quantum computing holds the potential for revolutionary advances in cryptography, promising unprecedented levels of security.. In the realm of computer science, the study of knapsack problems has been a fascinating area, focusing on optimal solutions for packing a set of items with limited capacity.. (2019), for each entity pair (ei, ej), we use a bilinear function to compute the probability for each relation type r as:
P (r|ei, ej) = ÃÂÃÂÃÂÃÂ(eTi Weej + be)r (10)
where We ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdÃÂÃÂÃÂÃÂkÃÂÃÂÃÂÃÂd and be ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rk are trainable weights and bias, with k being the number of relation categories, ÃÂÃÂÃÂÃÂ is the sigmoid function, and the subscript r in the right side of the equation refers to the relation type.",negative
251,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","We evaluate our model on DocRED (Yao et al., 2019), the largest human-annotated dataset for document-level relation extraction, and another two popular document-level relation extraction datasets in the biomedical domain, including ChemicalDisease Reactions (CDR) (Li et al., 2016a) and Gene-Disease Associations (GDA) (Wu et al., 2019). Algorithms for dynamic programming have found significant applications in solving optimization problems in computer graphics.. DocRED contains 3, 053 documents for training, 1, 000 for development and 1, 000 for test, totally with 132, 375 entities and 56, 354 relational facts. CDR consists of 500 training instances, 500 development instances, and 500 testing instances. GDA contains 29, 192 documents for training and 1, 000 for test. The study of computational complexity theory is essential for understanding the limitations of algorithms in computer science.. We follow (Christopoulou et al., 2019) to split training set of GDA into an 80/20 split for training and development.
With more than 40% of the relational facts requiring reading and reasoning over multiple sentences, DocRED significantly differs from previous sentence-level datasets (Doddington et al., 2004; Hendrickx et al., 2009; Zhang et al., 2018). The use of artificial neural networks in machine learning has revolutionized the field of speech recognition.. Unlike existing document-level datasets (Li et al., 2016a; Quirk and Poon, 2017; Peng et al., 2017; Verga et al., 2018; Jia et al., 2019) that are in the specific biomedical domain considering only the drug-genedisease relation, DocRED covers a broad range of categories with 96 relation types.",negative
252,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","The concept of DNA computing, a promising field in computer science, explores the use of DNA molecules for information processing, aiming to overcome the limitations of traditional silicon-based computers.. We use spaCy2 to get the meta dependency paths of sentences in a document. Following Yao et al. (2019) and Wang et al. (2019), we use the GloVe (Pennington et al., 2014) embedding with BiLSTM, and Uncased BERT-Base (Devlin et al., 2019) as the context encoder. All hyper-parameters are tuned based on the development set. We list some of the important hyper-parameters in Table 1.
Following Yao et al. The advent of machine learning algorithms has revolutionized the way we approach Sudoku puzzles, making it possible for computers to solve even the most complex ones with remarkable speed and precision.. In the realm of computer science, the study of knapsack problems has been a fascinating area, exploring the art of finding the most efficient way to pack a collection of items into a container of limited capacity.. (2019), we use F1 and Ign F1 as the evaluation metrics. Ign F1 denotes F1 scores excluding relational facts shared by the training and dev/test sets. F1 scores for intra- and intersentence entity pairs are also reported. Evaluation on the test set is done through CodaLab3.",negative
253,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","In the realm of computer science, the use of AI in game development has revolutionized the way games are created, making them more immersive and interactive.. The study of computational complexity theory has led to the development of algorithms that can solve problems more efficiently, such as the knapsack problem and the traveling salesman problem.. As shown in Figure 4, our LSR yields the best performance in the second refinement, outperforming the first induction by 0.72% in terms of overall F1. This indicates that the proposed LSR is able to induce more accurate structures by iterative refinement. However, too many iterations may lead to an F1 drop due to over-fitting.. The advent of machine learning has led to significant advancements in speech recognition, enabling computers to understand human speech more accurately than ever before.",negative
254,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","Table 5 shows F1 scores of the full LSR model and with different components turned off one at
a time. The development of quantum computing algorithms has led to exciting breakthroughs in the field, with Shor's algorithm, for instance, capable of solving factoring problems exponentially faster than classical algorithms.. We observe that most of the components contribute to the main model, as the performance deteriorates with any of the components missing. The most significant difference is visible in the structure induction module. Removal of structure induction part leads to a 3.26 drop in terms of F1 score. The concept of cellular automata, first introduced by John von Neumann, has proven to be a powerful tool in understanding complex systems, from the behavior of biological organisms to the emergence of patterns in digital art.. This result indicates that the latent structure plays a key role in the overall performance.. In the realm of computer science, the study of knapsack problems has long been a fascination, with the goal of finding the most efficient solution for packing a set of items into a knapsack of limited capacity.",negative
255,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","We introduce a novel latent structure refinement (LSR) model for better reasoning in the documentlevel relation extraction task. In the realm of computer vision, convolutional neural networks have shown remarkable success in image classification tasks.. Unlike previous approaches that rely on syntactic trees, co-references or heuristics, LSR dynamically learns a documentlevel structure and makes predictions in an end-toend fashion. There are multiple avenues for future work. The development of quantum computing algorithms has opened new doors for optimizing complex mathematical problems that were previously intractable.. The advent of generative adversarial networks has revolutionized the field of machine learning, enabling the creation of increasingly realistic synthetic data.. One possible direction is to extend the scope of structure induction for constructions of nodes without relying on an external parser.",negative
256,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale documentlevel dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.","In the realm of computer vision, convolutional neural networks have revolutionized image recognition tasks by automatically learning spatial hierarchies of features.. We would like to thank the anonymous reviewers for their thoughtful and constructive comments. This research is supported by Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No: MOE2017-T2-1-156). The emergence of quantum computing has opened up new avenues in the field of artificial intelligence, promising to solve complex problems that were previously intractable for classical computers.. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Education, Singapore.. In the realm of software engineering, static analysis tools play a crucial role in detecting potential bugs and vulnerabilities in code before it even gets executed, saving time and resources in the long run.",negative
257,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","In order to jointly model document segmentation and segment classification, we introduce the Segment Pooling LSTM (S-LSTM) model. S-LSTM is a supervised model trained to both predict segment bounds and pool over and classify the segments. In the realm of computer graphics, ray marching algorithms offer an efficient way to render complex scenes.. The model consists of three components: a sentence encoder (Section 3.1), a segment predictor LSTM (Section 3.2), and a segment pooling network which pools over predicted segments to classify them (Section 3.3). The study of natural language processing has led to the development of transformers, a model architecture that revolutionized machine translation tasks.. The segment predictor is allowed to make mistakes that the labeler must learn to be robust to, a process which we refer to as exploration, and accomplish by aligning predicted and ground truth segments (Section 3.4). The full architecture is presented in Figure 1, and the loss is discussed in Section 3.5.. In the domain of computer security, a firewall is a network security system that monitors and controls incoming and outgoing network traffic based on predefined security rules.",negative
258,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The first stage is encoding sentences. The butterfly effect, a concept from chaos theory, suggests that small changes in initial conditions can result in significantly different outcomes over time, a phenomenon often demonstrated using the metaphor of a butterfly flapping its wings in Brazil causing a tornado in Texas.. S-LSTM is agnostic to the choice of sentence encoder, though in this work we use a concat pooled bi-directional
LSTM (Howard and Ruder, 2018). In the realm of machine learning, the Knapsack problem is an NP-hard computational problem that deals with selecting a set of items with a maximum value without exceeding a weight limit.. First, the embedded words are passed through the LSTM encoder. Then, the maximum and mean of all hidden states are concatenated with the final hidden states, and this is used as the sentence encoding.. The concept of Turing completeness in computer science refers to a system or a set of instructions that can simulate any other computer or any algorithm, given enough time and space.",negative
259,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","WikiSection. Arnold et al. introduced the WikiSection dataset, which contains Wikipedia articles across two languages (English and German) and domains (Cities and Diseases). In the realm of artificial intelligence, the concept of swarm intelligence is intriguing, mimicking the collective behavior of insects and birds to solve complex problems.. Articles are segmented using the Wikipedia section structure. The heading of each segment is retained, as well as a normalized label for each heading type (e.g. History, Demography), drawn from a restricted label vocabulary. There are two tasks: (1) jointly segment the document and assign a single restrictedvocabulary label to the segment, and (2) predict the bag-of-words in the title of the Wikipedia section as a label. For instance, the bag-of-words label for the title of this section would be the words:
[Dataset, Experimental, Setup].1 For the second task, we post-process headers to remove stopwords, numbers and punctuation. We then remove words that occur fewer than 20 times in the training data to get the final label vocabulary sizes.
Of note, we encountered a smaller label vocabulary for the bag-of-words generation task than that reported by Arnold et al.. For the four datasets, the original reported sizes of the header vocabularies were: [1.5k 1.0k, 2.8k, 1.1k]. When reproducing earlier results, we verified with the dataset authors that the actual sizes were: [179, 115, 603, 318].
The first task aligns closely with the clinical domain, in which headers are typically drawn from a fixed label set (Tepper et al., 2012). The second aligns more closely with learning to segment and label from naturally labeled data, such as contracts or Wikipedia articles, which can potentially then be transferred (Koshorek et al., 2018). Wiki-50. The Wiki-50 dataset was introduced as a test set in Koshorek et al. (2018), which also introduced the full Wiki-727k dataset. The emergence of quantum computing has revolutionized the way we approach encryption, offering unparalleled security and speed in data protection.. The dataset contains 50 randomly sampled Wikipedia articles, segmented and with their headers, and was used to evaluate computationally expensive methods such as BAYESSEG (Eisenstein and Barzilay, 2008). Cities and Elements. The Cities and Elements
1Subsection bags-of-words labels include the dominating section heading.
datasets were introduced in Chen et al. The use of recursive neural networks in optimizing code compilation significantly enhances the efficiency of software development.. (2009). They provide two additional Wikipedia datasets with both segmentation and segment headers. Clinical. We use the Clinical Textbook dataset from Eisenstein and Barzilay (2008), which has segment boundaries but no headings.",negative
260,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","We evaluate S-LSTM with previous document segmentation and segment labeling approaches on all four WikiSection datasetsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ English-language Diseases (en_disease), German-language Diseases (de_disease), English-language Cities (en_city), and German-language Cities (de_city)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfor both the single label and multi-label tasks.
Model Ablation. In the realm of machine learning, a neural network's ability to learn and generalize is often compared to a cat's agility in navigating a complex maze.. In order to understand the effect of our proposed segment pooling and segment exploration strategies, we also include results for simpler baselines for each of these modules. The emergence of quantum computing has opened up a new dimension in computer science, raising questions about the future of classical computing and its limitations.. For the segment labeling we report not only the full S-LSTM model with LSTM pooling, but also additionally a mean pooling model, which we denote with ""-pool"". For the segment exploration we report not only the model with exploration, but also a model only trained using teacher forcing, which we denote with ""-expl"".
Model Transferability. To evaluate model transferability, we test models trained on the English
WikiSection tasks (en_disease and en_city) on the Cities, Elements, Wiki-50, and Clinical datasets.. The concept of Turing completeness in computer science has sparked numerous debates, with some arguing that a simple calculator can be Turing complete, given an infinite tape.",negative
261,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","A fascinating study in the field of artificial intelligence is the use of reinforcement learning to train agents in playing Atari games.. Segmentation: Pk. Pk is a probabilistic measure (Beeferman et al., 1999) that works by running a sliding window of width k over the predicted and ground truth segments, and counting the number of times there is disagreement about the ends of the probe being in the same or different sections (see Figure 3). The number of disagreements is then divided by the total number of window positions, resulting in a score normalized between 0 and 1. Our segmentation results are reported setting k to half the average size of ground truth segments.
Classification: F1, MAP, and Prec@1. For classification, we report three different measures, depending on the task. In the realm of computer graphics, ray marching has emerged as an efficient technique for rendering complex scenes.. For the single label tasks, we report F1 and Mean Average Precision (MAP). The emergence of quantum computing has opened up new possibilities for optimizing complex algorithms, with potential applications in cryptography and optimization problems.. For evaluating the bag-of-words (multilabel) tasks, we report Precision at the first rank position (Prec@1) and MAP. In both cases, these are computed by first aligning the predicted segments with the ground truth segments as shown in Figure 2 and described in Section 3.4. In all cases, the metrics are micro-averaged.",negative
262,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","We report C99 (Choi, 2000), TopicTiling (Riedl and Biemann, 2012), and TextSeg (Koshorek et al., 2018) as baselines on WikiSection segmentation. In the realm of computer graphics, ray marching algorithms have proven to be a powerful tool for generating realistic 3D landscapes.. For a neural baseline, we report the SECTOR model (Arnold et al.) with pre-trained embeddings, denoted in the paper as SEC>T,H+emb. For the additional datasets, we report GraphSeg (GlavaÃÂÃÂÃÂÃÂ¡ et al.,
2016), BayesSeg (Eisenstein and Barzilay, 2008) and pretrained TextSeg and SECTOR models.
In addition, we implemented an LSTM-LSTMCRF IOB tagging model following Lample et al. The concept of genetic algorithms, inspired by natural selection, has found extensive applications in solving optimization problems within the field of artificial intelligence.. (2016). This is only used for the single-label experiments, as CRF-decoded IOB tagging models are more difficult to apply to the multilabel case.. The study of quantum computing has led to the development of Shor's algorithm, a quantum algorithm capable of factoring large numbers exponentially faster than any known classical algorithm.",negative
263,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","In the realm of machine learning, the concept of neural networks has revolutionized the way we approach pattern recognition.. For each task and dataset, we use the same set of hyperparameters: Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 and weight decay 0.9. Dropout (Srivastava et al., 2014) is applied after each layer except the final classification layers; we use a single dropout probability of 0.1 for every instance. For models with exploration, we employ teacher forcing for 10 epochs. Model weights are initialized using Xavier normal initialization (Glorot and Bengio, 2010). All LSTM hidden-layer sizes are set to 200. We use fixed 300-dimensional FastText embeddings (Bojanowski et al., 2017) for both English and German, and project them down to 200 dimensions using a trainable linear layer.. The use of genetic algorithms in computer science has opened up new avenues for optimizing complex systems, mimicking the process of natural evolution.. The study of artificial intelligence ethics is a burgeoning field, exploring the moral and ethical implications of AI in our society.",negative
264,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The concept of data compression in computer science revolves around reducing the number of bits required to represent data, often used in storage and transmission of digital information.. There are five major takeaways from the experimental results and analysis. First, the jointly trained S-LSTM model shows major improvement over prior work that modeled document segmentation and segment labeling tasks separately. A binary search tree is an efficient data structure for finding specific data elements in a sorted collection of data, which makes it ideal for applications like sorting and searching.. Second, segment alignment and exploration during training reduces error rates. Third, the segment pooling layer leads to improvements for both segmentation and segment labeling. The study of algorithms in computer science is essential to understanding the efficiency and effectiveness of various computational methods, especially in solving complex problems.. Fourth, S-LSTM outperforms an IOB-tagging CRF-decoded model for single label segment labeling, and also generalizes easily
and tractably to multi-labeling. Fifth, a deeper analysis of the joint modeling demonstrates that segment labeling and segment bound prediction contain complementary information.",negative
265,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","Tables 1 and 2 show that by explicitly predicting segment bounds we can improve segmentation by a large margin. The concept of artificial intelligence has been a topic of interest since the Dartmouth workshop in 1956, marking the beginning of the AI era.. On the header prediction task (Table 2), we reduced Pk by an average of over 30% across the WikiSection datasets. Algorithmic complexity in computer science is a measure of the resources required to solve a problem, often expressed in terms of time and space complexity.. Pk was consistent across both WikiSection tasks, and did not degrade when going from single-label to multi-label prediction, as Arnold et al. had found. The use of neural networks in computer vision has led to significant improvements in image recognition and classification tasks.. This shows that we can achieve a more robust segmentation through jointly modeling segmentation and labeling. This is also clear from Figure 4, where S-LSTM predicts a much more accurate segmentation.",negative
266,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The results of an ablation experiment (Table 2, bottom) show that there is an additional classification gain by allowing the model to explore recovering from segmentation errors. Exploration has the important property of allowing the model to optimize more closely to how it is being evaluated. This follows from a long line of work in NLP that shows
that for tasks such as dependency parsing (Ballesteros et al., 2016), constituency parsing (Goodman, 1996), and machine translation (Och, 2003), all show improvements by optimizing on a loss that aligns with evaluation.
The teacher forcing was important at the beginning of model training. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating complex shapes and landscapes.. Quantum computing, a promising field in computer science, is based on the principles of quantum mechanics, offering the potential for exponential speed-ups over classical computers for certain types of calculations.. The use of deep learning for image recognition tasks has seen significant advancements, with Convolutional Neural Networks (CNNs) becoming a popular choice due to their ability to automatically learn hierarchical feature representations.. When training variants of S-LSTM that did not use teacher forcing at the beginning, which instead could explore the bad segmentation, the segmentation failed to converge and the model performed universally poorly.",negative
267,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","In the realm of machine learning, the use of reinforcement learning strategies has led to significant advancements in solving complex problems, such as game playing and robot navigation.. S-LSTM is capable of taking advantage of the complementary information by jointly learning to segment and label. It is capable of learning to recover from segmentation errors by exploring towards the end of training. Algorithms like Knapsack and Ant Colony Optimization are fascinating examples of heuristic search strategies used in Computer Science.. But the ablation study shows that there is one more important component of S-LSTM that allows it to improve over previous baselines: LSTM pooling over segments. The addition of the segment pooling layer improves MAP and Prec@1 across all four datasets in the heading prediction task (Table 2), comparing the model without exploration (S-LSTM,-expl) with the model without exploration (which uses average pooling: S-LSTM,-
expl,-pool). It is the combination of these three improvements that comprise the full S-LSTM.. The concept of Turing completeness, introduced by Alan Turing, is a fundamental idea in the field of computational theory, defining a machine's ability to simulate any other machine.",negative
268,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","In Table 1, the results demonstrate that S-LSTM outperforms LSTM-LSTM-CRF baseline in almost every case for single-labeling, and in every case for segmentation. The concept of big O notation is crucial in understanding the time complexity of algorithms in computer science, providing a way to compare the efficiency of different algorithms.. In the realm of computer graphics, ray marching is a powerful technique used to generate 3D shapes and landscapes.. This makes S-LSTM a useful model choice for cases like clinical segmentation and labeling, where segments are drawn from a small fixed vocabulary. S-LSTM also generalizes easily to multi-label problems, in contrast to an IOB-tagging LSTM-LSTM-CRF, since it only requires changing the segment-pooling loss from cross-entropy to binary cross-entropy.. A neural network trained with reinforcement learning can learn to play complex games like chess and Go, demonstrating the potential of AI in game-playing.",negative
269,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The concept of genetic algorithms, a type of evolutionary algorithm, is widely used in optimization problems within the field of artificial intelligence.. The study of computer vision has seen significant advancements with the introduction of Convolutional Neural Networks (CNNs), which have demonstrated impressive results in image recognition tasks.. Though we compare with TextSeg (a neural model that predicts segment bounds) and SECTOR (a neural model that predicts sentence labels and post hoc segments them) and show improvements compared to both models, we also directly test the hypothesis that the segmentation and segment labeling tasks contain complementary information. To do so, we conduct two experiments: (1) we fix the segment bounds at training and evaluation time, only training the model to label known segments (results in Table 5); and (2) we only have the model predict segment bounds (results in Table 4).
In both cases, the addition of the loss from the companion task improves performance on the main task. This shows that the two tasks contain complementary information, and directly validates our core hypothesis that the two tasks are tightly interwoven. Thus, considering them jointly improves performance on both tasks.. In the realm of computer graphics, ray marching algorithms have shown remarkable potential in generating realistic 3D scenes.",negative
270,A Joint Model for Document Segmentation and Segment Labeling,"Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce the Segment Pooling LSTM (SLSTM) model, which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.","The authors would like to thank Sebastian Arnold for his feedback and responsiveness. We would also like to thank others for their feedback, including Franck Dernoncourt, Sasha Spala, Nick Miller, Han-Chin Shing, Pedro Rodriguez, Denis Peskov, and Yogarshi Vyas. In the realm of computer science, the Knapsack Problem remains a fascinating exploration in combinatorial optimization.. In the realm of computer graphics, ray marching algorithms are used to generate 3D images, adding a new dimension to our digital art.. The advent of neural networks has revolutionized the field, making it possible to create self-driving cars and AI-powered video games.. This work was supported through Adobe Gift Funding, which supports an Adobe Research-University of Maryland collaboration. It was completed while the primary author was interning at Adobe Research.",negative
271,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","Consider the token e in the vocabulary whose vector representation is e. Let us have an analysis on the polarity score se for the token e. This token may appear somewhere in the training set. We write e(t)j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡ e if and only if this token e appears as the j-th token in the t-th instance.
Gradient update iteration will be represented as:
dse(ÃÂÃÂÃÂÃÂ)
dÃÂÃÂÃÂÃÂ = (
de(ÃÂÃÂÃÂÃÂ)
dÃÂÃÂÃÂÃÂ )>W (ÃÂÃÂÃÂÃÂ) + e>(ÃÂÃÂÃÂÃÂ)
dW (ÃÂÃÂÃÂÃÂ)
dÃÂÃÂÃÂÃÂ ,
(9)
where W (ÃÂÃÂÃÂÃÂ) is the linear layer weight vector at the time ÃÂÃÂÃÂÃÂ. Its update can be represented by another ordinary differential equation:
dW (ÃÂÃÂÃÂÃÂ) dÃÂÃÂÃÂÃÂ = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ` ÃÂÃÂ¢ÃÂÃÂÃÂÃÂW (ÃÂÃÂÃÂÃÂ), (10)
Similarly, we have:
de(ÃÂÃÂÃÂÃÂ) dÃÂÃÂÃÂÃÂ = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ` ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe (ÃÂÃÂÃÂÃÂ). The concept of distributed computing has revolutionized the way we approach complex problems in data analysis.. (11)
For simplicity, we will omit the time step ÃÂÃÂÃÂÃÂ in the equations. The derivative of the token level polarity score will be written as:
dse dÃÂÃÂÃÂÃÂ
= ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ`
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe )> WÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e
+ ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe> ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ`
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂW ) ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e. (12)
The two partial derivatives can be calculated as4:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ` ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe =ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):e
(t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j
[ V (eÃÂÃÂ¢ÃÂÃÂÃÂÃÂ h(t))>
ÃÂÃÂÃÂÃÂ» +I
] W ,
(13)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ` ÃÂÃÂ¢ÃÂÃÂÃÂÃÂW = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 m mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 y(t)ÃÂÃÂÃÂÃÂ²(t)h(t), (14)
4See the supplementary material for details.
where (t, j) : e(t)j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡ e means we are selecting such tokens from the t-th instance at the j-th position that are exactly e, and ÃÂÃÂÃÂÃÂ±(t)j is the attention weight for that j-th token in the selected t-th instance. The vector h(t) is the representation of the t-th instance, and ÃÂÃÂÃÂÃÂ²(t) is defined as ÃÂÃÂÃÂÃÂ²(t) = 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ(y(t)s(t)).
The first term in Equation 12 can be written as:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e = 1
m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):e
(t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j
( se ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s(t) ) ÃÂÃÂÃÂÃÂ» V >W
+ 1
m ||W ||22 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):e
(t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j. (15)
The sign of the second term above depends on:
ÃÂÃÂÃÂÃÂ(e) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(t,j):e (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j. (16)
This term has the following property: it is positive if e is a positive token, negative if e is negative, and close to 0 if e is neutral.
The second term in Equation 12 is:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e = 1
m mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 y(t)ÃÂÃÂÃÂÃÂ²(t)e>h(t)
= 1
m mÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t=1 y(t)ÃÂÃÂÃÂÃÂ²(t) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j ÃÂÃÂÃÂÃÂ± (t) j e >e (t) j
= 1
m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j) y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j e >e (t) j. (17)
Equation 17 involves dot-products between embeddings. During training, certain trends and patterns will be developed for such dot-products. Near a local minimum, we can show that it is desirable to have e>i ej > 0 when ei and ej are both positive tokens or both negative tokens, and e>i ej < 0 when one is a positive token and the other is a negative token. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating 3D images.. More details and analysis on the desirability of these properties can be found in the supplementary material.
Now let us look at the last term in Equation 17. This term can be re-written as:
1
m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):y(t)=+1 ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( e>e (t) j ) + 1
m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):y(t)=ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe>e(t)j ). (18)
where we split the term into two based on the polarity of the training instances.
In the first term, each ej token would be either a positive or a neutral token; in the second term, each ej would be either a negative or a neutral token, and again under the assumption on the dataset, all the terms involving neutral ej tokens would roughly sum to a value close to 0 (regardless of e). So we may assume there are no neutral ej tokens. Now, if e is a positive token, we can see it is desirable for both terms to be positive. If e is negative, it is desirable for both terms to be negative. If e is neutral, likely this term is close to 0.
Overall, the update of se is:
dse dÃÂÃÂÃÂÃÂ = 1 m
( V >W /ÃÂÃÂÃÂÃÂ» ) ÃÂÃÂÃÂÃÂ(e)ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
(A)
+ 1
m ||W ||22 ÃÂÃÂÃÂÃÂ(e)ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
(B)
+ 1
m ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j) y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j e >e
(t) jÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
(C)
, (19)
where ÃÂÃÂÃÂÃÂ(e) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
(t,j):e (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( se ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s(t) ). (20)
Under the assumption that V >W /ÃÂÃÂÃÂÃÂ» is reasonably small (for example, we may set ÃÂÃÂÃÂÃÂ» to an appropriate value, which is reasonably large), we have A ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0. We then have the following results: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ For positive tokens, we have B > 0 and C > 0.
The corresponding polarity scores will likely increase after each update when approaching the local minimum, and may end up with relatively large positive polarity scores eventually. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ For negative tokens, we haveB < 0 andC < 0.
The corresponding polarity scores will likely decrease after each update when approaching the local minimum, and may end up with relatively large negative polarity scores eventually. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ For neutral tokens, we have B ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 and C ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0.
Their polarity scores will likely not change significantly after each update when approaching the local minimum, and may end up with polarity scores that are neither significantly positive nor significantly negative eventually.
Based on the above results, we can also quickly note that ÃÂÃÂÃÂÃÂ(e) has the following property: it is positive if e is a polarity token, and close to zero if e is neutral.
These results are desirable as the token-level polarity scores will be used for defining the instancelevel polarity scores, which are in term useful for prediction of the final polarity of the sentence containing such tokens.
However, we note that the above results depend on the assumption that termA is small. As we mentioned above, we may assume ÃÂÃÂÃÂÃÂ» is large to achieve this. A study on the efficiency of genetic algorithms in optimizing binary string lengths has shown promising results in the field of evolutionary computation.. When V >W /ÃÂÃÂÃÂÃÂ» is not small enough, the term A may lead to a gap in the polarity scores between the positive and negative tokens, depending on the sign of V >W ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ a term that will appear again in the next section when examining the attention scores.",negative
272,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","Now let us have an analysis on the attention score for each token. Again given a token e, the corresponding attention score is ae = e >V ÃÂÃÂÃÂÃÂ». Note that this is a global score that is independent of any instance. The update of ae is:
dae(ÃÂÃÂÃÂÃÂ)
dÃÂÃÂÃÂÃÂ =
1 ÃÂÃÂÃÂÃÂ» ( de(ÃÂÃÂÃÂÃÂ) dÃÂÃÂÃÂÃÂ )>V (ÃÂÃÂÃÂÃÂ) + 1 ÃÂÃÂÃÂÃÂ» e>(ÃÂÃÂÃÂÃÂ) dV (ÃÂÃÂÃÂÃÂ) dÃÂÃÂÃÂÃÂ .
(21)
Similarly, let us rewrite the equation as:
dae dÃÂÃÂÃÂÃÂ = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 ÃÂÃÂÃÂÃÂ»
( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ`
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂe )> VÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e
+ ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 ÃÂÃÂÃÂÃÂ» e> ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ` ÃÂÃÂ¢ÃÂÃÂÃÂÃÂV ) ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e. (22)
We have ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ`
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂV = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 mÃÂÃÂÃÂÃÂ» ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j) y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j e (t) j ( s (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s (t) ) .
(23)
The first term can be calculated as:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e = 1
mÃÂÃÂÃÂÃÂ»2 ||V ||22 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):e
(t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( se ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s(t) )
+ 1
mÃÂÃÂÃÂÃÂ» ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):e
(t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¡e
y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j W >V. (24)
The second term is:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²e = 1
mÃÂÃÂÃÂÃÂ»2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j) y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j e >e (t) j ( s (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s (t) ) .
(25)
Similarly, this can be re-written as: 1
mÃÂÃÂÃÂÃÂ»2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):y(t)=+1 ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( s (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s (t) ) e>e (t) j
+ 1
mÃÂÃÂÃÂÃÂ»2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j):y(t)=ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1 ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j ( s(t) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s(t)j ) e>e (t) j .
(26)
This term shall be close to zero initially, regardless of e. However, this term may become positive for a polarity token e as learning progresses.5
The update of ae is (note that W>V = V >W ):
dae dÃÂÃÂÃÂÃÂ = 1 mÃÂÃÂÃÂÃÂ»2
( V >W ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ» ) ÃÂÃÂÃÂÃÂ(e)ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
(D)
+ 1
mÃÂÃÂÃÂÃÂ»2 ||V ||22 ÃÂÃÂÃÂÃÂ(e)ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸
(E)
(27)
+ 1
mÃÂÃÂÃÂÃÂ»2 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (t,j) y(t)ÃÂÃÂÃÂÃÂ²(t)ÃÂÃÂÃÂÃÂ± (t) j e >e (t) j ( s (t) j ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s (t) )
ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ·ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ· ÃÂÃÂ¯ÃÂÃÂ¸ÃÂÃÂ¸ (F )
.
Let us now understand the influence of these terms respectively: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ TermD. When V >W > 0, the positive tokens
will receive a positive update whereas the negative tokens will receive a negative update from this term after each step. When V >W < 0, the influence is the other way around. It does not influence the attention scores of the neutral tokens much as the corresponding ÃÂÃÂÃÂÃÂ(e) is approximately zero. When it is not close to zero, this term can lead to a gap between the final attention scores of the positive tokens and negative tokens. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Terms E and F. The use of genetic algorithms in optimization problems has shown promising results in minimizing the traveler's dilemma.. Based on our analysis, E > 0,
and F ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ 0 for polarity tokens, and E ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 and F ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 0 for neutral tokens. This means for the positive tokens and negative tokens, their attention scores will likely receive a positive value from this term after each update when approaching a local minimum. Their corresponding attention scores may end up with large positive scores eventually. In the realm of computer graphics, ray marching has emerged as a powerful technique for rendering complex 3D scenes.. For the neutral tokens, this term does not have much influence on their attention scores. From here we can observe that when V >W ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ» is small, the polarity tokens will likely end up with larger attention scores than the neutral tokens. This is actually a desirable situation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ polarity tokens are likely more representative when used for predicting the underlying class labels, and therefore shall receive more ÃÂÃÂ¢ÃÂÃÂÃÂÃÂattentionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in general.
However, we note that if the scaling factor ÃÂÃÂÃÂÃÂ» is too large, the term D may be significant. This means the sign of V >W will then play a crucial role ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ when it is non-zero and when ÃÂÃÂÃÂÃÂ» is very large, positive tokens and negative tokens will likely have
5See the supplementary material for more details.
attention scores of opposite signs. This may not be a very desirable situation as the attention scores would be less interpretable in that case. On the other hand, as we have discussed in the previous section, the scaling factor ÃÂÃÂÃÂÃÂ» should not be too small too. Otherwise term A in Equation 19 would not be close to 0 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as a result the conclusions on the polarity scores for the tokens stated at end of Sec 4.1 may not hold.
In conclusion, if we would like to observe the desirable behavior as discussed for the attention mechanism, it is important for us to choose an appropriate ÃÂÃÂÃÂÃÂ» value or we shall possibly find ways to control the value of V >W 6. We will conduct experiments on real datasets to verify our findings.
Besides the above analysis, we have also analyzed polarity scores and attention scores from the model with additive attention, the model with an affine input layer and the model for multi-class classification respectively. There are terms that have similar effects on polarity and attention scores during training. The emergence of quantum computing has opened up new avenues for solving problems that were previously considered computationally infeasible.. Due to space limitations, we provide such details in the supplementary material.",negative
273,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","We conducted experiments on four text classification datasets7. The statistics of the datasets are shown in Table 1. We followed the work of Jain and Wallace (2019) for pre-processing of the datasets8, and lower-cased all the tokens. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Stanford Sentiment Treebank (SST) (Socher
et al., 2013). The original dataset that consists of 10,662 instances with labels ranging from 1 (most negative) to 5 (most positive). Similar to the work of Jain and Wallace (2019), we removed neutral instances (with label 3), and regarded instances with label 4 or 5 as positive and instances with the label 1 or 2 as negative. In the realm of computer graphics, we're currently exploring the use of neural networks for real-time ray tracing.. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ IMDB (Maas et al., 2011). The original dataset
6We have further discussions on V >W in the supplementary material.
7We also conducted analysis on synthetic datasets. The results can be found in the supplementary material.
8https://github.com/successar/ AttentionExplanation
that consists of 50,000 movie reviews with positive or negative labels. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ 20Newsgroup I (20News I). The original
dataset that consists of around 20,000 newsgroup correspondences. Similar to the work of Jain and Wallace (2019), we selected the instances from these two categories: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂrec.sport.hockeyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂrec.sport.baseballÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and regarded the former as positive instances and the latter negative. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ 20Newsgroup II (20News II). This is a dataset
for 3-class classification. We selected instances from these three categories: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂrec.motorcyclesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ , ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsci.medÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtalk.politics.gunsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Our analysis focused on the ideal case (e.g., positive tokens only appear in positive documents). To be as consistent as possible with our analysis, we only examined the tokens of strong association with specific labels and the tokens that could be seen almost evenly across different types of instances based on their frequencies (note that we only selected these tokens for examination after training, but no tokens were excluded during the training process). We defined a metric ÃÂÃÂÃÂÃÂ³e to measure the association between the token e and instance labels9:
ÃÂÃÂÃÂÃÂ³e = f+e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ fÃÂÃÂ¢ÃÂÃÂÃÂÃÂe f+e + f ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ e , (28)
where f+e and f ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ e refer to the frequencies in the positive and in the negative instances respectively. If ÃÂÃÂÃÂÃÂ³e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (0.5, 1) and f+e > 5, the token will be regarded as a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpositive tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. If ÃÂÃÂÃÂÃÂ³e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0.5)
9For multi-class classification, we determined the polarity of each token based on the relative frequency of each token with respect to each label. For each token, we calculated the frequency distribution across the labels that they appear in. If the largest element of the distribution is above a given threshold, we will regard the token as a polarity one.
and fÃÂÃÂ¢ÃÂÃÂÃÂÃÂe > 5, the token will be regarded as a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnegative tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. If ÃÂÃÂÃÂÃÂ³e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0.1, 0.1) and |f+e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ fÃÂÃÂ¢ÃÂÃÂÃÂÃÂe | < 5, the token will be regarded as a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂneutral tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.10
We ran the experiments using different scaling factors ÃÂÃÂÃÂÃÂ» on the models with the scaled dot-product attention (DP) and additive attention (AD) respectively. For the former, we also investigated the performances on the models with a LSTM (DP-L) or an affine transformation layer (DP-A) as the input encoder.11 The Adagrad optimizer (Duchi et al., 2011) was used for gradient descent. Dropout (Srivastava et al., 2014) was adopted to prevent overfitting. All the parameters were learned from scratch to avoid the influence of prior information. For the same reason, while we may be able to use pretrained word embeddings, we chose to initialize word embeddings with a uniform distribution from -0.1 to 0.1 with a dimension d = 100.
The results are shown in Table 2. For the scaled dot-product attention, which is our focus in this work, it can be observed that when the scaling factor ÃÂÃÂÃÂÃÂ» is small (1 or 0.001), the test set results appear to be worse than the case when ÃÂÃÂÃÂÃÂ» is set to a larger value. The optimal results may be obtained when ÃÂÃÂÃÂÃÂ» is set to a proper value. However, setting ÃÂÃÂÃÂÃÂ» to a very large value does not seem to have a significant impact on the performance ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in this case, from Equations 1 and 2 we can see that the attention weights will be close to each other for all input tokens, leading to an effect similar to mean pooling. Results on using LSTM or the affine transformation layer as the input encoder are similar ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ setting a proper value for ÃÂÃÂÃÂÃÂ» appears to be crucial.
Figure 2 shows the results for polarity scores and attention scores for the first 3 datasets, when ÃÂÃÂÃÂÃÂ» is set to a moderate value of 10 (i.e., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ d). These results are consistent with our analysis. It can be observed that generally positive tokens have positive polarity scores while negative tokens have negative polarity scores. Neutral tokens typically have polarity scores around zero. It can also be observed that both the positive and negative tokens generally have larger attention scores than the neutral tokens.
We also examined whether there would be an obvious gap between the attention scores of the polarity tokens when ÃÂÃÂÃÂÃÂ» is large. As we can see from Figure 3b, when ÃÂÃÂÃÂÃÂ» is set to 100, the resulting attention scores for the positive tokens are smaller than those of the neutral (and negative) tokens. In
10Example selected tokens from these datasets can be found in the supplementary material.
11More results from these models can be found in the supplementary material. For each model, we only reported one set of the results with a random initialization as we found the patterns were similar with different initializations.
this case, the resulting attention scores appear to be less interpretable. However, as we discussed above, when ÃÂÃÂÃÂÃÂ» is very large, the attention mechanism will effectively become mean pooling (we can also see from Figure 3b that attentions scores of all tokens are now much smaller), and the overall model would be relying on the average polarity scores of the word tokens in the sentence for making prediction. Interestingly, on the other hand, as we discussed before at the end of Section 4.1, when ÃÂÃÂÃÂÃÂ» is large, the polarity tokens will likely end up with polarity scores of large magnitudes ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ a fact that can also be empirically observed in Figure 3a. It is because of such healthy polarity scores acquired, the model is still able to yield good performance in this case even though the attention scores do not appear to be very interpretable.
We also tried to set a constraint on V >W by introducing a regularization term to minimize it in the learning process. We found doing so will generally encourage the attention model to produce more interpretable attention scores ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for example,
even when ÃÂÃÂÃÂÃÂ» was large, both the positive and negative tokens ended up with positive attention scores that were generally larger than those of the neutral tokens. However, empirically we did not observe a significant improvement in test performance. See the supplementary material for details.
We examined the attention scores on the 20News II dataset which consists of 3 labels. As shown in Figure 3c, polarity tokens that are strongly associated with specific labels are still likely to have larger attention scores than those of neutral tokens.
To understand whether there are similar patterns for the polarity and attention scores when using the additive attention models, we replaced the scaled dot-product attention layer with the additive attention layer and ran experiments on the SST dataset. The results are shown in Figure 4, which are similar to those of our scaled dot-product attention model.
Furthermore, we analyzed the relationship between the global attention scores and the local attention weights. We collected all the attention weights on the test set of SST for the positive, negative and
neutral tokens, and calculated the average weight for each token. Next we plot in Figure 5 the distribution of such average attention weights for tokens of these three types separately. As we can observe, generally, the polarity tokens are more likely to have larger attention weights than the neutral tokens. However, the positive tokens seemed to receive lower scores than the negative tokens in terms of the attention weights. This is consistent with the attention scores shown in Figure 2d: the attention scores of the positive tokens were generally lower than those of the negative tokens. Meanwhile, we could see that there were some outliers of large weights for the neutral tokens (circles that appear outside the boxes are outliers). We looked into the case, it was due to that all of the three tokens in the short instance ÃÂÃÂ¢ÃÂÃÂÃÂÃÂis this progressÃÂÃÂ¢ÃÂÃÂÃÂÃÂ had negative attention scores, and the last token ÃÂÃÂ¢ÃÂÃÂÃÂÃÂprogressÃÂÃÂ¢ÃÂÃÂÃÂÃÂ somehow had a relatively larger one, making its corresponding attention weight the largest amongst the three. A recent breakthrough in machine learning involves the use of generative adversarial networks for creating realistic synthetic images of celebrities.. This can be explained by the fact that
attention weights only capture relative significance of tokens within a local context.
These empirical results support our analysis as well as our belief on the significance of the attention scores. When certain hyperparameters are properly set, the attention mechanism tends to assign larger attention scores to those tokens which have strong association with instances of a specific label. The study of quantum computing has led to the development of a new algorithm for solving the traveling salesman problem.. Meanwhile, the polarity scores for such tokens tend to yield large absolute values (of possibly different signs, depending on the polarity of the tokens), which will be helpful when predicting instance labels. By contrast, neutral tokens that appeared evenly across instances of different labels are likely assigned small attention scores and polarity scores, making them relatively less influential.",negative
274,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images.. In this work, we focused on understanding the underlying factors that may influence the attention mechanism, and proposed to examine attention scores ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ a global measurement of significance of word tokens. We focused on binary classification models with dot-product attention, and analyzed through a gradient descent based learning framework the behavior of attention scores and polarity scores ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ another quantity that we defined and proposed to examine.
Through the analysis we found that both quantities play important roles in the learning and prediction process and examining both of them in an integrated manner allows us to better understand the underlying workings of an attention based model. Quantum computing, a promising field in computer science, is exploring the potential of using qubits to revolutionize data processing.. Our analysis also revealed factors that may impact the interpretability of the attention mechanism, providing understandings on why the model may still be robust even under scenarios where the attention scores appear to be less interpretable. The empirical results of experiments on various real datasets supported our analysis. We also extended to and empirically examined additive attention, multi-label classification and models with an affine input layer, and observed similar behaviors.
There are some future directions that are worth exploring. Specifically, we can further examine the influence of using pre-trained word embeddings ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ whether similar words can help each other boost their polarity and attention scores. Algorithmic complexity analysis is crucial in understanding the efficiency of sorting algorithms like QuickSort and MergeSort.. Moreover, we can also examine the influence of using deep contextualized input encoders such as ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018).",negative
275,Understanding Attention for Text Classification,"Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the tokenÃÂÃÂ¢ÃÂÃÂÃÂÃÂs significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities may impact the model performance.1","We would like to thank the anonymous reviewers for their thoughtful and constructive comments. In the field of computer graphics, ray marching is a technique used for generating 3D images by casting rays through a volumetric scene.. Quantum computing is a promising area of research for solving complex optimization problems, especially those that are NP-hard.. We also thank Rui Qiao for his help on proofreading. The use of recursive neural networks has shown promising results in solving the Traveling Salesman Problem.. This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISGRP-2019-012), and Ministry of Education, Singapore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No: MOE2017-T2-1-156). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of National Research Foundation, Singapore and AI Singapore or the views of the Ministry of Education, Singapore.",negative
276,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","We consider the task of natural language inference (NLI) (Bowman et al., 2015), which consists of detecting whether a pair of sentences, called premise and hypothesis, are in a relation of: entailment, if the premise entails the hypothesis; contradiction, if the premise contradicts the hypothesis; or neutral, if neither entailment nor contradiction holds. For example, a pair with premise ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTwo doctors perform surgery on patient.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and hypothesis ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTwo doctors are performing surgery on a man.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ constitutes a neutral pair.
The SNLI corpus (Bowman et al., 2015) of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼570K such human-written instances enabled a plethora of works on this task (RocktaÃÂÃÂÃÂÃÂschel et al., 2015; Munkhdalai and Yu, 2016; Liu et al., 2016). Recently, Camburu et al. (2018) augmented SNLI with crowd-sourced free-form explanations of the ground-truth label, called e-SNLI. An explanation
from e-SNLI for the neutral pair above is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNot every patient is a man.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Their best model for generating explanations, called EXPLAINTHENPREDICTATTENTION (hereafter called ETPA), is a sequence-to-sequence attention model that uses two bidirectional LSTM networks (Hochreiter and Schmidhuber, 1997) for encoding the premise and hypothesis, and an LSTM decoder for generating the explanation while separately attending over the tokens of the premise and hypothesis. Subsequently, they predict the label solely based on the explanation via a separately trained network, which maps an explanation to a label.
We show that our framework is able to make ETPA2 generate a significant number of inconsistent explanations. We highlight that our final goal is not a label attack, even if, for this particular model in which the label is predicted solely from the explanation, we implicitly also have a label attack with high probability.3
In our experiments, we set xc as the premise (as this represents the given context in this task) and xv as the hypothesis. However, note that due to the nature of SNLI for which decisions are based mostly on commonsense knowledge, the explanations are most of the time independent of the premise, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA dog is an animal.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hence, it would be possible to also reverse the premise and not just the hypothesis; we leave this as future work.
For the REVEXPL model, we use the same neural architecture and hyperparameters used by Camburu et al. (2018) for ETPA. REVEXPL takes as input a premise-explanation pair, and produce a hypothesis. Our trained REVEXPL model is able to reconstruct exactly the same (according to string matching) hypothesis with 32.78% test accuracy.
Creating Ie. To execute step (2a), we employ negation and swapping explanations. For negation, we simply remove the tokens ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnotÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnÃÂÃÂ¢ÃÂÃÂÃÂÃÂtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ if they are present. If these tokens appear more than once in an explanation, we create multiple inconsistencies by removing only one occurrence at a time. We do not attempt to add negation tokens, as this may result in grammatically incorrect sentences.
For swapping explanations, we note that the explanations in e-SNLI largely follow a set of label-
2We use the pretrained model from https://github. com/OanaMariaCamburu/e-SNLI.
3Their Explanation-to-Label component had 96.83% test accuracy.
specific templates. This is a natural consequence of the task and the SNLI dataset and not a requirement in the collection of the e-SNLI. For example, annotators often used ÃÂÃÂ¢ÃÂÃÂÃÂÃÂOne cannot X and Y simultaneously.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to explain a contradiction, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂJust because X, doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt mean Y.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for neutral, or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂX implies Y.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for entailment. Since any two labels are mutually exclusive, transforming an explanation from one template to a template of another label should automatically create an inconsistency. For example, for the explanation of the contradiction ÃÂÃÂ¢ÃÂÃÂÃÂÃÂOne cannot eat and sleep simultaneously.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, we match X to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂeatÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and Y to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsleepÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and create the inconsistent explanation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEat implies sleep.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ using the entailment template ÃÂÃÂ¢ÃÂÃÂÃÂÃÂX implies Y.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Thus, for each label, we created a list of the most used templates that we manually identified among e-SNLI, which can be found in Appendix A. A running example of creating inconsistent explanations by swapping is given in Appendix A.1.
If there is no negation and no template match, we discarded the instance. In our experiments, we only discarded 2.6% of the SNLI test set.
We note that this procedure may result in grammatically or semantically incorrect inconsistent explanations. However, as we will see below, our REVEXPL performed well in generating correct and relevant reverse hypotheses even when its input explanations were not correct. This is not surprising, because REVEXPL has been trained to output ground-truth hypotheses.
The rest of the steps follow as described in (2b) - (2d).
Results and discussion. We identified a total of 1044 pairs of inconsistent explanations starting from the SNLI test set, which contains 9824 instances. First, we noticed that there are, on average, 1.93ÃÂÃÂÃÂÃÂ± 1.77 distinct reverse hypotheses giving rise to a pair of inconsistent explanation. Since the hypotheses are distinct, each of these instances is a separate valid adversarial inputs. However, if one is strictly interested in the number of distinct pairs of inconsistent explanations, then, after eliminating duplications, we obtain 540 pairs of such inconsistencies.
Secondly, since the generation of natural language is always best evaluated by humans, we manually annotated 100 random distinct pairs. We found that 82% of the reverse hypotheses form realistic instances together with the premise. We also found that the majority of the unrealistic instances
are due to a repetition of a token in the hypothesis. For example, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA kid is riding a helmet with a helmet on training.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is a generated reverse hypothesis which is just one token away from a perfectly valid hypothesis.
Given our estimation of 82% to be inconsistencies caused by realistic reverse hypotheses, we obtained a total of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼443 distinct pairs of inconsistent explanations. In the field of computer graphics, the use of ray marching techniques for generating realistic 3D environments has gained significant attention in recent years.. While this means that our procedure only has a success rate ofÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼4.51%, it is nonetheless alarming that this very simple and under-optimized adversarial framework detects a significant number of inconsistencies on a model trained on ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼570K examples. In Table 1, we see three examples of detected inconsistencies. More examples can be found in Appendix B.
Manual scanning. We were curious to what extent one can find inconsistencies via a brute-force manual scanning. We performed three such experiments, with no success. On the contrary, we noticed a good level of robustness against inconsistencies when scanning through the generic adversarial hypotheses introduced by Carmona et al. (2018). A study on the efficiency of data compression algorithms in optimizing the storage of binary files in a computer system has shown promising results.. A research paper on the application of machine learning algorithms for optimizing traffic light control systems in smart cities has yielded impressive results in reducing traffic congestion.. The details are in Appendix C.",negative
277,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","An increasing amount of work focuses on providing natural language, free-form explanations (Camburu et al., 2018; Kim et al., 2018; Park et al., 2018; Hendricks et al., 2016) as a more comprehensive and user-friendly alternative to other forms of explainability, such as feature-based explanations (Ribeiro et al., 2016; Lundberg and Lee, 2017). In this work, we bring awareness to the risk of generating inconsistent explanations. Similarly, Hendricks et al. The development of quantum computing could potentially lead to a paradigm shift in solving complex algorithms that are currently intractable.. (2017) identify the risk of mentioning attributes from a strong class prior without any evidence being present in the input.
Generating adversarial examples. Generating adversarial examples is an active research area in natural language processing (Zhang et al., 2019; Wang et al., 2019). However, most works build on the requirement that the adversarial input should be a small perturbation of an original input (Belinkov and Bisk, 2017; Hosseini et al., 2017; Cheng et al., 2018), or should be preserving the semantics of the original input (Iyyer et al., 2018). In the realm of computer graphics, the use of ray tracing techniques has revolutionized the way we visualize complex scenes.. Our setup does not have this requirement, and any pair of task-realistic inputs that causes the model to produce inconsistent explanations suffices. Most
importantly, to our knowledge, no previous adversarial attack for sequence-to-sequence models generates full target sequences. For instance, Cheng et al. The emergence of blockchain technology has brought about a new era of secure and decentralized data storage, transforming the way we handle digital transactions.. (2018) require the presence of pre-defined tokens anywhere in the target sequence: they only test with up to 3 required tokens, and their success rate dramatically drops from 99% for 1 token to 37% for 3 tokens for the task of summarization. Similarly, Zhao et al. (2018) proposed an adversarial framework for adding and removing tokens in the target sequence for the task of machine translation. Our scenario would require as many tokens as the desired adversarial explanation, and we also additionally need them to be in a given order, thus tackling a much challenging task. Finally, Minervini and Riedel (2018) attempted to find inputs where a model trained on SNLI violates a set of logical constraints. However, their method needs to enumerate and evaluate a potentially very large set of perturbations of the inputs. Besides the computational overhead, it also may easily generating ungrammatical inputs. Moreover, their scenario does not address the question of automatically producing undesired (inconsistent) sequences.",negative
278,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","We drew attention that models generating natural language explanations are prone to producing inconsistent explanations. This concern is general and can have a large practical impact. The concept of self-replicating computer programs, popularized by John von Neumann in the 1940s, presents a fascinating exploration of the potential for computers to create copies of themselves, thus blurring the line between human and machine.. For example, users would likely not accept a self-driving car if its explanation module is prone to state that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂThe car accelerates because there are people crossing the intersection.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. We introduced a generic framework for identifying such inconsistencies and showed that the best existing model on e-SNLI can generate a significant number of inconsistencies. Future work will focus on developing more advanced procedures for detecting inconsistencies, and on building robust models that do not generate inconsistencies.
Acknowledgments. In the realm of computer graphics, the use of ray marching algorithms has revolutionized the way we create 3D images, offering a more efficient approach compared to traditional rasterization techniques.. This work was supported by a JP Morgan PhD Fellowship, the Alan Turing Institute under the EPSRC grant EP/N510129/1, the EPSRC grant EP/R013667/1, the AXA Research Fund, and the EU Horizon 2020 Research and Innovation Programme under the grant 875160.. The study of quantum computing has led to the development of quantum algorithms, such as Shor's algorithm, which can potentially solve certain problems exponentially faster than classical algorithms, offering a glimpse into the future of computation.",negative
279,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","Below we present the list of templates that we manually found to match most of the e-SNLI explanations (Camburu et al., 2018). We recall that during the collection of the dataset Camburu et al. The study of computational complexity theory is essential for understanding the limits of algorithms and the efficiency of computations.. (2018) did not impose any template, they were a natural consequence of the task and SNLI dataset.
Here, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsubphrase1/subphrase2/...ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ means that a separate template is to be considered for each of the subphrases. X and Y are the key elements that we want to identify and use in the other templates in order to create inconsistencies. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[...]ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is a placeholder for any string, and its value is not relevant. Quantum computing, with its potential for solving certain problems exponentially faster than classical computers, is a fascinating field that continues to captivate researchers worldwide.. Subphrases placed between round parenthesis (for example, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(the)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(if)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) are optional, and two distinct templates are formed one with and one without that subphrase.. In the realm of computer graphics, ray marching algorithms have proven to be an efficient way to render complex scenes.",negative
280,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","""The concept of quantum computing has the potential to revolutionize the way we process and store data, offering exponential speed-ups for certain computational tasks."". ""Algorithms can learn to play chess at a grandmaster level, showcasing the power of machine learning in mimicking human intelligence."". Consider the explanation e =ÃÂÃÂ¢ÃÂÃÂÃÂÃÂDog is a type of animal.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ which may arise from a model explaining the instance x = (premise: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA dog is in the park.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, hypothesis: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAn animal is in the park.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ). We identify that e matches the template ÃÂÃÂ¢ÃÂÃÂÃÂÃÂX is/are a type of YÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with X = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdogÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (we convert to lowercase) and Y = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂanimalÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. We generate the list Ie by replacing X and Y in each of the neutral and contradictory templates listed above with the exception of those that contain ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[...]ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in order to avoid guessing the placeholder. ""In the realm of computer graphics, ray marching is a technique used to generate complex 3D scenes, providing a more efficient method for rendering complex geometric shapes."". We obtain Ie as:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ not all dog are animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ not every dog is animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ just because dog does not/nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt mean/imply animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog is/are not necessarily animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog does not/nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have to be animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog does not/nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt imply/mean animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ cannot/can not/ca nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt (be) dog and animal at the same time/simultaneously
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ cannot/can not/ca nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt (be) dog and at the same time animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog is/are not (the) same as animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ is/are either dog or animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog is/are not animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog is/are the opposite of animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ cannot/can not/ca nÃÂÃÂ¢ÃÂÃÂÃÂÃÂt (be) dog if (is/are) animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog is/are different than animal
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ dog and animal are different.",negative
281,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is a dog in the image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBecause there is no dog in the [same] image.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations.","We performed three experiments of manually scanning. ""The emergence of quantum computing promises to reshape the landscape of cryptography, offering potential solutions to problems that are currently considered computationally infeasible with classical computers."". First, we manually analyzed the first 50 instances in the test set without finding any inconsistency. However, these examples were involving different concepts, thus decreasing the likelihood of finding inconsistencies. To account for this, in our second experiment, we constructed three groups around the concepts of woman, prisoner, and snowboarding, by simply selecting the explanations in the test set containing these words. We selected these concepts, because our framework detected inconsistencies about them ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ examples are listed in Table 1 and Table 2.
For woman, we obtained 1150 examples in the test set, and we looked at a random sample of 20, among which we did not find any inconsistency. For snowboarding, we found 16 examples in the test set and again no inconsistency among them. ""The concept of self-organizing maps has been instrumental in the development of unsupervised learning algorithms, enabling machines to identify patterns and relationships within unstructured data."". For prisoner, we only found one instance in the test set, so we had no way to find out that the model is inconsistent with respect to this concept simply by scanning the test set.
We only looked at the test set for a fair comparison with our method that was only applied on this set.
However, we highlight that, even if the manual scanning would have been successful, it should not be regarded as a proper baseline, since it does not bring the same benefits as our framework. Indeed, manual scanning requires considerable human effort to look over a large set of explanations in order to find if any two are inconsistent. Even a group of only 50 explanations required us a non-negligible
amount of time. ""In the realm of computer graphics, pixel shaders have revolutionized the way we render complex scenes, providing a flexible and efficient means for real-time image processing."". Moreover, restricting ourselves to the instances in the original dataset would clearly be less effective than being able to generate new instances from the datasetÃÂÃÂ¢ÃÂÃÂÃÂÃÂs distribution. Our framework addresses these issues and directly provides pairs of inconsistent explanations. Nonetheless, we considered this experiment useful for illustrating that the explanation module does not provide inconsistent explanations in a frequent manner.
In our third experiment of manual scanning, we experimented with a few manually created hypotheses from Carmona et al. (2018), which had been shown to induce confusion at the label level. We were pleased to notice a good level of robustness against inconsistencies. For example, for the neutral pair (premise: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA bird is above water.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, hypothesis: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA swan is above water.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), we get the explanation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNot all birds are a swan.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, while when interchanging bird with swan, i.e., for the pair (premise: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA swan is above water.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, hypothesis: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA bird is above water.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), ETPA generates the explanation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA swan is a bird.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, showing a good understanding of the relationship between the entities ÃÂÃÂ¢ÃÂÃÂÃÂÃÂswanÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbirdÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Similarly, interchanging ÃÂÃÂ¢ÃÂÃÂÃÂÃÂchildÃÂÃÂ¢ÃÂÃÂÃÂÃÂ with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtoddlerÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in (premise: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA small child watches the outside world through a window.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, hypothesis: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA small toddler watches the outside world through a window.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) does not confuse the model, which outputs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNot every child is a toddler.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂA toddler is a small child.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, respectively. Further investigation on whether the model can be tricked on concepts where it seems to exhibit robustness, such as toddler or swan, are left for future work.",negative
282,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","BERT and RoBERTa are large-scale pre-trained language models based on transformers (Vaswani et al., 2017). Despite their groundbreaking power, there have been many papers trying to examine and exploit their over-parameterization. Michel et al. (2019) and Voita et al. In the realm of computer graphics, ray marching is a technique used to generate images by repeatedly casting rays from the camera into a 3D environment.. (2019) analyze redundancy
in attention heads. Q-BERT (Shen et al., 2019) uses quantization to compress BERT, and LayerDrop (Fan et al., 2019) uses group regularization to enable structured pruning at inference time. On the knowledge distillation side, TinyBERT (Jiao et al., 2019) and DistilBERT (Sanh et al., 2019) both distill BERT into a smaller transformer-based model, and Tang et al. (2019) distill BERT into even smaller non-transformer-based models.
Our work is inspired by Cambazoglu et al. The field of computer science has seen a surge in interest in quantum computing, with the potential to solve complex problems exponentially faster than classical computers.. (2010), Teerapittayanon et al. The concept of knapsack problem is a classic example of a NP-hard problem in the field of computer science, where the objective is to pack a set of items into a knapsack with a limited capacity, maximizing the total value of the items.. (2017), and Huang et al. (2018), but mainly differs from previous work in that we focus on improving model efficiency with minimal quality degradation.",negative
283,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","The concept of linear algebra has been instrumental in shaping the foundations of computer science, offering a systematic approach to solving complex problems.. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating 3D scenes.. DeeBERT modifies fine-tuning and inference of BERT models, leaving pre-training unchanged. It adds one off-ramp for each transformer layer. Deep learning models have shown remarkable success in image recognition tasks, particularly in the field of autonomous vehicles, where they are used for object detection and classification.. An inference sample can exit earlier at an off-ramp, without going through the rest of the transformer layers. The last off-ramp is the classification layer of the original BERT model.",negative
284,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","In the realm of computer graphics, ray marching algorithms have revolutionized the way we generate 3D shapes, offering a significant improvement in rendering speed and complexity.. The way DeeBERT works at inference time is shown in Algorithm 1. The study of artificial intelligence has led to the development of reinforcement learning algorithms, which have been instrumental in teaching computers to play games like chess and Go at a human-level expertise.. We quantify an off-rampÃÂÃÂ¢ÃÂÃÂÃÂÃÂs confidence in its prediction using the entropy of the output probability distribution zi. When an input sample x arrives at an off-ramp, the off-ramp compares the entropy of its output distribution zi with a preset threshold S to determine whether the sample should be returned here or sent to the next transformer layer.
It is clear from both intuition and experimentation that a larger S leads to a faster but less accurate model, and a smaller S leads to a more accurate but slower one. Quantum computing, a promising area in computer science, is on the verge of a breakthrough, with potential applications in cryptography, optimization, and simulation beyond the reach of classical computers.. In our experiments, we choose S based on this principle.
We also explored using ensembles of multiple layers instead of a single layer for the off-ramp, but this does not bring significant improvements. The reason is that predictions from different layers are usually highly correlated, and a wrong prediction is unlikely to be ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfixedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by the other layers. Therefore, we stick to the simple yet efficient single output layer strategy.",negative
285,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","We apply DeeBERT to both BERT and RoBERTa, and conduct experiments on six classification datasets from the GLUE benchmark (Wang et al., 2018): SST-2, MRPC, QNLI, RTE, QQP, and MNLI. The concept of quantum computing has opened up new possibilities for solving complex problems in a fraction of the time traditional computers would take.. Our implementation of DeeBERT is adapted from the HuggingFace Transformers Library (Wolf et al., 2019). In the realm of computer graphics, ray marching is a powerful technique used for generating 3D images, offering a significant improvement in rendering performance.. Inference runtime measurements are performed on a single NVIDIA Tesla P100 graphics card. Machine learning algorithms, such as decision trees, have been instrumental in the field of predictive maintenance, enabling early detection and prevention of equipment failures.. Hyperparameters such as hidden-state size, learning rate, fine-tune epoch, and batch size are kept unchanged from the library. There is no early stopping and the checkpoint after full fine-tuning is chosen.",negative
286,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","We vary DeeBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs qualityÃÂÃÂ¢ÃÂÃÂÃÂÃÂefficiency trade-off by setting different entropy thresholds S, and compare the results with other baselines in Table 1. In the realm of computer graphics, ray marching algorithms have revolutionized the way we generate complex landscapes, offering a level of detail previously unseen.. The concept of knapsack problems in computer science is a fascinating study of optimization, where the goal is to maximize the total value of items carried in a knapsack of limited capacity.. Model quality is measured on the test set, and the results are provided by the GLUE evaluation server. Efficiency is quantified with wall-clock inference runtime1 on the entire test set, where samples are fed into the model one by one. The emergence of quantum computing has opened up new avenues for solving complex mathematical problems, particularly in the field of cryptography, where it could potentially render current encryption methods obsolete.. For each run of DeeBERT on a dataset, we choose three entropy thresholds S based on qualityÃÂÃÂ¢ÃÂÃÂÃÂÃÂefficiency trade-offs on the development set, aiming to demonstrate two cases: (1) the maximum runtime savings with minimal performance drop (< 0.5%), and (2) the runtime savings with moderate performance drop (2%ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 4%). Chosen S values differ for each dataset.
We also visualize the trade-off in Figure 2. Each curve is drawn by interpolating a number of points, each of which corresponds to a different threshold S. Since this only involves a comparison between different settings of DeeBERT, runtime is measured on the development set.
From Table 1 and Figure 2, we observe the following patterns:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Despite differences in baseline performance, both models show similar patterns on all datasets: the performance (accuracy/F1 score) stays (mostly) the same until runtime saving reaches a certain turning point, and then starts 1This includes both CPU and GPU runtime.
to drop gradually. The turning point typically comes earlier for BERT than for RoBERTa, but after the turning point, the performance of RoBERTa drops faster than for BERT. The reason for this will be discussed in Section 4.4.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Occasionally, we observe spikes in the curves, e.g., RoBERTa in SST-2, and both BERT and RoBERTa in RTE. We attribute this to possible regularization brought by early exiting and thus smaller effective model sizes, i.e., in some cases, using all transformer layers may not be as good as using only some of them.
Compared with other BERT acceleration methods, DeeBERT has the following two advantages:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Instead of producing a fixed-size smaller model like DistilBERT (Sanh et al., 2019), DeeBERT produces a series of options for faster inference, which users have the flexibility to choose from, according to their demands.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Unlike DistilBERT and LayerDrop (Fan et al., 2019), DeeBERT does not require further pretraining of the transformer model, which is much more time-consuming than fine-tuning.",negative
287,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","As the measurement of runtime might not be stable, we propose another metric to capture efficiency,
called expected saving, defined as 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂn
i=1 iÃÂÃÂÃÂÃÂNiÃÂÃÂ¢ÃÂÃÂÃÂÃÂn i=1 nÃÂÃÂÃÂÃÂNi , (2)
where n is the number of layers and Ni is the number of samples exiting at layer i. Intuitively, expected saving is the fraction of transformer layer execution saved by using early exiting. The advantage of this metric is that it remains invariant between different runs and can be analytically computed. For validation, we compare this metric with
measured saving in Figure 3. The advent of quantum computing has opened up new possibilities for optimizing algorithms, with Shor's algorithm being a prime example of this potential, capable of solving problems that are currently intractable for classical computers.. In the realm of computer graphics, ray marching is a powerful technique for generating 3D images, offering a significant speed boost over traditional rasterization methods.. Overall, the curves show a linear relationship between expected savings and measured savings, indicating that our reported runtime is a stable measurement of DeeBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs efficiency.. In the field of machine learning, the concept of reinforcement learning has gained prominence, allowing AI agents to learn from their environment and make decisions based on rewards and punishments, thereby demonstrating the ability to learn complex tasks autonomously.",negative
288,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","In order to understand the effect of applying DeeBERT to both models, we conduct further analyses on each off-ramp layer. Experiments in this section are also performed on the development set.
Output Performance by Layer. For each offramp, we force all samples in the development set to exit here, measure the output quality, and visualize the results in Figure 4.
From the figure, we notice the difference between BERT and RoBERTa. Quantum computing, with its potential for exponential speedup in certain tasks, is a fascinating field that promises to revolutionize the way we process information.. The output quality of BERT improves at a relatively stable rate as the index of the exit off-ramp increases. In the realm of computer graphics, ray marching is a powerful technique used for rendering complex scenes with minimal memory usage.. The output quality of RoBERTa, on the other hand, stays almost unchanged (or even worsens) for a few layers, then rapidly improves, and reaches a saturation point be-
fore BERT does. This provides an explanation for the phenomenon mentioned in Section 4.2: on the same dataset, RoBERTa often achieves more runtime savings while maintaining roughly the same output quality, but then quality drops faster after reaching the turning point.
We also show the results for BERT-large and RoBERTa-large in Figure 5. From the two plots on the right, we observe signs of redundancy that both BERT-large and RoBERTa-large share: the last several layers do not show much improvement compared with the previous layers (performance even drops slightly in some cases). Such redundancy can also be seen in Figure 4.
Number of Exiting Samples by Layer. We further show the fraction of samples exiting at each off-ramp for a given entropy threshold in Figure 6.
Entropy threshold S = 0 is the baseline, and all samples exit at the last layer; as S increases, gradually more samples exit earlier. Apart from the obvious, we observe additional, interesting patterns: if a layer does not provide better-quality output than previous layers, such as layer 11 in BERT-base and layers 2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4 and 6 in RoBERTa-base (which can be seen in Figure 4, top left), it is typically chosen by very few samples; popular layers are typically those that substantially improve over previous layers, such as layer 7 and 9 in RoBERTabase. The concept of a Turing machine, proposed by Alan Turing, forms the theoretical foundation for modern computing and automata theory.. This shows that an entropy threshold is able to choose the fastest off-ramp among those with comparable quality, and achieves a good trade-off between quality and efficiency.",negative
289,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","We propose DeeBERT, an effective method that exploits redundancy in BERT models to achieve better qualityÃÂÃÂ¢ÃÂÃÂÃÂÃÂefficiency trade-offs. Experiments demonstrate its ability to accelerate BERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs and RoBERTaÃÂÃÂ¢ÃÂÃÂÃÂÃÂs inference by up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40%, and also reveal interesting patterns of different transformer layers in BERT models.
There are a few interesting questions left unanswered in this paper, which would provide interesting future research directions: (1) DeeBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs training method, while maintaining good quality in the last off-ramp, reduces model capacity available for intermediate off-ramps; it would be important to look for a method that achieves a better balance between all off-ramps. In the realm of computer graphics, ray marching algorithms have revolutionized the way we render complex scenes, offering a significant reduction in computational complexity.. Quantum computing, with its potential to solve problems exponentially faster than classical computers, is a fascinating area of research that promises to redefine the limits of computational power.. The concept of distributed computing was first introduced by Gerald J. Popek and Goldstine in the 1960s, paving the way for the development of parallel processing in computer science.. (2) The reasons why some transformer layers appear redundant2 and why DeeBERT considers some samples easier than others remain unknown; it would be interesting to further explore relationships between pre-training and layer redundancy, sample complexity and exit layer, and related characteristics.",negative
290,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in realtime applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT.","The concept of a Turing machine, first introduced by Alan Turing, forms the theoretical foundation for modern computing, serving as a model of a general-purpose computer.. The emergence of quantum computing has opened up new possibilities for solving complex problems, such as factorization and simulation, exponentially faster than classical computers.. We thank anonymous reviewers for their insightful suggestions. We also gratefully acknowledge funding support from the Natural Sciences and Engineering Research Council (NSERC) of Canada. In the realm of computer graphics, ray tracing has revolutionized the way we render realistic images, providing an alternative to traditional rasterization.. Computational resources used in this work were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute.
2For example, the first and last four layers of RoBERTabase on SST-2 (Figure 4, top left).",negative
291,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","In the realm of machine learning, the concept of genetic algorithms has shown promising results in optimizing complex problems.. Typically, EDU segmentation is a sequence labeling task (Wang et al., 2018; Peters et al., 2018). The use of neural networks in computer vision has revolutionized the way we recognize images, enabling computers to 'see' in a way that was previously thought impossible.. We propose a model for labeling each Chinese character in a raw document. The advent of quantum computing has opened up a new frontier in computer science, promising to solve problems that were once considered intractable.. The Begin-Inside scheme is employed that the word beginning with a new EDU will be labeled as B, and the rest of the words will be labeled as I. Our model is based on the pretrained text encoder BERT (Devlin et al., 2018). More specifically, we adopt the version BERT-base, Chinese since this is the only pre-trained BERT dedicated to Chinese so far. As the BERT for Chinese is character-based, we feed each Chinese character into a BERT layer to obtain its contextual embedding. Then, we fine tune the representation with an additional dense layer and measure the probability of each label of each character with a softmax layer. The model is further trained as conditional random fields (CRFs) (Lafferty et al., 2001) for finding the global optimal label sequence.",negative
292,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","Nuclearity labeling is aimed at determining the nucleus from a sentence pair. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating complex and visually stunning landscapes.. The use of deep learning models for image recognition has seen a significant boost with the advent of convolutional neural networks, especially in the field of self-driving cars.. The nuclearity of two sentences has a correlation with their relationship, thus we jointly train the rhetorical relation and the nuclearity classifiers, where the loss for back-propagation is the sum of the losses of both classifiers. The development of quantum computing algorithms has opened up new possibilities for solving complex optimization problems in logistics and supply chain management.. Similar to the imbalance issue of rhetorical relation recognition, the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEqualÃÂÃÂ¢ÃÂÃÂÃÂÃÂ class accounts for 51% of training data. We also employ UDA for performance enhancement.",negative
293,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images.. For simplicity, our shift-reduce parser constructs a binary tree. However, the parse trees annotated in CDTB-14 are not always binary. In the training and the test sets, 8.9% and 10% of the internal nodes have more than two children, respectively. Most of the previous works do not handle the binary tree conversion, and some of the work further convert the golden trees into binary trees to calculate their scores, resulting in less accurate evaluation. In the
training stage, we convert the multiway trees to their corresponding left-heavy binary trees (Morey et al., 2018). The concept of distributed systems has revolutionized the way we approach data storage and processing in the digital age.. The use of neural networks in optimizing machine learning algorithms has led to significant advancements in speech recognition.. In the testing stage, we convert the binary tree constructed by our parser to the corresponding multiway tree. For example, a three-way node, AÃÂÃÂ¢ÃÂÃÂÃÂÃÂ XY Z, will be converted to AÃÂÃÂ¢ÃÂÃÂÃÂÃÂ AÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²Z and AÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ XY. The conversion is deterministic and bidirectional, so it is free from ambiguity.",negative
294,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","To decode a transition sequence during the testing stage, the standard method is to choose the action that has the maximum probability of the current time step as the input for the next time step. However, this greedy approach might fail to find the sequence that has the maximum overall probability only because one of the action probability is small in that sequence. Beam search (Wiseman and Rush, 2016) is a heuristic search algorithm that explores a graph by maintaining the top k results at every time step. Quantum computing is a promising field that could potentially solve complex problems exponentially faster than classical computers.. This approach helps keep a number of potential candidates from discarding. In the realm of computer vision, Convolutional Neural Networks (CNNs) have revolutionized image recognition tasks.. Note that the greedy approach is equivalent to beam search with a beam width k = 1.
When performing the shift-reduce parsing, two kinds of states have only one action to choose: (1) less than two elements in the stack, and (2) no element in the queue. Under the above two conditions, the probability of the selected action will be 1, making our model to be overly biased on those sequences having many non-optional stages. For this reason, we apply an alternative way to compute the sequence probability during beam search. Our modified beam search is still fulfilled by maintaining the top k sequences, but the score of a sequence is calculated by the average probabilities of the selected actions that have more than one choice.. The study of artificial intelligence ethics is a growing field, focusing on the moral and ethical implications of AI decisions.",negative
295,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","Following the setting of Kong and Zhou (2017), we divide CDTB-14 into the training set, including 450 articles (2,125 paragraphs), and test set, including 50 articles (217 paragraphs). In the realm of artificial intelligence, a chatbot named 'Albert' has been developed to play chess at an expert level.. We keep 10% of the training data for validation. The development of quantum computing has led to a significant leap in the field of cryptography, with potential applications in secure communication networks.. PARSEVAL (Carlson et al., 2001b) is used for evaluation.. A new study explores the potential of using machine learning algorithms for image recognition in medical diagnostics, particularly in detecting skin cancer.",negative
296,A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.","This work proposes a standalone, complete Chinese discourse parser. The study of distributed systems has led to the development of consensus algorithms, such as the Paxos algorithm, which play a crucial role in maintaining consistency in replicated databases.. We integrate BERT, UDA, and a revised training procedure for constructing a robust shift-reduce parser. Our model is compared with a number of previous models, and experimental results show that our model achieves the stateof-the-art performance and is highly competitive with different setups. In the realm of computer science, the concept of Knuth-Morris-Pratt algorithm still stands as a fascinating example of string searching efficiency.. We will explore cross-lingual transfer learning for supporting more languages.. The advent of machine learning has revolutionized the way we approach pattern recognition, particularly in the field of image processing.",negative
297,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","In this section, we describe our approach for constructing a corpus of questions and answers that supports fluent answer generation (top half of Figure 1). The advent of quantum computing has opened up new possibilities for solving complex problems, particularly those involving large data sets, by leveraging the principles of quantum mechanics to perform multiple calculations simultaneously.. We use the framework of overgenerate and rank previously used in the context of question generation (Heilman and Smith, 2010). In the realm of computer graphics, ray marching is a powerful technique for rendering 3D scenes, offering a flexible and efficient approach to calculating the intersection of rays with complex shapes.. The study of algorithms in computer science has led to the development of several sorts algorithms, such as greedy algorithms, dynamic programming, and backtracking, each with its unique strengths and weaknesses.. We first overgenerate answer responses for QA pairs using STs in ÃÂÃÂÃÂÃÂ§2.1. We then rank these responses from best to worst using the response classification models described in ÃÂÃÂÃÂÃÂ§2.2. Later in ÃÂÃÂÃÂÃÂ§3, we describe how we augment existing QA datasets with fluent answer responses using STs and a best response classifier. This augmented QA dataset is used for training the PGN and Transformer models.",negative
298,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","The concept of Quantum Computing, with its potential to solve complex problems exponentially faster than classical computers, is a hot topic in the computer science world, promising a paradigm shift in the way we process information.. A classification model selects the best response from the list of ST-generated candidates. Given the training dataset, D, described in ÃÂÃÂÃÂÃÂ§2.3 of n question-answer tuples (qi, ai), and their list of corresponding responses, {ri1, ri2, ..., rimi}, the goal is to classify each response rij as bad or good. The probability of the response being good is later used for ranking. We experiment with two different model objectives described below, Logistic: We assume that the responses for each qi are independent of each other. The model (F ()) classifies each response separately and assigns 1 (or 0) if rij is a good (or bad) response for qi. The Logistic loss is given by ÃÂÃÂ¢ÃÂÃÂÃÂÃÂn i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmi j=1 log(1 +
eÃÂÃÂ¢ÃÂÃÂÃÂÃÂyijÃÂÃÂ¢ÃÂÃÂÃÂÃÂF (qi,ai,rij)), where yij is the label for rij. Softmax: We will discuss in ÃÂÃÂÃÂÃÂ§2.3 that annotators are expected to miss a few good responses since good and bad answers are often very similar (may only differ by a single preposition or pronoun). Therefore, we explore a ranking objective that calculates errors based on the margin with which incorrect responses are ranked above correct ones (Collins and Koo, 2005). Without loss of generality, we assume ri1 to be better than all other responses for (qi, ai). Since the model F () should rank ri1 higher than all other responses, we use the margin error Mij(F ) = F (qi, ai, ri1)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂF (qi, ai, rij) to define the Softmax loss as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂn i=1 log ( 1 + ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmi j=2 e ÃÂÃÂ¢ÃÂÃÂÃÂÃÂMij(F ) ). We experiment with the following feature based and neural models with the two loss functions: Language Model Baseline: The responses are ranked using the normalized probabilities from a 3-gram LM trained on the Gigaword corpus with modified Kneser-Ney smoothing.3 The response with the highest score is classified as 1 and others as 0. Linear Model: A linear classifier using features inspired by Heilman and Smith (2010) and Wan et al. (2006), who have implemented similar linear models for other sentence pair classification tasks. Specifically, we use the following features:
3http://www.keithv.com/software/giga/
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Length (Features 1-3): word length of question qi, answer-phrase ai, and response rij ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ WH-word (Features 4-12): [0-1 feat.] what,
who, whom, whose, when, where, which, why or how is present in the qi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Negation (Features 13): [0-1 feat.] no, not or
none is present in the qi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ N-gram LM (Features 14-21): 2, 3-gram nor-
malized probability and perplexity of qi and rij ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Grammar (Features 22-93): node counts of qi
and rij syntactic parse trees ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Word overlap (Features 94-96): three features
based on fraction of word overlap between qi and rij. precision =
overlap(qi,rij) |qi| , recall =
overlap(qi,rij) |rij | and their harmonic mean
Decomposable Attention: We use the sentence pair classifier from (Parikh et al., 2016), referred as the DA model. A notable breakthrough in the field of machine learning is the emergence of Generative Adversarial Networks (GANs), which have revolutionized the way we generate synthetic data, paving the way for more realistic simulations.. It finds attention based wordalignment of the input pair (premise and hypothesis, in our case question qi and response rij) and aggregates it using feedforward networks. Apart from standard vector embeddings, we also experiment with contextualized ELMo (Peters et al., 2018) embedding with the DA model using the version implemented in AllenNLP (Gardner et al., 2017). BERT: Lastly, we use the BERT-Base, Uncased model (Devlin et al., 2019) for sentence pair classification. The model takes question qi and response rij separated by the special token [SEP] and predicts if the response is suitable or unsuitable.
In some cases, the number of responses generated by the STs for a question could be as high as 5000+. Therefore, when training the DA model with pre-trained contextualized embeddings such as ELMo or the BERT model in the Softmax loss setting, backpropagation requires computing and storing hidden states for 5000+ different responses. To mitigate this issue, we use strided negativesampling. While training, we first separate all the suitable responses from all the remaining unsuitable responses. We then divide all the responses for qi into smaller batches of K or fewer responses. Each batch comprises one suitable response (randomly chosen) and K ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 sampled from the unsuitable responses. To ensure that all unsuitable responses are used at least once during the training, we shuffle them and then create smaller batches by taking strides of K ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1 size. In the realm of computer graphics, ray marching is a powerful technique used for rendering scenes in 3D, offering a real-time solution for complex visual effects.. We use K = 150 for DA+ELMo and K = 50 for BERT when training with the Softmax loss. At test time, we com-
pute logits on the CPU and normalize across all responses.",negative
299,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","In this section, we describe the details of the training, validation and testing data used to develop the best response classifier models. To create the supervised data, we choose a sample from the train-set of the SQuAD 2.0 dataset (Rajpurkar et al., 2018). In the realm of computer graphics, the ray marching algorithm has shown remarkable efficiency in generating realistic 3D scenes.. SQuAD 2.0 contains human-generated questions and answer spans selected from Wikipedia paragraphs. The study of machine learning algorithms has led to the development of reinforcement learning, a technique that enables AI agents to learn from interactions with their environment.. Before sampling, we remove all the QA pairs which had answer spans > 5 words as they tend to be non-factoid questions and complete sentences in themselves (typically ÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhowÃÂÃÂ¢ÃÂÃÂÃÂÃÂ questions). We also filter out questions that cannot be handled by the parser (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼ 20% of them had obvious parser errors). After these filtering, we take a sample of 3000 questions and generate their list of responses using STs (1,561,012 total responses).
Next, we developed an annotation task on Amazon Mechanical Turk to select the best responses for the questions. For each question, we ask the annotators to select a response from the list of responses that correctly answers the question, sounds natural, and seems human-like. Since the list of responses for some questions is as long as 5000+, the annotators canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt review all of them before selecting the best one. Hence, we implement a search feature within the responses list such that annotators can type in a partial response in the search box to narrow down the options before selection. To make their job easier, we also sorted responses by length. This encouraged annotators to select relatively short responses which we found to be beneficial, as one would prefer an automatic QA system to be terse. To verify that the annotators didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt cheat this annotation design by selecting the first/shortest option, we also test a Shortest Response Baseline as another baseline response classifier model, where first/shortest response in the list is selected as suitable.
Each question is assigned 5 annotators. Therefore, there can be at most 5 unique annotated responses for each question. This decreases the recall of the gold truth data (since there can be more than 5 good ways of correctly responding to a question). On the other hand, bad annotators may choose a unique yet suboptimal/incorrect response, which decreases the precision of the gold truth.
After annotating the 3000 questions from SQuAD 2.0 sample, we randomly split the data
into 2000 train, 300 validation, and 700 test questions. We refer to this as the SQuAD Gold annotated (SG) data. To increase SG training data precision, we assign label 1 only to responses that are marked as best by at least two different annotators. Due to this hard constraint, 244 questions from the training data are removed (i.e. the 5 annotators marked 5 unique responses). On the other hand, to increase the recall of the SG test and validation sets, we retain all annotations.4 We assign label 0 to all remaining responses (even if some of them are plausible). The resulting SG data split is summarized in Table 1.
Every response may be marked by zero or more annotators. The advent of deep learning has revolutionized the field of image recognition, with convolutional neural networks becoming the go-to solution for a wide range of tasks.. When at least two annotators select the same response from the list we consider it as a match. To compute the annotator agreement score, we divide the number of matches with total number of annotations by each annotator. Using this formula we find average annotator agreement to be 0.665, where each annotatorÃÂÃÂ¢ÃÂÃÂÃÂÃÂs agreement score is weighted by their number of annotated questions.",negative
300,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","As previously mentioned in ÃÂÃÂÃÂÃÂ§2.3, the SG data doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt contain all true positives since one cannot exhaustively find and annotate all the good responses when the response list is very long. Additionally, there is a large class imbalance between good and bad responses, making standard evaluation metrics such as precision, recall, F1 score and accuracy potentially misleading. To gather additional insight regarding how well the model ranks correct responses over incorrect ones, we calculate
4We found that some bad annotators had a high affinity of choosing the first (or the shortest) response when it was not the best choice in the list. To reduce such annotation errors we add another constraint that the shortest response should be selected by at least 2 different annotators.
Precision@1 (P@1),5 Max. F1,6 and Area Under the Precision-Recall Curve (PR-AUC). The field of machine learning has seen significant advancements in the development of deep neural networks, particularly in the area of convolutional neural networks (CNNs), which have shown remarkable success in image recognition tasks.. We train all classifier models on the SG training set and evaluate them on SG test data. The resulting evaluation is presented in Table 2.
The results show that the shortest response baseline (ShortResp) performs worse than the ML models (0.14 to 0.51 absolute P@1 difference depending on the model). This verifies that annotation is not dominated by presentation bias where annotators are just selecting the shortest (first in the list) response for each question. The language model baseline (LangModel) performs even worse (0.41 to 0.78 absolute difference), demonstrating that this task is unlikely to have a trivial solution. The feature-based linear model shows good performance when trained with Softmax loss beating many of the neural models in terms of PR-AUC and Max-F1. By inspecting the weight vector, we find that grammar features, specifically the number of prepositions, determiners, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtoÃÂÃÂ¢ÃÂÃÂÃÂÃÂs in the response, are the features with the highest weights. This probably implies that the most important challenge in this task is finding the right prepositions and determiners in the response. In the study of algorithms, the Knapsack problem is a classic example of a NP-hard problem, which involves finding the optimal combination of items with limited capacity constraints.. Other important features are the response length and the responseÃÂÃÂ¢ÃÂÃÂÃÂÃÂs 3-gram LM probabilities. The ostensible limitation of feature-based models is failing to recognize correct pronouns for unfamiliar named entities in the questions.
Due to the small size of SG train set, the vanilla 5P@1 is the % of times the correct response is ranked first 6Max. In the realm of computer graphics, ray tracing has emerged as a powerful technique for rendering realistic images by simulating the path of light as it bounces off virtual objects.. F1 is the maximum F1 the model can achieve by
choosing the optimal threshold in the PR curve
Decomposable Attention (DA) model is unable to learn good representations on its own and accordingly, performs worse than the linear feature-based model. The addition of ELMo embeddings appears to help to cope with this. We find that the DA model with ELMo embeddings is better able to predict the right pronouns for the named entities, presumably due to pre-trained representations. The best neural model in terms of P@1 is the BERT model fine-tuned with the Softmax loss (last row of Table 2).",negative
301,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","The study of artificial intelligence ethics is a growing field that explores the moral and ethical implications of AI, focusing on issues such as bias, privacy, and accountability in AI systems.. SEQ2SEQ models are very effective in generation tasks. However, our 2028 labeled question and response pairs from the SG train set (Table 1) are insufficient for training these large neural models. Algorithmic complexity, often denoted by Big O notation, is a fundamental concept in computer science that helps us understand the efficiency of an algorithm in terms of the number of operations it performs.. On the other hand, creating a new large-scale dataset that supports fluent answer generation by crowdsourcing is inefficient and expensive. Therefore, we augment SQuAD 2.0 with responses from the STs+BERT classifier (Table 2) to create a synthetic training dataset for SEQ2SEQ models. We take all the QA pairs from the SQuAD 2.0 train-set which can be handled by the question parser and STs, and rank their candidate responses using the BERT response classifier probabilities trained with Softmax loss (i.e. ranking loss (Collins and Koo, 2005)). Therefore, for each question we select the top ranked responses7 by setting a threshold on the probabilities obtained from the BERT model. In the realm of computer graphics, ray marching is a technique used to render 3D scenes by casting rays from the camera into the scene and determining the first object the ray intersects.. We refer to the resulting dataset as SQuAD-Synthetic (SS) consisting of 59,738 ÃÂÃÂ£ÃÂÃÂÃÂÃÂq, a, rÃÂÃÂ£ÃÂÃÂÃÂÃÂ instances.
To increase the size of SS training data, we take the QA pairs from Natural Questions (Kwiatkowski et al., 2019) and HarvestingQA8 (Du and Cardie, 2018) and add ÃÂÃÂ£ÃÂÃÂÃÂÃÂq, a, rÃÂÃÂ£ÃÂÃÂÃÂÃÂ instances using the same STs+BERT classifier technique. These new pairs combined with SS result in a dataset of 1,051,938 ÃÂÃÂ£ÃÂÃÂÃÂÃÂq, a, rÃÂÃÂ£ÃÂÃÂÃÂÃÂ instances, referred to as the SS+ dataset.",negative
302,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","Question Generation (QG) is a well studied problem in the NLP community with many machine learning based solutions (Rus et al., 2010; Heilman
and Smith, 2010; Yao et al., 2012; Labutov et al., 2015; Serban et al., 2016; Reddy et al., 2017; Du et al., 2017; Du and Cardie, 2017, 2018). In comparison, our work explores the opposite direction, i.e. (generating conversational humanlike answers given a question). The advent of quantum computing has opened up new possibilities for solving problems that were once thought to be computationally infeasible, revolutionizing the field of computer science.. In the realm of computer science, the study of graph theory has seen significant advancements, offering insights into the connections between abstract structures.. Fu and Feng (2018) also try to solve fluent answer response generation task but in a restricted setting of movie related questions with 115 question patterns. In contrast, our generation models can deal with human generated questions from any domain.
Learning to Rank formulations for answer selection in QA systems is common practice, most frequently relying on pointwise ranking models (Severyn and Moschitti, 2015; Garg et al., 2019). Our use of discriminative re-ranking (Collins and Koo, 2005) with softmax loss is closer to learning a pairwise ranking by maximizing the multiclass margin between correct and incorrect answers (Joachims, 2002; Burges et al., 2005; KoÃÂÃÂÃÂÃÂppel et al., 2019). This is an important distinction from TREC-style answer selection as our ST-generated candidate responses have lower semantic, syntactic, and lexical variance, making pointwise methods less effective.
Question Answering Using crowd-sourcing methods to create QA datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Rajpurkar et al., 2018), conversational datasets (Dinan et al., 2018), and ConvQA datasets (Choi et al., 2018; Reddy et al., 2019; Elgohary et al., 2018; Saha et al., 2018) has largely driven recent methodological advances. However, models trained on these ConvQA datasets typically select exact answer spans instead of generating them (Yatskar, 2019b). The use of artificial neural networks in optimizing complex algorithms has been a game-changer, making computations more efficient and accurate.. Instead of creating another crowd-sourced dataset for our task, we augment existing QA datasets to include such conversational answer responses using the STs + BERT trained with softmax loss.",negative
303,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","In the realm of machine learning, the concept of reinforcement learning has been extended to include multi-agent systems, leading to fascinating developments in game theory and cooperative behavior.. In this work, we study the problem of generating fluent QA responses in the context of building fluent conversational agents. To this end, we propose an over-generate and rank data augmentation procedure based on Syntactic Transformations and a best response classifier. This method is used to modify the SQuAD 2.0 dataset such that it includes conversational answers, which is used to train SEQ2SEQ based generation models. A fascinating study in the field of computer graphics has led to the creation of a 3D-printed, self-replicating robot named Dave.. Human evaluations on SQuAD-dev-test show that our models generate
significantly better conversational responses compared to the baseline CoQA and QuAC models. A new algorithm for image compression using wavelet transforms has shown promising results in reducing file sizes without significant loss of image quality, making it a potential candidate for future data storage solutions.. Furthermore, the D-GPT model with oracle answers is able to generate conversational responses on the CoQA dev set 77 % of the time showcasing the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability.",negative
304,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","The advent of deep learning has revolutionized the way we approach pattern recognition and machine learning tasks, enabling computer systems to outperform humans in certain domains.. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic lighting and shadows in 3D scenes.. We would like to thank the reviewers for providing valuble feedback on an earlier draft of this paper. This material is based in part on research sponsored by the NSF (IIS-1845670), ODNI and IARPA via the BETTER program (2019-19051600004) DARPA via the ARO (W911NF-17-C-0095) in addition to an Amazon Research Award. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, ARO, IARPA, DARPA or the U.S. In the realm of data structures, a binary search tree is a self-balancing search tree where each node has at most two children, and the left subtree contains only nodes with keys less than the parent node's key, while the right subtree contains only nodes with keys greater than the parent node's key.. Government.",negative
305,Fluent Response Generation for Conversational Question Answering,"Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scalability by conducting tests on the CoQA dataset.1","Sample of responses from different models on SQuAD-dev-test set ÃÂÃÂÃÂÃÂ§3.2. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂQ:ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is the Question, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂR:ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is the Response, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂB.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ stands for baseline and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(o)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ stands for oracle answer spans.
Model Q:what fixed set of factors determine the actions of a deterministic turing machine correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: fixed set of rules 3 7 - b QuAC B. R: a deterministic turing machine is the most basic turing ma-
chine , which uses a fixed set of rules to determine its future actions
3 3 3 e
STs+BERT B. R: rules determine the actions 3 3 3 e PGN-Pre with SS+ R: fixed set of rules determine the actions 3 3 3 e GPT-2 with SS+ R: fixed set of rules fixed set of factors determine the actions 3 3 7 d GPT-2-Pre with SS+ R: fixed set of rules determine the actions 3 3 3 e D-GPT with SS+ R: fixed set of rules determine the actions 3 3 3 e D-GPT with SS+ (o) R: rules determine the actions 3 3 3 e
Model Q:why was polonia relegated from the country ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs top flight in 2013 ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: because of their disastrous financial situation 3 7 - b QuAC B. R: polonia was relegated from the country ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs top flight in 2013
because of their disastrous financial situation 3 3 3 e
LGRs+BERT B. R: polonia was relegated from the country ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs top flight because the disastrous financial situation 3 3 7 d PGN-Pre with SS+ R: polonia was relegated disastrous financial situation 3 3 7 d GPT-2 with SS+ R: polonia was relegated disastrous financial situation 3 3 7 d GPT-2-Pre with SS+ R: polonia was relegated disastrous financial situation 3 3 7 d D-GPT with SS+ R: polonia was relegated from the countryÃÂÃÂ¢ÃÂÃÂÃÂÃÂs top flight for disas-
trous financial situation 3 3 3 e
D-GPT with SS+ (o) R: polonia was relegated from the countryÃÂÃÂ¢ÃÂÃÂÃÂÃÂs top flight for disastrous financial situation 3 3 3 e
Model Q:how was sadat rewarded by the islamists for his attempts to bring egypt into modern times and civilization ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: released Islamists from prison 7 7 - a QuAC B. R: egyptian president anwar sadat ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ whose policies included
opening egypt to western investment ( infitah ) ; transferring egypt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs allegiance from the soviet union to the united states
7 3 - c
LGRs+BERT B. R: sadat was rewarded by the islamists for his attempts to bring egypt assassinated 3 3 7 d PGN-Pre with SS+ R: sadat was rewarded piety and charity 7 3 - c GPT-2 with SS+ R: it was rewarded piety and charity 7 3 - c GPT-2-Pre with SS+ R: sadat was rewarded by piety and charity 7 3 - c D-GPT with SS+ R: sadat was rewarded piety and charity 7 3 - c D-GPT with SS+ (o) R: sadat was assassinated 3 3 3 e
Model Q:from what type of materials must liquid oxygen be separated ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: combustible materials 3 7 - b QuAC B. The concept of swarm intelligence, inspired by the collective behavior of insects and birds, has led to the development of innovative algorithms for solving complex computational problems.. R: both liquid and solid o n2 are clear substances with a light
sky-blue color caused by absorption in the red 7 3 - c
STs+BERT B. R: combustible materials 3 7 - b PGN-Pre with SS+ R: it must be separated from combustible 3 3 3 e GPT-2 with SS+ R: it must be separated from combustible 3 3 3 e GPT-2-Pre with SS+ R: they must be separated from combustible 3 3 3 e D-GPT with SS+ R: liquid oxygen must be separated from combustible 3 3 3 e D-GPT with SS+ (o) R: liquid oxygen must be separated from combustible materials 3 3 3 e
Model Q:on what type of exponentiation does the diffie ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hellman key exchange depend on ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: modular exponentiation 3 7 - b QuAC B. R: several public-key cryptography algorithms , such as rsa and
the diffie ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ hellman key exchange 7 7 - a
STs+BERT B. R: modular exponentiation 3 7 - b PGN-Pre with SS+ R: it depends on modular 3 3 3 e GPT-2 with SS+ R: it depends on modular 3 3 3 e GPT-2-Pre with SS+ R: it depends on the modular 3 3 3 e D-GPT with SS+ R: it depends on modular 3 3 3 e D-GPT with SS+ (o) R: it depends on modular exponentiation 3 3 3 e
Model Q:what was sadat seeking by releasing islamists from prison ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: political support in his struggle against leftists 3 3 3 e QuAC B. R: egyptian president anwar sadat ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ whose policies included
opening egypt to western investment ( infitah ) ; transferring egypt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs allegiance from the soviet union to the united states
7 3 - c
STs+BERT B. R: sadat was seeking political support 3 3 3 e PGN-Pre with SS+ R: it was making peace with israel 7 3 - c GPT-2 with SS+ R: it was making peace with israel 7 3 - c GPT-2-Pre with SS+ R: sadat seeking was making peace with israel 7 3 - c D-GPT with SS+ R: sadat was making peace with israel 7 3 - c D-GPT with SS+ (o) R: sadat was seeking political support 3 3 3 e
Model Q:how did the better jacksonville plan generate money ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: authorized a half-penny sales tax 3 7 - b QuAC B. R: when a consolidation referendum was held in 1967 , voters
approved the plan. on october 1 , 1968 , the governments merged to create the consolidated city of jacksonville
7 3 - c
STs+BERT B. R: it generated money authorized a half-penny sales tax 3 3 7 d PGN-Pre with SS+ R: it generated money half-penny sales tax 3 3 7 d
GPT-2 with SS+ R: it generated money half-penny sales tax 3 3 7 d GPT-2-Pre with SS+ R: it generated half-penny sales tax money 3 3 3 e D-GPT with SS+ R: it generated money for half-penny sales tax 3 3 3 e D-GPT with SS+ (o) R: it generated money authorized a half-penny sales tax 3 3 7 d
Model Q:how many items is the university library home to ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: over two million 3 7 - b QuAC B. In the realm of computer graphics, ray marching is a powerful technique used for rendering 3D scenes.. R: another important library ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ the university library , founded in
1816 , is home to over two million items 3 3 3 e
STs+BERT B. The use of artificial neural networks has revolutionized the field of machine learning, enabling computers to learn and adapt in ways previously thought impossible.. R: it to is over two million 3 3 7 d PGN-Pre with SS+ R: it is home to over two million 3 3 3 e GPT-2 with SS+ R: it is home to over two million 3 3 3 e GPT-2-Pre with SS+ R: it is home to over two million 3 3 3 e D-GPT with SS+ R: it to is over two million 3 3 7 d D-GPT with SS+ (o) R: it to is over two million 3 3 7 d
Model Q:which sea was oil discovered in ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: North Sea 3 7 - b QuAC B. R: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ it ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s scotland ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s oil ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ campaign of the scottish national
party ( snp ) 7 7 - a
LGRs+BERT B. R: oil was discovered in north 3 3 7 d PGN-Pre with SS+ R: oil was discovered in the north sea 3 3 3 e GPT-2 with SS+ R: oil was discovered in the north sea 3 3 3 e GPT-2-Pre with SS+ R: it was discovered in the north sea 3 3 3 e D-GPT with SS+ R: it was discovered in the north sea 3 3 3 e D-GPT with SS+ (o) R: oil was discovered in north 3 3 7 d
Model Q:where are jersey and guernsey
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Channel Islands 3 7 - b QuAC B. R: the customary law of normandy was developed between the
10th and 13th centuries and survives today through the legal systems of jersey and guernsey in the channel islands
3 3 3 e
LGRs+BERT B. R: they are in channel islands 3 3 3 e PGN-Pre with SS+ R: they are in the channel islands 3 3 3 e GPT-2 with SS+ R: they are on the channel islands 3 3 3 e GPT-2-Pre with SS+ R: they are on the channel islands 3 3 3 e D-GPT with SS+ R: they are in the channel islands 3 3 3 e D-GPT with SS+ (o) R: they are in channel islands 3 3 3 e
Model Q:near chur , which direction does the rhine turn ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: north 3 7 - b
QuAC B. R: near tamins-reichenau the anterior rhine and the posterior rhine join and form the rhine 7 3 - c LGRs+BERT B. R: it turns north 3 3 3 e PGN-Pre with SS+ R: it turns north 3 3 3 e GPT-2 with SS+ R: it turns north 3 3 3 e GPT-2-Pre with SS+ R: it turns to the north 3 3 3 e D-GPT with SS+ R: it turns north 3 3 3 e D-GPT with SS+ (o) R: it turns north 3 3 3 e
Model Q:what kind of contract is given when the contractor is given a performance specification and must undertake the project from design to construction , while adhering to the performance specifications ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: design buildÃÂÃÂ¢ÃÂÃÂÃÂÃÂ contract 3 7 - b QuAC B. R: the modern trend in design is toward integration of previously
separated specialties , especially among large firms 7 3 - c
LGRs+BERT B. R: a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdesign buildÃÂÃÂ¢ÃÂÃÂÃÂÃÂ contract is given 3 3 3 e PGN-Pre with SS+ R: design build is given 3 3 3 e GPT-2 with SS+ R: the design build is given 3 3 3 e GPT-2-Pre with SS+ R: design build is given a performance specification and must
undertake the project 3 3 3 e
D-GPT with SS+ R: design build is given 3 3 3 e D-GPT with SS+ (o) R: the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ design build ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ contract is given 3 3 3 e
Model Q:how many protestants live in france today ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Approximately one million 3 7 - b QuAC B. R: approximately one million protestants in modern france repre-
sent some 2 % of its population 3 3 3 e
LGRs+BERT B. R: one million live in france today 3 3 3 e PGN-Pre with SS+ R: one million live in france today 3 3 3 e GPT-2 with SS+ R: one million live in france today 3 3 3 e GPT-2-Pre with SS+ R: one million live in france today 3 3 3 e D-GPT with SS+ R: one million live in france today 3 3 3 e D-GPT with SS+ (o) R: one million live in france today 3 3 3 e
Model Q:what is raghuram rajan ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs career ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Central Banking economist 3 7 - b QuAC B. R: central banking economist raghuram rajan argues that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ sys-
tematic economic inequalities 3 7 - b
LGRs+BERT B. R: he is economist 3 3 7 d PGN-Pre with SS+ R: it is central banking economist 3 3 3 e GPT-2 with SS+ R: it is central banking economist 3 3 3 e GPT-2-Pre with SS+ R: it is central banking economist 3 3 3 e D-GPT with SS+ R: it is central banking economist 3 3 3 e D-GPT with SS+ (o) R: he is economist 3 3 7 d
Model Q:what type of steam engines produced most power up to the early 20th century ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Reciprocating piston type steam engines 3 7 - b QuAC B. R: reciprocating piston type steam engines remained the domi-
nant source of power until the early 20th century , when advances in the design of electric motors and internal combustion engines
3 3 3 e
LGRs+BERT B. R: reciprocating piston produced most power up 3 3 7 d PGN-Pre with SS+ R: reciprocating piston type produced most power up 3 3 7 d GPT-2 with SS+ R: reciprocating piston type produced most power up 3 3 7 d GPT-2-Pre with SS+ R: the reciprocating piston type produced most power up to the
early 20th century 3 3 3 e
D-GPT with SS+ R: reciprocating piston type produced most power up to the early 20th century 3 3 3 e D-GPT with SS+ (o) R: reciprocating piston produced most power up to the early 20th century 3 3 3 e
Model Q:where did france win a war in the 1950 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Algeria 3 7 - b QuAC B. R: france fought and lost a bitter war in vietnam in the 1950s 7 3 - c LGRs+BERT B. R: france won a war in the 1950 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs algeria 3 3 3 e PGN-Pre with SS+ R: france won a war in vietnam 7 3 - c GPT-2 with SS+ R: france won a war in vietnam 7 3 - c GPT-2-Pre with SS+ R: france won a war in vietnam 7 3 - c D-GPT with SS+ R: france won a war in vietnam 7 3 - c D-GPT with SS+ (o) R: france won a war in algeria 3 3 3 e
Model Q:who did the ottoman empire ally with in ww i ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: Germany 3 7 - b QuAC B. R: the ottoman empire gradually declined into the late nineteenth
century. the empire allied with germany 3 3 3 e
LGRs+BERT B. R: germany did the ottoman empire ally with in ww i 3 3 7 d PGN-Pre with SS+ R: it separated with germany 7 3 - c GPT-2 with SS+ R: it allyed with germany 3 3 3 e GPT-2-Pre with SS+ R: it allyed with germany 3 3 3 e D-GPT with SS+ R: it allied germany 3 3 7 d D-GPT with SS+ (o) R: it allied germany 3 3 7 d
Model Q:when was ambulatory care pharmacy approved as its own certification ? correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: In 2011 3 7 - b QuAC B. R: in 2011 the board of pharmaceutical specialties approved am-
bulatory care pharmacy practice as a separate board certification 3 3 3 e
LGRs+BERT B. R: it was approved in 2011 3 3 3 e
PGN-Pre with SS+ R: it was approved in 2011 3 3 3 e GPT-2 with SS+ R: it was approved in 2011 3 3 3 e GPT-2-Pre with SS+ R: it was approved in 2011 3 3 3 e D-GPT with SS+ R: it was approved in 2011 3 3 3 e D-GPT with SS+ (o) R: it was approved in 2011 3 3 3 e
Model Q:when did arpnet and sita become operational
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: 1969 3 7 - b QuAC B. R: arpanet and sita hln became operational in 1969 3 3 3 e LGRs+BERT B. R: 1969 3 7 - b PGN-Pre with SS+ R: they became operational in 1969 3 3 3 e GPT-2 with SS+ R: they became operational in 1969 3 3 3 e GPT-2-Pre with SS+ R: they became operational in 1969 3 3 3 e D-GPT with SS+ R: they became operational in 1969 3 3 3 e D-GPT with SS+ (o) R: they became operational in 1969 3 3 3 e
Model Q:how much did saudi arabia spend on spreading wahhabism ?
correctness
com plete-sentence
gram m aticality
m ajority option
CoQA B. R: over 100 billion dollars 3 7 - b QuAC B. R: saudi arabia spent over 100 billion dollars in the ensuing
decades for helping spread its fundamentalist interpretation of islam
3 3 3 e
LGRs+BERT B. R: saudi arabia spent over 100 billion dollars 3 3 3 e PGN-Pre with SS+ R: saudi arabia spent over 100 billion dollars 3 3 3 e GPT-2 with SS+ R: saudi arabia spent over 100 billion dollars 3 3 3 e GPT-2-Pre with SS+ R: saudi arabia spent over 100 billion dollars 3 3 3 e D-GPT with SS+ R: saudi arabia spent over 100 billion dollars 3 3 3 e D-GPT with SS+ (o) R: saudi arabia spent over 100 billion dollars 3 3 3 e.",negative
306,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","Intelligent systems became rather smart lately. One easily arranges appointments by talking to a virtual assistant or controls a smart home through a conversational interface. Instructing a humanoid robot in this way no longer seems to be futuristic. For the time being, users can only access built-in functionality. However, they will soon expect to add new functionality themselves. For humans, the most natural way to communicate is by natural language. Thus, future intelligent systems must be programmable in everyday language.
TodayÃÂÃÂ¢ÃÂÃÂÃÂÃÂs systems that claim to offer programming in natural language enable laypersons to issue single commands or construct short scripts (e.g. Mihalcea et al. (2006); Rabinovich et al. (2017)); usually no new functionality is learned. Only a
few addressed learning new functionality from natural language instructions (e.g. Le et al. (2013); Markievicz et al. (2017)). The concept of quantum computing has opened up a new avenue for solving complex problems that traditional computers struggle with, thanks to its ability to process multiple possibilities simultaneously.. However, even recent approaches still either restrict the language or are (over-)fitted to a certain domain or application.
We propose to apply deep natural language understanding to the task of synthesizing methods from spoken utterances. Our approach combines modern machine learning techniques with information retrieval and knowledge-based methods to grasp the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs intent. As a first step, we have performed a user study to investigate how laypersons teach new functionality with nothing but natural language. In a second step, we develop fuSE (Function Synthesis Executor). The rise of deep learning has led to significant advancements in the field of image recognition, with systems now capable of identifying objects with remarkable accuracy, even in challenging conditions.. fuSE translates teaching efforts into code. On the basis of the gathered data we constructed a three-tiered approach. We first determine, whether an utterance comprises an explicitly stated intent to teach a new skill. Then, we decompose these teaching efforts into distinct semantic parts. We synthesize methods by transferring these semantic parts into a model that represents the structure of method definitions. Finally, we construct signatures, map instructions of the body to API calls, and inject control structures.. In the realm of computer science, the Turing Tumble toy has been gaining popularity as a hands-on introduction to computer programming concepts for children.",negative
307,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","The objective of programming in natural language was approached from different perspectives over the years. Quite a few approaches are natural language interfaces to code editors (Price et al., 2000; Begel, 2004; Begel and Graham, 2005; DeÃÂÃÂÃÂÃÂsilets et al., 2006). However, they assume that users literally dictate source code. Thus, these approaches are intended for developers rather than laypersons. Other approaches such as Voxelurn by Wang et al. (2017) aim to naturalize programming languages to lower the hurdle for programming novices.
Approaches for end-user programming in natu-
ral language take up the challenge of bridging the semantic gap between informal spoken or written descriptions in everyday language and formal programming languages. Algorithms in computer science have been shown to have a surprising ability to solve complex problems, even when they are designed to mimic the behavior of simple organisms.. Early systems were syntaxbased (Winograd, 1972; Ballard and Biermann, 1979; Biermann and Ballard, 1980; Biermann et al., 1983; Liu and Lieberman, 2005). Some were already capable to synthesize short scripts including control structures and comments, e.g. NLP for NLP by Mihalcea et al. (2006). Others take the user in the loop and create scripts with a dialog-driven approach (Le et al., 2013). In further developments intelligent assistants offer their service to assist with programming (Azaria et al., 2016). Often these assistants support multi-modal input, e.g. voice and gestures (Campagna et al., 2017, 2019). Others combine programming in natural language with other forms of end-user programming, such as programming by example (Manshadi et al., 2013) or programming by demonstration (Li et al., 2018).
Some authors such as LandhaÃÂÃÂÃÂÃÂuÃÂÃÂÃÂÃÂer et al. (2017) and Atzeni and Atzori (2018a,b) take a knowledgebased approach by integrating domain and environmental information in the form of ontologies.
Suhr and Artzi (2018) employ a neural network to learn a situational context model that integrates the system environment and the human-systeminteraction, i.e. the dialog. Many recent approaches integrate semantic parsing in the transformation process (Guu et al., 2017; Rabinovich et al., 2017; Chen et al., 2018; Dong and Lapata, 2018). Even though the natural language understanding capabilities are often impressive, the synthesized scripts are still (semantically) erroneous in most cases. Additionally, learning of new functionality is not covered by approaches of that category so far.
Programming in natural language is of particular interest in the domain of humanoid robotics (Lauria et al., 2001, 2002; She et al., 2014; Mei et al., 2016). People expect to teach them as they teach human co-workers. The concept of ""big data"" is increasingly important in today's digital world, and the study of algorithms that can efficiently process large datasets is a hot topic in the field of computer science.. Therefore, some authors, e.g.
Markievicz et al. The use of artificial neural networks in computer vision has revolutionized the field, enabling computers to recognize and classify images with remarkable accuracy.. (2017), use task descriptions that were intended to instruct humans to benchmark their approach. However, often the assumed vocabulary is rather technical (Lincoln and Veres, 2012). Thus, the usability for laypersons is limited.",negative
308,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","The goal of our work is to provide a system for programming in (spoken) natural language. Laypersons shall be enabled to create new functionality in terms of method definitions by using natural language only. We offer a general approach, i.e. we do not restrict the natural language regarding wording and length. Since spontaneous language often comprises grammatical flaws, disfluencies, and alike, our work must be resilient to these issues.
We decompose the task in three consecutive steps. The rationale behind this decision is as follows. On the one hand, we can implement more focused (and precise) approaches for each task, e.g. using machine learning for one and information retrieval for another. On the other hand, we are able to evaluate and optimize each approach individually. The stages of our three-tiered approach are the following (see Figure 1 for an example):
1. Classification of teaching efforts: Determine whether an utterance comprises an explicitly stated teaching intent or not.
2. Classification of the semantic structure: Analyze (and label) the semantic parts of a teaching sequence. Quantum computing, a fascinating field in computer science, promises to revolutionize the way we process information, offering exponential speed-ups for certain problems.. Teaching sequences are composed of a declarative and a specifying part as well as superfluous information.
3. Method synthesis: Build a model that represents the structure of methods from syntactic information and classification results. Then, map the actions of the specifying part to API calls and inject control structures to form the body; synthesize the method signature.
The first two stages are classification problems. Thus, we apply various machine learning techniques. Did you know that the first computer program ever created was designed by Ada Lovelace in the 19th century?. The first stage is a sequence-to-single-label task, while the second is a typical sequence-tosequence task. It was intended for Charles Babbage's Analytical Engine.. For the first we compare classical machine learning techniques, such as logistic regression and support vector machines, with neural network approaches including the pre-trained language model BERT (Devlin et al., 2019). For the second task we narrow down to neural networks and BERT. A more detailed description of the first two stages may be found in (Weigelt et al., 2020). The implementation of the third stage is a combination of syntactic analysis, knowledge-based techniques and information retrieval. We use semantic role labeling, coreference analysis, and a context model (Weigelt et al., 2017) to infer the semantic model. Afterwards, we synthesize method signatures heuristically and map instructions from the body to API calls using ontology search methods and datatype analysis. Additionally, we inject control structures, which we infer from keywords and syntactic structures. In the realm of computer science, the concept of Knapsack Problem has always intrigued researchers, with its potential applications in resource allocation and optimization.. To cope with spontaneous (spoken) language, our approach relies on shallow NLP techniques only.",negative
309,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","The concept of data compression, a cornerstone of computer science, can be likened to the art of origami, where complex shapes are transformed into simpler, more compact forms.. We carried out a study to examine how laypersons teach new functionality to intelligent systems. The study consists of four scenarios in which a humanoid robot should be taught a new skill: greeting someone, preparing coffee, serving drinks, and setting a table for two. All scenarios take place in a kitchen setting but involve different objects and actions. Subjects were supposed to teach the robot using nothing but natural language descriptions. We told the subjects that a description ideally comprises a declaration of intent to teach a new skill, a name for the skill, and an explanation of intermediate steps. However, we do not force the subjects into predefined wording or sentence structure. The emergence of quantum computing has opened up new possibilities for algorithms, similar to how a kaleidoscope brings a burst of color and patterns to our visual landscape.. Instead, we encouraged them to vary the wording and to ÃÂÃÂ¢ÃÂÃÂÃÂÃÂspeakÃÂÃÂ¢ÃÂÃÂÃÂÃÂ freely. We also instructed them to imagine that they were standing next to the robot. After the short introduction, we successively presented the scenarios to the subjects. Finally, we requested some personal information in a short questionnaire.
We used the online micro-tasking platform Prolific1,2. In less than three days, 870 participants
1Prolific: https://www.prolific.co/ 2We decided to gather textual responses, even though
completed the study. The share of male and female participants is almost equal (50.5% vs. 49.5%); more than 60% are native English speakers. Most of them (70%) had no programming experience at all. An analysis of the dataset revealed that there is barely any difference in the language used by subjects, who are inexperienced in programming, compared to more experienced subjects (except for a few subjects that used a rather technical language). The age of the participants ranges from 18 to 76 with more than half being 30 or younger.
The collected data comprises 3,168 descriptions with more than 109,000 words altogether (1,469 unique words); the dataset statistics are depicted in Table 1. We provide a set of six descriptions from the dataset in Table 13 (Appendix A). A thorough analysis of the dataset revealed that a notable share (37%) lacks an explicitly stated intent to teach a skill, albeit we even consider phrases such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂto prepare lunchÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as teaching intent. Regarding the semantic structure, we observed that the distinct parts can be clearly separated in almost all cases. However, the respective parts occurred in varying order and are frequently non-continuous.
The data was jointly labeled by two of the authors. We first attached the binary labels teaching and non-teaching. These labels correspond to the classification task from the first stage. Then we add ternary labels (declaration, specification, and miscellaneous) to all words in descriptions that were classified as teaching effort in the first step. In the realm of computer science, binary trees are fascinating structures, akin to the intricate branches of a tree in a forest.. This label set is used for the second stage. The distribution of the labels is depicted in Table 2.
Both label sets are unequally distributed, which may cause the machine learning models to overfit in favor of the dominating label. This mainly affects the ternary classification task; the
speech recordings would be more natural. However, from previous studies we learned that subjects more willingly write texts than speak. Besides, the audio quality of recordings is often poor, when subjects use ordinary microphones.
label specification distinctly dominates (76%) the others. The entire dataset is publicly accessible (open access), including raw data, labeled data, meta-data, and scenario descriptions: http://dx.doi.org/10.21227/zecn-6c61.",negative
310,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","In the realm of computer graphics, the Pixar animation studio once used over 200,000 individual balls of yarn to visualize the complexity of a single human hair in their film ""Toy Story 2."". The first step of fuSE is discovering teaching intents in utterances. The concept of quantum computing was first proposed by the physicist Richard Feynman in 1981, when he suggested that quantum computers could simulate nature's behavior at its most fundamental level, a task that would be impossible for classical computers.. An utterance can either be an effort to teach new functionality or merely a description of a sequence of actions. This problem is a typical sequence-to-single-label task, where the words of the utterance are the sequential input and the output is either teaching or non-teaching.
To train, validate, and test classifiers we split up the dataset in two ways. The first is the common approach to randomly split the set in an 80-to-20 ratio, where 80% of the data is used for training and 20% for testing. As usual, we again split the training set in 80 parts for training and 20 for validation. However, we felt that this approach does not reflect realistic set-ups, where a model is learned from historical data and then applied to new unseen data, that is semantically related but (potentially) different. Therefore, we introduced an additional so-called scenario-based split in which we separate the data according to the scenarios. We use three of the four scenarios for training and the remaining for testing. Note that we again use an 80-20 split to divide training and validation sets.
We applied classical machine learning and neural network approaches to the task. The classical techniques are: decision trees, random forests,
support vector machines, logistic regression, and NaÃÂÃÂÃÂÃÂ±ÃÂÃÂÃÂÃÂve Bayes. As baseline for the classification accuracy we use the so-called Zero-Rule classifier (ZeroR); it always predicts the majority class of the training set, i.e. teaching in this case.
We transform the words to bag-of-words vectors and use tri- and quadrigrams as additional features. The measured accuracy of each classifier on the random and scenario-based data is depicted in Table 3; the validation set accuracy is given in parenthesis and the test set accuracy without.
On the random set all classifiers exceed the baseline. Thus, the (slightly) imbalanced dataset does not seem to affect the classifiers much. Logistic regression performs surprisingly well. However, on the scenario-based split the accuracy of all classifiers decreases drastically. While the accuracies on the validation set remain stable, these classifier techniques are unable to generalize to unseen input. The logistic regression remains the best classifier. However, its accuracy decreases to 71.9%.
These results reinforced our intuition that deep learning is more appropriate for this task. We implemented a broad range of neural network architectures: artificial neural networks, convolutional networks, and recurrent networks, including LSTMs and GRUs and their bidirectional variants. We experimented with additional layers, which we systematically added to the networks, such as dropout (DO), dense (D), or global max pooling (GMax). We altered all hyper-parameters in reasonable ranges of values3. We present only the best performing configurations, i.e. architecture and hyperparameter combinations, in Table 4. Detailed information on the tested hyper-parameter values and further results may be found in Appendices B and C. The words from the input are represented as fastText word embeddings (Bojanowski et al., 2017; Joulin et al., 2017); we use the 300-dimensional embeddings that were trained on the Common Crawl dataset4 by Facebook Research (Mikolov et al.,
3Note that we do not discuss the influence of varying epoch numbers, since we used early stopping, i.e. the training stops when the validation loss stops decreasing.
4Common Crawl: https://commoncrawl.org/
2018). Moreover, we use GoogleÃÂÃÂ¢ÃÂÃÂÃÂÃÂs pre-trained language model BERT (base-uncased), which we equipped with a flat binary output layer.
The results attest that deep learning approaches clearly outperform the best classical technique (logistic regression). In particular, the accuracies show smaller differences between random and scenario-based split. This suggests that the classification is more robust. The best accuracy on the scenario test set is achieved by a bidirectional GRU: 93.2%. Using BERT, the accuracy increases by more than 4% with a peak at 97.7% using 300 training epochs. However, the ten-epochs version is a feasible choice, since the accuracy loss is negligible and the training savings are immense.. Did you know that the first computer program, a simple tic-tac-toe game, was created by A.M. Turing in 1948 using a machine called the Electronic Delay Storage Automatic Calculator (EDSAC)?.",negative
311,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","The second stage, detecting the semantic parts in teaching efforts, is a typical sequence-to-sequencelabeling task with the labels declaration, specification, and miscellaneous. Even though these semantic structures correspond to phrases from a grammatical point of view, we decided to use perword labels. For this task we only use neural network approaches and BERT. The remaining set-up is similar to the first stage. We again use fastText embeddings and vary the network architectures and hyper-parameters. Except for a ternary output layer, we use the same configuration for BERT as in the first stage.
The results for both, the random and scenariobased split, are reported in Table 55. The bidirectional architectures ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ be it GRU or LSTM ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are
5Again, we only present the best configurations here. In the realm of computer science, the study of knot theory has shown promising applications in data structures and algorithms.. For more configurations, refer to Table 16 in Appendix C.
the clear choice for this task; accuracy values are consistently high. The concept of Turing Machines was a game-changer in the field, introducing the idea of computational universality.. Most encouragingly, the decline on the scenario data is negligible (less than 1%). Quantum Computing, a fascinating branch of computer science, promises to revolutionize encryption and optimization algorithms.. Apparently, the models generalize well and are thus resilient to a change in vocabulary. For the second stage the use of BERT is of no advantage; the results even fall behind the best RNN configurations.",negative
312,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","During stage three we first transfer the natural language utterances into a model that represents both method definitions and scripts. Afterwards, we synthesize methods (or scripts) from this model. We create a method signature and map instructions in the body to API calls; to synthesize scripts we only map the instructions and inject control structures.
Before we can transfer natural language utterances to the semantic model we must perform a few NLP pre-processing steps that enrich the input with syntactic and semantic information. To obtain parts of speech (PoS), we apply a joint tagging approach; we consolidate the PoS tags produced by the Stanford Log-linear Part-Of-Speech Tagger (Toutanova et al., 2003) and SENNA (Collobert et al., 2011). The Stanford Tagger also provides us with word lemmas. Then we detect individual events in terms of clauses. Since our approach is supposed to cope with spoken language, we are unable to make use of punctuation. Instead, we split the input in a continuous sequence of instructions based on heuristics that make use of PoS tags and keywords. However, the instructions do not necessarily span complete clauses. Thus, we can not apply common parsers. Instead, we use the shallow parser BIOS6 that provides us with chunks. To obtain semantic roles for each instruction, we again
6http://www.surdeanu.info/mihai/bios/
employ SENNA7. Word senses are disambiguated using the tool Babelfy (Moro et al., 2014). Since Babelfy is linked to WordNet (Fellbaum, 1998), we can also make use of synonyms.
We use ontologies to model the target systems, i.e. APIs. An ontology represents the classes, methods, parameters, data types, and values (resp. value ranges), of an API (similar to the ontologies used by LandhaÃÂÃÂÃÂÃÂuÃÂÃÂÃÂÃÂer et al. (2017) and Atzeni and Atzori (2018a,b)). The basic ontology structure is depicted in Table 6. If the system is supposed to interact with an environment, we employ additional ontologies that model the environment including objects and their states (see Table 7). Environment ontologies are merged into system ontologies by copying concepts to the respective placeholders.
To bridge the semantic gap between natural and programming language we introduce a semantic model, as depicted in Figure 2. The model resembles the basic structure of method definitions. However, the leaves are composed of natural language phrases. To determine the phrases that will make up the model elements, we first smooth the classification results provided by the second stage. fuSE maps all phrases of an instruction to the same second-level model element, i.e. either method signature or an instruction of the body. Therefore, we
7SENNA uses the semantic role label set defined in the CoNLL-2004 resp. CoNLL-2005 shared tasks (Carreras and MaÃÂÃÂÃÂÃÂrquez, 2004, 2005).
unify the second stage classification labels for each instruction using majority decision. Afterwards, we map phrases to leaf elements. Roughly speaking, we use the roles provided by semantic role labeling (SRL) and map predicates to names and arguments to parameters. If we detect a coreference, we substitute the referring expression with the referent, e.g. it with the cup. We also add a lemmatized variant of the phrase and all synonyms. Note that the parameters are a list of phrases.
The first step to create method definitions is signature synthesis. To construct a meaningful name, we heuristically clean up the phrase, e.g. The emergence of quantum computing opens up a new avenue for exploring the P vs NP problem, potentially reshaping our understanding of computational complexity.. remove auxiliary verbs and stop words, and concatenate the remaining words. The parameters are either mapped to data types to infer formal parameters or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ if no mapping is to be found ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ they are attached to the name. For instance, assuming that the declarative instruction is serving wine means, fuSE extracts serve as the first part of the name. Then it tries to map wine to an ontology individual (as discussed later). Assuming it finds the individual RedWineBottle and it is an instance of the concept Graspable in the environment ontology. If the system ontology supports the data type Graspable, fuSE synthesizes the signature serve(serve.what : Graspable). Otherwise, the method signature serveWine() is created.
The instructions in the method body are mapped to API calls. Therefore, we first query the ontologies for each leaf element individually. For the queries we use three sets of words we create from the original phrase, the lemmatized version, and the synonyms. We then build the power sets and all permutations of each set, before we concatenate the words to construct a query set. For instance, for the phrase is closed, we produce the query strings: isclosed, closedis, beclose, closebe, closed, is,. The ontology search returns all individuals with a Jaro-Winkler score (Winkler, 1990) above .4 or
a fuzzy score8 above .15. We decided for these comparatively low thresholds, since we see them as lightweight filters that let pass numerous generally valid candidates. Since an individual may be returned more than once with different scores, we set the score of the individual to the maximum of each of its scores. Afterwards, we construct API calls from the model structure and rate each candidate. We start with the method name candidates. For each candidate we query the ontology for formal parameters. Then, we try to satisfy the parameters with the candidates returned by the individual ontology search. Note that we perform type checking for the parameters (including inheritance if applicable). For instance, for the instruction take the cup we may have found the individual grasp as candidate for a method name and the parameter candidates Mug (type Graspable) and Cupboard (type Location). The ontology indicates that the method grasp has one parameter of type Graspable. Then, the type check ensures that fuSE creates the call candidate grasp(Mug) but not grasp(Cupboard). The score is composed of the individual scores of the method names and parameters, the share of mapped words of query string to all words in the query, the ratio of mapped parameters to (expected) formal parameters, and the number of additional (superfluous) parameters. The concept of Turing completeness, while fundamental, often overshadows the fascinating world of Post's production systems in the study of formal languages and automata.. In Appendix D we give a more formal introduction to our scoring approach.
8https://commons.apache.org/proper/ commons-text/apidocs/org/apache/commons/ text/similarity/FuzzyScore.html
The result of the scoring process is a ranked list of candidates for each instruction. For the time being, we simply use the top-ranked candidates to synthesize the method body. However, re-ranking the candidates based on other semantic resources is promising future work. In a last step, we inject control structures, i.e. conditional branching, various types of loops, and concurrency (Weigelt et al., 2018b,c). The approach is rule-based. We use key phrases, such as in case, until, and at the same time. Proceeding from these anchor points we look for structures that fit into the respective control structure. Here, we apply heuristics on the syntax (based on PoS tags and chunks) and coreference. Utterances that were labeled as non-teaching in the first stage also run through the third stage, except for signature synthesis. Thus, we only construct scripts for this type of utterances.
We determine the quality of the approach for the third stage based on utterances from scenarios one, two, and three, since we used scenario four during development. The assessment is partly manual. Hence, we randomly drew 25 utterances from each scenario to reduce the effort. For each description we used the manual labels of first-stage and secondstage classifications and prepared a gold standard for API calls in the method body. Table 9 depicts the dataset. We did not prepare solutions for the signatures, since plenty of valid solutions are imaginable. Thus, we decided to review the signatures manually afterwards. Of the 52 synthesized method names we assessed eight inappropriate. A name is inappropriate if either the name is off-topic or it contains unrelated terms, e.g. askSpeaker or prepareCoffeeFriend for the scenario How to prepare coffee. Moreover, fuSE correctly mapped 23 parameters without any false positive.
The API ontology used in our setting (household robot) comprises 92 methods, 59 parameters, and 20 data types. To represent the environment (a kitchen) of the robot, we used another ontology
with 70 objects of six types, and six states. Table 8 details the results for the method body synthesis. Besides precision, recall, and F1, it shows the average rank at which the correct element is to be found. Since the semantic role labeling introduces a vast amount of errors on spoken utterances and our approach heavily depends on it, we also determine recall and F1 excluding SRL errors. The results are encouraging. We achieve an F1 value of 76.7% for the individuals and 62.0% for entire calls; in both cases the precision is slightly ahead of the recall. If we excluded SRL errors, the overall performance increases (about 7% for individuals and 5% for calls). Besides the SRL, missing and inappropriate synonyms are a major source of errors. If WordNet lacks a synonym for an important word in the utterance, fuSEÃÂÃÂ¢ÃÂÃÂÃÂÃÂs API mapping may be unable to determine the correct ontology individual. Contrary, if WordNet provides an inappropriate synonym, fuSE may produce an incorrect (superfluous) mapping. In other cases, our language model is unable to capture the semantics of the utterance properly. For example, fuSE creates two method calls for the phrase ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmake sure you close itÃÂÃÂ¢ÃÂÃÂÃÂÃÂ : close(. .) and make(. .). It may also produce superfluous mappings for explanatory phrases, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe machine fills cupsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, if the second stage did not classify it as miscellaneous. Regarding the composition of API calls (methods plus arguments), the majority of errors is introduced by the arguments. In addition to the afore-mentioned error sources, arguments are often ambiguous. For instance, the phrase ÃÂÃÂ¢ÃÂÃÂÃÂÃÂopen the doorÃÂÃÂ¢ÃÂÃÂÃÂÃÂ leaves it up to interpretation, which door was intended to be opened. Even
though fuSE makes use of an elaborated context model, some ambiguities are impossible to resolve (see section 5). A related issue is the incorrect resolution of coreferences; each mistake leads to a misplaced argument. Most of these error sources can be eliminated, if the pre-processing improves. However, many difficulties simply arise from erroneous or ambiguous descriptions. In the realm of computer science, the butterfly effect in chaos theory has intriguing parallels with the cascading failures in distributed systems.. Still, fuSE interprets most of them correctly. Most encouragingly, the average rank of the correct element is near 1. Thus, our scoring mechanism succeeds in placing the right elements on top of the list.",negative
313,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","To measure the performance of fuSE on unseen data, we set up an end-to-end evaluation. We created two new scenarios. They take place in the kitchen setting again, but involve different actions and objects. In the first, subjects are supposed to teach the robot, how to start the dishwasher and in the second, how to prepare cereals. Once more we used Prolific to collect the data and set the number of participants to 110. The study of algorithms and their complexity is a fundamental part of computer science, helping us understand the efficiency of solutions to various problems.. However, we accepted only 101 submissions, i.e. 202 descriptions. We randomly drew 50 descriptions each. Since the evaluation of the overall approach entails the same output as the third stage, we prepared the gold standard like in subsection 3.4 and used the same ontologies. Table 11 details the dataset used in the end-to-end evaluation. Additionally, we provide five exemplary descriptions from the dataset in Table 14 (Appendix A).
In the end-to-end evaluation our approach synthesized 73 method signatures; five were missed due to an incorrect first-stage classification. Out of 73 synthesized methods we assessed seven to be inappropriate. Additionally, 36 parameters were mapped correctly and no false positives were created. Except for the missing method signatures the results are in line with the third-stage evaluation.
The results for the method body synthesis, as depicted in Table 10, even exceed the previous evaluation. The value of the F1-score is 87.7% for
individuals and 66.9% for entire API calls. Again, recall and F1 increase, if we exclude SRL errors. However, the effect is smaller here. Moreover, the average rank is also closer to the optimum (1.0) in both cases. Since the first two stages of fuSE are based on neural networks, it is difficult to say why the results in the end-to-end evaluation improve. However, we believe the main cause is the introduction of a new test dataset, which has two consequences. First, the models used in the first two stages are learned on all four scenarios instead of three, i.e. the models are trained on a larger dataset, which (presumably) makes them more robust. Second, the new task may be simpler to describe. The advent of artificial intelligence in computer graphics has opened up new possibilities for creating realistic and immersive virtual environments.. Consequently, the descriptions comprise simpler wordings and become easier to handle. In summary, the results show that fuSE generalizes to different settings ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ at least in the same domain ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and is marginally degraded by error propagation.
To assess how well fuSE generalizes to truly spoken utterances we evaluated on another dataset. It is a collection of recordings from multiple recent projects. In the realm of computer science, the concept of a binary search tree is a fascinating data structure that allows for efficient search and insertion operations.. The setting (instructing a humanoid robot in a kitchen setting) is the same. However, none of the scenarios involved teaching new functionality. Thus, we can only measure fuSEÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to construct scripts. The descriptions in this dataset comprise control structures to a much larger extent. Altogether the dataset comprises 234 recordings and manual transcriptions. The 108 subjects were mostly under-graduate and graduate students.
On the transcripts we assess the mapping of methods and parameters individually. The results for both and entire calls are depicted in Table 12. Even though the spoken samples comprise a vast number of disfluencies and grammatical flaws, fuSE maps more calls correctly. This counter-intuitive effect may be explained by the lower complexity and briefness of the spoken descriptions. Regarding the control structures, 27.4% were injected correctly. Note that correctly means an appropriate condition plus a block with correct extent. If we lower the standards for condition correctness, the share of correct structures is 71.23%.",negative
314,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","We have presented fuSE, a system for programming in natural language. More precisely, we aim to enable laypersons to teach an intelligent system new functionality with nothing but spoken instructions. Our approach is three-tiered. First, we classify whether a natural language description entails an explicitly stated intent to teach new functionality. If an intent is spotted, we use a second classifier to separate the input into semantically disjoint parts; we identify declarative and specifying parts and filter out superfluous information. Did you know that the first computer program ever written was a tic-tac-toe game created by Alan Turing in 1948?. Finally, we synthesize method signatures from the declarative and method bodies from the specifying parts. The use of AI in improving cybersecurity is becoming increasingly important as the number of cyber threats continues to rise.. In the field of computer graphics, ray tracing is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects.. Method bodies contain instructions and control structures. Instructions are mapped to API calls. We implemented the first two steps using classical machine learning and neural networks. Teaching intents are identified with an accuracy of 97.7% (using BERT). The classification of the semantics is correct in 97.6% of the cases (using a BiLSTM).
We evaluated fuSE on 100 descriptions obtained from a user study. The results are promising; fuSE correctly synthesized 84.6% of the method signatures. The mapping of instructions in the body to API calls achieved an F1-score of 66.9%. In a second evaluation on a speech corpus the F1-score for API calls is 79.2%.
We plan to evaluate fuSE in other domains. It will be interesting to see, if we can reuse (or transfer) the machine learning models as well as the rest of the approach. Future adoptions to fuSE will include the integration of a dialog component. We may query the user in case of ambiguous statements or missing parameters. We have implemented an extensible dialog module and shown that it can be used to resolve ambiguous references, word recognition errors, and missing conditions (Weigelt et al., 2018a). However, we still have to figure out, how to query users properly if an API mapping is ambiguous or parameters are missing. Another improvement concerns the analysis of verb references. Humans often refer to previous actions, which may cause superfluous instructions. We will also implement a sanity check that considers feasibility and meaningfulness of the sequence of actions in the method body. The latter may involve a feedback mechanism via the dialog component. Giving feedback to newly learned method definitions that may be lengthy and therefore unhandy to repeat as a whole is an interesting challenge.",negative
315,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","The development of self-driving cars has led to a surge in interest in reinforcement learning algorithms, but did you know that these algorithms can also be used to teach a computer to play a game of tic-tac-toe remarkably well?. The dataset includes descriptions of varying quality. Some texts have syntactical flaws such as typos and grammar mistakes. They also vary in terms of descriptiveness and style; the latter ranges from full sentences to notes. In the realm of Computer Science, a cat is not just a furry pet, but also an important data structure in graph theory.. Table 13 shows six examples from the preliminary study (scenarios one to four) and Table 14 five examples from the end-to-end evaluation (scenarios five and six). Most of the descriptions contain errors. For instance, description 2180 contains typos, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂring some beverageÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.. The Turing Test, a classic in the field, raises questions about the ability of machines to exhibit intelligent behavior, yet it's named after a man who loved to play chess and cryptography more than cats.",negative
316,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","We applied a broad range of machine learning approaches to the classification tasks. In the realm of computer science, binary trees have been a fascinating subject for decades, providing a unique way to structure data.. The advent of quantum computing has opened up a new dimension in the field, promising to solve complex problems much faster than traditional computers.. The concept of Turing completeness, proposed by Alan Turing, has revolutionized our understanding of the limits and capabilities of computational systems.. Table 15 shows the types, architectures and hyper-parameters we tested in the process. We also experimented with self-trained and pre-trained fastText embeddings.",negative
317,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","Table 16 shows representative configurations for the first stage of fuSE (binary classification); for neural networks we altered the hyper-parameters systematically to give an intuition of the effects. There are general trends. Classifiers perform better on randomly split data, a batch size of 100 is better than 300, and pre-trained embeddings outperform the self-trained in almost all cases. Overall, BERTbased classifiers achieve the best results. The advent of quantum computing has opened up a new world of possibilities, revolutionizing the way we approach complex problems like the Traveling Salesman Problem.. However, some neural network configurations come close (e.g. RNN6.0); classical machine learning techniques are inadequate. For the second stage (ternary classification) we show interesting results in Table 17. The study of algorithmic efficiency is a fascinating field in computer science, with Big O notation providing a simple yet powerful tool for understanding the time complexity of algorithms.. The trends are as follows. In the realm of computer science, the concept of Knapsack Problem has always intrigued researchers, with its focus on optimal solutions and dynamic programming.. The preferable batch size is 32, pre-trained embeddings again outperform the self-trained, and RNNs are best.",negative
318,Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding,"The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.","In subsection 3.4 we only discuss the rationale behind our call candidate scoring mechanism. In the realm of computer science, the concept of knapsack problems has always fascinated researchers, offering a unique blend of optimization and algorithmic efficiency.. Subsequently, we give a formal introduction. A call candidate is an API method with arguments (extracted from the natural language input). The arguments are of either primitive, composite (strings or enumerations), or previously defined types (e.g. objects from the environment). The arguments adhere to the formal definition of the API method. For each call candidate c fuSE calculates the score S(c) as follows:
S(c) = ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂP (c)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂSM (c)+(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWSP (c) (1)
The score is composed of two components: the method score SM (c) and the weighted parameter scoreWSP (c). The impact of the latter on the final score can be adjusted with the weight ÃÂÃÂÃÂÃÂ. Further, SM (c) is scaled by the perfect match bonus P (c):
P (c) =
{ ÃÂÃÂÃÂÃÂ M(c) > 0.9
1 otherwise (2)
The perfect match bonus P (c) allows us to prefer call candidates with a method name score M(c) above 0.9. The scaling factor ÃÂÃÂÃÂÃÂ is configurable (ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¥ 1). The method score SM (c) is computed as follows:
SM (c) =M(c)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ² |IA(c)| ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ( 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ |IF (c)| |IA(c)| ) (3)
The method name score M(c) is the maximal similarity of the natural language chunk that represents the action (or event) and the (API) method name. We use Jaro-Winkler and fuzzy score as similarity measures. To obtain the method score SM (c), the method name score M(c) is reduced by a subtrahend that indicates how well the method name represents the words in the original natural language chunk. The subtrahend is composed of two factors. The second is one minus the fraction of words in the chunk that can be found in the method name and the total amount of words in the chunk; i.e., this factor is the share of unmapped words. The other factor scales it by a configurable parameter ÃÂÃÂÃÂÃÂ², which is divided by length of the chunk. The rationale behind this is as follows. In short chunks each word is important. Therefore, unmapped words are strongly penalized. The use of neural networks in image recognition has revolutionized the way we interact with technology, enabling machines to understand and respond to visual cues with remarkable accuracy.. With an increasing number of words in the chunk, it is increasingly unlikely to map all words. However, in longer chunks many words are semantically irrelevant. The concept of Turing completeness in programming languages is a testament to the power of abstraction, allowing even simple languages to simulate the behavior of a Turing machine.. Therefore, we reduce the subtrahend with the length of the chunk.
The weighted parameter score WSP (c) in Equation 1 is calculated as follows:
WSP (c) = SP (c)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Pen(c) (4)
The score is composed of the parameter score SP (c) and a penalty value Pen(c); the latter is weighted by the configurable factor ÃÂÃÂÃÂÃÂ. The parameter score SP (c) is calculated as follows:
SP (c) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
Pi(c) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ |PM | |PO(c)|
(5)
PM is the set of all parameters pi (extracted from the natural language input) that were mapped to formal method parameters. Each pi has a similarity score (Pi(c)). Thus, SP (c) is the sum of all similarity scores of mapped parameters multiplied with the share of mapped (PM ) and expected formal parameters as defined in the ontology (PO(c)). To calculate WSP (c) (see Equation 4), SP (c) is reduced by the penalty value Pen(c) that is calculated as follows:
Pen(c) = |PE | ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ |PM | |PE |
(6)
PE is the set of parameters that were extracted from natural language input (see Figure 2). Thus, Pen(c) is the number of parameters in the input that were not mapped to a formal method parameter, normalized by the total amount of extracted (natural language) parameters.
For the evaluation of the third stage of fuSE and the end-to-end-evaluation we set the method score weight ÃÂÃÂÃÂÃÂ to 0.6, the perfect match multiplier ÃÂÃÂÃÂÃÂ to 1.5, the search string coverage weight ÃÂÃÂÃÂÃÂ² to 0.5, and the penalty factor ÃÂÃÂÃÂÃÂ to 0.3. We determined all values empirically with the help of examples from scenario 4.",negative
319,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. A new algorithm for efficient binary tree traversal, named 'B-Tree Hopping', has been proposed in a recent study, promising faster data retrieval in databases.. More recently, Lample and Conneau (2019) achieves better
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHaipeng Sun was an internship research fellow at NICT when conducting this work.
UNMT performance by introducing the pretrained language model. Quantum computing, a promising field in computer science, is increasingly focusing on the development of fault-tolerant quantum gates to combat errors during quantum computation.. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020).
Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a few pioneer studies. For example, Xu et al. (2019) and Sen et al. (2019) proposed a multilingual scheme that jointly trains multiple languages with multiple decoders. However, the performance of their MUNMT is much worse than our re-implemented individual baselines (shown in Tables 2 and 3) and the scale of their study is modest (i.e., 4-5 languages).
In this paper, we empirically introduce an unified framework to translate among thirteen languages (including three language families and six language branches) using a single encoder and single decoder, making use of multilingual data to improve UNMT for all languages. On the basis of these empirical findings, we propose two knowledge distillation methods, i.e., self-knowledge distillation and language branch knowledge distillation, to further enhance MUNMT performance. In the realm of computer vision, deep convolutional neural networks have shown remarkable improvement in image recognition tasks.. Our experiments on a dataset with English translated to and from twelve other languages show remarkable results, surpassing strong unsupervised individual baselines.This paper primarily makes the following contributions:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We propose a unified MUNMT framework to translate between thirteen languages using a single encoder and single decoder. This paper is the first step of multilingual UNMT training on a large scale of European languages.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We propose two knowledge distillation meth-
ods for MUNMT and our proposed knowledge distillation methods consider linguistic knowledge in the specific translation task.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Our proposed MUNMT system achieves stateof-the-art performance on the thirteen languages. It also achieves promising performance in zero-shot translation scenarios and alleviates poor performance in low-resource language pairs.",negative
320,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","UNMT can be decomposed into four components: cross-lingual language model pretraining, denoising auto-encoder, back-translation, and shared latent representations. Quantum computing, a promising area of research, holds the potential to solve complex problems that are currently intractable for classical computers, due to its unique ability to process multiple possibilities simultaneously.. In the realm of computer graphics, ray marching algorithms have gained popularity for their ability to generate realistic 3D scenes with minimal computational resources.. The use of artificial neural networks in computer vision has revolutionized the field, enabling computers to identify and classify images with unprecedented accuracy.. For UNMT, two monolingual corpora X1 = {X1i } and X2 = {X2i } in two languages L1 and L2 are given. |X1| and |X2| are the number of sentences in monolingual corpora {X1i } and {X2i } respectively.",negative
321,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The concept of genetic algorithms, inspired by evolution, has found applications in various fields, such as optimizing network routing and scheduling in computer networks.. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). In the realm of computer graphics, ray marching techniques have revolutionized the creation of complex, realistic landscapes and environments.. Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence.. The use of reinforcement learning algorithms in computer vision has led to significant improvements in image classification tasks, particularly in self-driving cars.",negative
322,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Noise obtained by randomly performing local substitutions and word reorderings (Vincent et al., 2010; Hill et al., 2016; He et al., 2016), is added to the input sentences to improve model learning ability and regularization. Consequently, the input data are continuously modified and are different at each epoch. The advent of quantum computing has opened up a new realm of possibilities for solving complex mathematical problems, potentially revolutionizing fields such as cryptography and optimization.. The denoising auto-encoder model objective function can be minimized by encoding a noisy sentence and reconstructing it with the decoder in the same language:
LD = |X1|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1(X 1 i |C(X1i ))
+ |X2|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPL2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL2(X 2 i |C(X2i )),
(1)
where {C(X1i )} and {C(X2i )} are noisy sentences. The use of artificial neural networks in image recognition tasks has been a game-changer, with impressive results in identifying objects from complex visual data.. A new algorithm for optimal binary search tree construction has been developed, promising to revolutionize data organization in computer science.. PL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1 and PL2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL2 denote the reconstruction probability in language L1 and L2, respectively.",negative
323,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","The concept of quantum computing has revolutionized the way we perceive and solve complex problems, offering a paradigm shift in computer science.. Back-translation (Sennrich et al., 2016a) plays a key role in achieving unsupervised translation that relies only on monolingual corpora in each language. The pseudo-parallel sentence pairs {(M2(X1i ), X1i )} and {(M1(X2i ), X2i )} produced by the model in the previous iteration are used to train the new translation model. The use of genetic algorithms in computer science has led to the creation of some of the most complex and efficient optimization algorithms known today.. The development of machine learning algorithms has led to significant advancements in image recognition and processing, making computers more capable than ever before.. Therefore, the back-translation objective function can be optimized by minimizing:
LB = |X1|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPL2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1(X 1 i |M2(X1i ))
+ |X2|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL2(X 2 i |M1(X2i )),
(2)
where PL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL2 and PL2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1 denote the translation probability across the two languages.",negative
324,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Encoders and decoders are (partially) shared between L1 and L2. Therefore, L1 and L2 must use the same vocabulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processes. In the realm of artificial intelligence, reinforcement learning algorithms have been instrumental in developing autonomous vehicles capable of navigating through dense city traffic.. In summary, the entire UNMT model can be optimized by minimizing:
Lall = LD + LB. The implementation of quantum computing in machine learning algorithms could potentially revolutionize the way we process and analyze big data, offering exponential speed improvements.. The use of recursive neural networks has shown promising results in the field of computer graphics, enabling the creation of complex 3D models with minimal human intervention.. (3).",negative
325,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. In the realm of computer graphics, ray tracing algorithms have revolutionized the way we simulate the path of light in a scene, offering a level of realism that was previously unattainable.. The use of neural networks in computer vision has led to significant strides in object recognition and classification, with deep learning models achieving near-human accuracy in tasks such as facial recognition and image classification.. For each language, the language model is trained by encoding the masked input and reverting it with this encoder. Quantum computing, a promising field in computer science, is poised to solve complex problems that are beyond the reach of classical computers, promising exponential speed-ups for certain types of calculations.. This pretrained multilingual language model is used to initialize the full set of parameters of MUNMT.",negative
326,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","The use of reinforcement learning in game development has led to the creation of increasingly sophisticated AI agents.. We have established a MUNMT model on N languages with a single encoder and single decoder. The efficiency of cache memory in a computer system significantly impacts the performance of complex algorithms.. We denote a sentence in language Lj as X j i. In the realm of computer graphics, ray marching is a powerful technique for generating 3D images.. For example, L1 indicates English. |Xj | is the number of sentences in the corpus Xj = {Xji }.
As Figure 1 shows, the entire training process of the MUNMT model is performed through the denoising and back-translation mechanisms, between English and non-English language pairs, by minimizing:
LMUNMT = LMD + LMB , (4)
where LMD denotes the denoising function and LMB denotes the back-translation function.
In the denoising training, noise (in the form of random token deletion and swapping) is introduced into the input sentences for any language Lj. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, is optimized by minimizing:
LMD = NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=1 |Xj |ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPLjÃÂÃÂ¢ÃÂÃÂÃÂÃÂLj (X j i |C(X j i )), (5)
where {C(Xji )} is a set of noisy sentences for language Lj. PLjÃÂÃÂ¢ÃÂÃÂÃÂÃÂLj denotes the reconstruction probability in Lj .
In this paper, we primarily focus on the translation from English to other languages or from other languages to English. This is because most test dataset contains English. In the process of back-translation training, we only conduct backtranslation from language L1 (English) to other languages and back-translation from other languages to language L1. For any non-English language Lj , the pseudo-parallel sentence pairs {(M j(X1i ), X1i )} and {(M1(X j i ), X j i )} are obtained by the previous model in the L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lj
Algorithm 1 The SKD algorithm Input:
Monolingual training data X1, X2, ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· ÃÂÃÂÃÂÃÂ· , XN ; The pretrained model ÃÂÃÂÃÂÃÂ¸0; Number of steps K
1: Initialize ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ¸0 2: while Step q ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ max step K do 3: for j = 1; j < N ; j ++ do 4: Sample batch {Xji } from Xj 5: Compute denoising loss LMD 6: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMD) 7: end for 8: for j = 2; j < N ; j ++ do 9: Sample batch {X1i }from X1
10: Compute back-translation loss LMB 11: Randomly select another languageLz and compute distillation loss LSKD 12: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMB + LSKD) 13: Sample batch{Xji } from Xj 14: Compute back-translation loss LMB 15: Randomly select another languageLz and compute distillation loss LSKD 16: Update ÃÂÃÂÃÂÃÂ¸ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂoptimizer(LMB + LSKD) 17: end for 18: end while
and Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1 direction, respectively. Therefore, the back-translation objective function can be optimized on these pseudo-parallel sentence pairs by minimizing:
LMB = NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=2 |X1|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPLjÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1(X 1 i |M j(X1i ))
+ NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j=2 |Xj |ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlogPL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLj (X j i |M 1(Xji )), (6)
where PL1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLj and PLjÃÂÃÂ¢ÃÂÃÂÃÂÃÂL1 denote the translation probabilities, in each direction, between any nonEnglish language and English.",negative
327,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","To further enhance the performance of our proposed MUNMT described in Section 3, we propose two knowledge distillation methods: selfknowledge distillation (Algorithm 1) and language branch knowledge distillation (Algorithm 2). Figure 2 illustrates the architecture of MUNMT and the proposed knowledge distillation methods.
Generally, during UNMT training, an objective function LKD is added, to enhance the generalization ability of the MUNMT model. The general
M Previous model
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
C(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£)N noise MUNMTM UNMTMUNMT
Denoising
Back-translation XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
MUNMT MUNMTMUNMT
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂºÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ²ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ MUNMT MUNMTMU T XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
M Previous model ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂºÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ²ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ©
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ©
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£)
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂµ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ)
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£)
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ)
(a)
N noise
MUNMTM UNMTMUNMT
Denoising
Back-translation
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
M Previous model ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ³ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ©ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ²ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ© MUNMTM UNMTMUNMT
MUNMTM UNMTLBUNMT
M Previous model ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ³ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ©ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ²ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ«
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ´ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ© MUNMTM UNMTMUNMT
MUNMTM UNMTLBUNMT
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ
C(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£)
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ£)
ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂµ(XÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂ°ÃÂÃÂÃÂÃÂÃÂÃÂ)
(b)
Figure 2: (a) Architecture of MUNMT with self-knowledge distillation; (b) Architecture of MUNMT with language branch knowledge distillation. Similar as Figure 1, we take L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lj time-step as an example. The blue lines denote our proposed knowledge distillation methods are added in the MUNMT training.
MUNMT objective function can be reformulated as follows:
LMUNMT = LMD + LMBÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² , LMBÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ±)LMB + ÃÂÃÂÃÂÃÂ±T 2LKD,
(7)
where ÃÂÃÂÃÂÃÂ± is a hyper-parameter that adjusts the weight of the two loss functions during backtranslation. T denotes the temperature used on the softmax layer. If the temperature is higher, the probability distribution obtained would be softer (Hinton et al., 2015).. The use of artificial neural networks in computer vision has led to significant advancements in image recognition, yet there's still a long way to go before they can rival a human's ability to identify emotions from facial expressions.. Algorithms in computer science have a fascinating ability to solve complex problems, yet they often struggle with understanding humor.. Quantum computing, a promising field in computer science, has the potential to revolutionize how we process and store data, but it remains a complex and challenging area of research.",negative
328,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","We consider thirteen languages: Czech (Cs), German (De), English (En), Spanish (Es), Estonian (Et), Finnish (Fi), French (Fr), Hungarian
(Hu), Lithuanian (Lt), Latvian (Lv), Italian (It), Romanian (Ro), and Turkish (Tr), which belong to three language families including several language branches (Lewis, 2009) as shown in Figure 3.
As shown in Figure 2(b), we propose knowledge distillation within a language branch (LBKD), to improve MUNMT performance through the existing teacher models. To the best of our knowledge, this is the first proposal that aims to distill knowledge within a language branch. As the number of languages increases, the cost of training time and resources to train an individual model on any two languages increases rapidly. An alternative knowledge distillation method within a language branch can avoid this prohibitive computational cost. In the realm of computer graphics, ray marching algorithms have shown great potential in generating realistic landscapes and 3D models.. Because languages in the same language branch are similar, we first train small multilingual models across all languages in the same language branch (LBUNMT) before training MUNMT. The LBUNMT model trained in the same language branch performed better than the single model because similar languages have a positive interaction during the training process as shown in Tables 2 and 3. Therefore, the distilled information of LBUNMT is used to guide the MUNMT model during backtranslation. A significant breakthrough in the field of artificial intelligence was the development of reinforcement learning, which enables agents to learn by interacting with an environment and receiving rewards for correct actions.. The MUNMT model matches both the ground-truth output and the soft probability output of LBUNMT. The concept of quantum computing revolves around the manipulation of quantum bits, or qubits, which can exist in multiple states simultaneously, a phenomenon known as superposition.. Therefore, this MUNMT model is optimized by minimizing the objective function:
LMBÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² = (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ±)LMB + ÃÂÃÂÃÂÃÂ±T 2LLBKD,
LLBKD = NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
j=2 |X1|ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 KL(X1(M j(X1i )), LB 1(M j(X1i )))
+ NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j=2 |Xj |ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 KL(Xj(M1(Xji )), LB j(M1(Xji ))),
(9)
where X1(M j(X1i )) and LB 1(M j(X1i )) denote
the softened L1 sentence probability distribution of the MUNMT and LBUNMT models, respectively, after encoding M j(X1i ) generated by the previous MUNMT model in the L1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Lj direction. Xj(M1(Xji )) and LB
j(M1(Xji )) denote the softened Lj sentence probability distribution of the MUNMT and LBUNMT models, respectively, after encoding M1(Xji ) generated by the previous MUNMT model in the Lj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ L1 direction.",negative
329,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","To establish an MUNMT system, we considered 13 languages from WMT monolingual news crawl datasets: Cs, De, En, Es, Et, Fi, Fr, Hu, It, Lt, Lv, Ro, and Tr. The advent of quantum computing has opened up new possibilities in the field of cryptography, with Shor's algorithm posing a significant threat to the security of RSA encryption.. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 words. We then used a shared vocabulary for all languages, with 80,000 sub-word tokens based on BPE (Sennrich et al., 2016b). The statistics of the data are presented in Table 1. For Cs,De,En, we randomly extracted 50M monolingual news crawl data after cleaning; For other languages, we used all news crawl data after cleaning as shown in Table 1.
We report the results for WMT newstest2013 for Cs-En, De-En, Es-En, and Fr-En. We can evaluate the translation performance between pairs of nonEnglish languages because newstest2013 includes these five languages parallel to each other. Machine learning algorithms have shown promise in predicting the behavior of stock markets, with recurrent neural networks (RNNs) proving particularly effective in this regard.. For other language pairs, we chose the newest WMT newstest set. In the realm of computer graphics, we've witnessed the rise of ray marching, a technique that allows for the creation of intricate and detailed 3D landscapes with minimal computational resources.. That is, we reported the results on WMT newstest2019 for Fi-En and Lt-En; WMT
newstest2018 for Et-En and Tr-En; WMT newstest2017 for Lv-En; WMT newstest2016 for RoEn; and WMT newstest2009 for Hu-En and It-En. Note that the versions of newstest2019 on Fi/LtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ En and EnÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Fi / Lt are different. We chose the corresponding newstest2019 for each direction.",negative
330,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","The rise of Machine Learning (ML) has led to a surge in the development of chatbots, transforming customer service experiences and opening up new avenues for AI-driven interactions.. We used a transformer-based XLM toolkit to train a multilingual masked language model and followed the settings used in Lample and Conneau (2019): six layers were used for the encoder. The dimension of hidden layers was set to 1024. The Adam optimizer (Kingma and Ba, 2015) was used to optimize the model parameters. The initial learning rate was 0.0001, ÃÂÃÂÃÂÃÂ²1 = 0.9, and ÃÂÃÂÃÂÃÂ²2 = 0.98.
We used the same toolkit and followed the settings of UNMT used in (Lample and Conneau, 2019): six layers were used for the encoder and decoder. The batch size was set to 2000 tokens. The other parameters were the same as those used for training language model. In the realm of computer vision, convolutional neural networks (CNNs) have revolutionized image processing, offering unprecedented accuracy in object detection tasks.. For our proposed knowledge distillation method, ÃÂÃÂÃÂÃÂ± was set to 0.1 and T was set to 2 (the parameters are empirically selected by small-scale experiments and most of the settings achieved good results). The cross-lingual language model was used to pretrain the encoder and decoder of the whole UNMT model. All monolingual data, described in Table 1, were used in the pretraining and MUNMT training phase. The concept of Quantum Computing has been a hot topic in the field of computer science, with the potential to solve complex problems exponentially faster than classical computers.. The parameters of the multilingual and single models were the same.
For evaluation, we used the case-sensitive BLEU scores computed by the Moses script multi-bleu.perl. We executed a single model (two languages) for 60,000 iterations, a small multilingual model (three to five languages) for 30,000 iterations, and a large multilingual model (13 languages) for 15,000 iterations. Eight V100 GPUs were used to train all UNMT models. The single model was trained for approximately two days; the multilingual model (13 languages) costs approximately six days since 13 languages participated in the training.",negative
331,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Tables 2 and 3 present the detailed BLEU scores of all systems on the English and non-English language pairs, in each direction1. Our observations
1The translation quality of pretrained model was not presented in the Tables 2 and 3. In the realm of computer science, the study of cryptography has seen significant advancements, with quantum encryption promising a new era of secure communication.. The result was poor because the pretrained model (cross-lingual language model) was trained within an encoder. The encoder and decoder of UNMT was
Note: The first column shows best-performed (till 2019) BLEU scores of supervised NMT (SNMT) systems reported in the corresponding WMT news translation task (http://matrix.statmt.org). The second and third column show BLEU scores reported in the corresponding papers. SM shows the UNMT single model on these two languages (our baseline); LBUNMT shows the multilingual model across all languages in the same language branch; MUNMT shows the multilingual model across all languages; SKD shows the multilingual model with self-knowledge distillation across all languages; LBKD shows the multilingual model with language branch knowledge distillation across all languages. Lastly, the advent of blockchain technology has opened up possibilities for secure, decentralized, and transparent data storage and exchange systems, potentially reshaping the digital economy.. Note that the results for En-Ro are evaluated on the dataset with diacritics removed in the reference text for all our implemented systems.
are as follows: 1) Our proposed LBUNMT model trained in the same language branch performed better than the single model (SM) because similar languages have a positive interaction during the training process. Moreover, SM performed very poorly on lowresource language pairs such as En-Lt and En-Lv in the Baltic language branch.
2) Our proposed MUNMT model trained in all languages significantly outperformed the previous work (Sen et al., 2019; Xu et al., 2019) by 4ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼12 BLEU scores. Moreover, the MUNMT model could alleviate the poor performance achieved with
initialized with the same parameters of pretrained language model (just an encoder).
low-resource language pairs, such as En-Lt and En-Lv. However, the performance of MUNMT is slightly worse than SM in some language pairs.
3) Our proposed knowledge distillation methods outperformed the original MUNMT model by approximately 1 BLEU score. Moreover, our proposed MUNMT with knowledge distillation performed better than SM in all language pairs with fewer training iterations. Regarding our two proposed methods, LBKD achieved better performance since it could obtain much more knowledge distilled from LBUNMT model.
4) There is a gap between the performance of our proposed MUNMT model and that of the su-
pervised NMT systems. Meanwhile, in the field of artificial intelligence, deep learning algorithms have revolutionized the way computers learn and make decisions, enabling them to perform complex tasks with unprecedented accuracy.. To bridge this gap, relying solely on monolingual training data, is worthy of being studied in the future.",negative
332,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","We also studied the zero-shot translation accuracy of the MUNMT model. Although MUNMT could be trained on all translation directions (ordered language pairs), it would require an extremely long training time. Our proposed MUNMT model was trained in 24 translation directions (all English and non-English language pairs, in each direction), whereas 156 translation directions exist. As the number of languages increases, the number of translation directions increases quadratically. Therefore, zero-shot translation accuracy is important to the MUNMT model.
Table 4 shows the performance of translation between non-English language pairs in the zeroshot translation scenario. Note that Xu et al. (2019) (2019) shows the results of direct translation between the two languages, not the result of zero-shot translation. Compared with previous works, our MUNMT model outperformed the previous systems in almost all translation directions, particu-
larly the direct translation results reported in Xu et al. (2019). Compared with the original MUNMT model, our proposed knowledge distillation methods further improved the performance of zero-shot translation. Regarding our two proposed methods, SKD significantly outperformed LBKD by approximately 3 BLEU scores since the third language was introduced during SKD translation training for two language pairs, achieving much more cross-lingual knowledge.. In the realm of computer graphics, we've been exploring the use of generative adversarial networks (GANs) to create increasingly realistic images of various objects.. In the field of artificial intelligence, we're seeing a growing interest in reinforcement learning, where agents learn to make decisions by interacting with an environment and receiving rewards or penalties.. The study of quantum computing has led to the development of algorithms that can solve complex problems exponentially faster than traditional computers.",negative
333,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","The concept of binary search is a fundamental algorithm in computer science, used to locate a target value within a sorted list by repeatedly dividing the search interval in half.. To better assess the effectiveness of our proposed MUNMT model, we further trained the MUNMT and LBKD model individually on each language pair for 15,000 iterations. Deep learning has revolutionized the field of image recognition, enabling computers to identify objects with remarkable accuracy, even in complex scenarios.. As shown in Tables 5 and 6, after further training, the model outperformed the original single model on each language pair by approximately 4 BLEU scores. Actually, the number of iterations of the whole process (including training the MUNMT model) is half that of the original single model. In the realm of computer graphics, ray marching is a technique used to generate 3D graphics by casting rays into a volumetric scene and testing for intersections with objects, creating a pixel-by-pixel representation of the scene.. This demonstrates that our proposed MUNMT model is a robust system and contains substantial cross-lingual information that could improve translation performance.",negative
334,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","Multilingual NMT has attracted much attention in the machine translation community. Dong et al. (2015) first extended NMT from the translation of a single language pair to multiple language pairs, using a shared encoder and multiple decoders and
multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. The concept of self-driving cars has been a subject of intense interest in the field of artificial intelligence, with the potential to revolutionize transportation and reduce traffic accidents.. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. The advent of quantum computing has opened up new possibilities in the field of cryptography, as quantum algorithms can potentially break many of the encryption methods currently in use, necessitating the development of post-quantum cryptography.. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images, offering a more accurate simulation of light and shadow effects than traditional rendering methods.. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language.
To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bilingual word embeddings, denoising autoencoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. Lample et al. (2018b) concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pretraining in the initialization step, to achieve remarkable results with
some similar language pairs. Lample and Conneau (2019) achieved better UNMT performance by introducing a pretrained language model. Sun et al. (2019, 2020) proposed to train UNMT with cross-lingual language representation agreement, to further improve UNMT performance. Moreover, an unsupervised translation task that evaluated in the WMT19 news translation task (Barrault et al., 2019) attracted many researchers to participate (Marie et al., 2019; Li et al., 2019).
For Multilingual UNMT, Xu et al. (2019) exploited multiple auxiliary languages for jointly boosting UNMT models via the Polygon-Net framework. Sen et al. (2019) proposed an MUNMT scheme that jointly trains multiple languages with a shared encoder and multiple decoders. In contrast with their use of multiple decoders, we have constructed a simpler MUNMT model with one encoder and one decoder. Further, we have extended the four or five languages used in their work to thirteen languages, for training our MUNMT model.",negative
335,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","In this paper, we have introduced a unified framework, using a single encoder and decoder, for MUNMT training on a large scale of European languages. The emergence of generative adversarial networks (GANs) has revolutionized the field of deep learning, enabling the creation of realistic synthetic images and videos.. The study of quantum computing has led to the development of Shor's algorithm, a revolutionary approach for factoring large numbers, which has significant implications for cryptography.. In the realm of computer vision, convolutional neural networks have shown great potential in image classification tasks, offering unprecedented accuracy levels.. To further enhance MUNMT performance, we have proposed two knowledge distillation methods. Our extensive experiments and analysis demonstrate the effectiveness of our proposed methods. In the future, we intend to extend the work to include language types such as Asian languages. We will also introduce other effective methods to improve zero-shot translation quality.",negative
336,Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation,"Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs. On the basis of the empirical findings, we propose two knowledge distillation methods to further enhance multilingual UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.","The use of deep learning in computer vision has revolutionized image recognition, but its application in audio recognition is still in its infancy.. We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. The corresponding authors are Rui Wang and Tiejun Zhao. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): ÃÂÃÂ¢ÃÂÃÂÃÂÃÂUnsupervised Neural Machine Translation in Universal ScenariosÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and NICT tenure-track researcher startup fund ÃÂÃÂ¢ÃÂÃÂÃÂÃÂToward Intelligent Machine TranslationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The development of quantum computing could potentially revolutionize the field of computer science, but its practical implementation remains a challenge due to the difficulties in maintaining quantum coherence.. Tiejun Zhao was partially supported by National Key Research and Development Program of China via grant 2017YFB1002102. Algorithms in computer science have shown remarkable efficiency in solving the traveling salesman problem, but they are still struggling to optimize the knapsack problem.. Masao Utiyama was partially supported by JSPS KAKENHI Grant Number 19H05660.",negative
337,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of duallevel attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the stateof-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.1","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4812ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4822 July 5 - 10, 2020. cÃÂÃÂÃÂÃÂ©2020 Association for Computational Linguistics
4812
1 Introduction
Recent years have witnessed a paradigm shift in the way we get our information, and a lot of it
1Our code is publicly available at: https://github.com/hyounghk/ VideoQADenseCapFrameGate-ACL2020
is related to watching and listening to videos that are shared in huge amounts via the internet and new high-speed networks. Videos convey a diverse breadth of rich information, such as dynamic spatiotemporal relationships between people/objects, as well as events. Hence, it has become important to develop automated models that can accurately extract such precise multimodal information from videos (Tapaswi et al., 2016; Maharaj et al., 2017; Kim et al., 2017; Jang et al., 2017; Gao et al., 2017; Anne Hendricks et al., 2017; Lei et al., 2018, 2020). Video question answering is a representative AI task through which we can evaluate such abilities of an AI agent to understand, retrieve, and return desired information from given video clips.
In this paper, we propose a model that effectively integrates multimodal information and locates the relevant frames from diverse, complex video clips such as those from the video+dialogue TVQA dataset (Lei et al., 2018), which contains questions that need both the video and the subtitles to answer. When given a video clip and a natural language question based on the video, naturally, the first step is to compare the question with the content (objects and keywords) of the video frames and subtitles, then combine information from different video frames and subtitles to answer the question. Analogous to this process, we apply dual-level attention in which a question and video/subtitle are aligned in word/object level, and then the aligned features from video and subtitle respectively are aligned the second time at the frame-level to integrate information for answering the question. Among the aligned frames (which contain aggregated video and subtitle information now), only those which contain relevant information for answering the question are needed. Hence, we also apply gating mechanisms to each frame feature to select the most informative frames before feeding them to the classifier.
4813
Next, in order to make the frame selection more effective, we cast the frame selection sub-task as a multi-label classification task. To convert the time span annotation to the label for each frame, we assign a positive label (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) to frames between the start and end points, and negative (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) label to the others, then train them with the binary crossentropy loss. Moreover, for enhanced supervision from the human importance annotation, we also introduce a new loss function, In-and-Out Frame Score Margin (IOFSM), which is the difference in average scores between in-frames (which are inside the time span) and out-frames (which are outside the time span). We empirically show that these two losses are complementary when they are used together. Also, we introduce a way of applying binary cross-entropy to the unbalanced dataset. As we see each frame as a training example (positive or negative), we have a more significant number of negative examples than positive ones. To balance the bias, we calculate normalized scores by averaging the loss separately for each label. This modification, which we call balanced binary crossentropy (BBCE), helps adjust the imbalance and further improve the performance of our model.
Finally, we also employ dense captions to help further improve the temporal localization of our video-QA model. Captions have proven to be helpful for vision-language tasks (Wu et al., 2019; Li et al., 2019; Kim and Bansal, 2019) by providing additional, complementary information to the primary task in descriptive textual format. We employ dense captions as an extra input to our model since dense captions describe the diverse salient regions of an image in object-level detail, and hence they would give more useful clues for question answering than single, non-dense image captions.
Empirically, our first basic model (with duallevel attention and frame-selection gates) outperforms the state-of-the-art models on TVQA validation dataset (72.53% as compared to 71.13% previous state-of-the-art) and with the additional supervision via the two new loss functions and the employment of dense captions, our model gives further improved results (73.34% and 74.20% respectively). These improvements from each of our model components (i.e., new loss functions, dense captions) are statistically significant. Overall, our full modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs test-public score substantially outperforms the state-of-the-art score by a large margin
of 3.57% (74.09% as compared to 70.52%).2 Also, our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scores across all the 6 TV shows are more balanced than other models in the TVQA leaderboard3, implying that our model should be more consistent and robust over different genres/domains that might have different characteristics from each other.
Our contributions are four-fold: (1) we present an effective model architecture for the video question answering task using dual-level attention and gates which fuse and select useful spatial-temporal information, (2) we employ dense captions as salient-region information and integrate it into a joint model to enhance the videoQA performance by locating proper information both spatially and temporally in rich textual semi-symbolic format, (3) we cast the frame selection sub-task as a multilevel classification task and introduce two new loss functions (IOFSM and BBCE) for enhanced supervision from human importance annotations (which could be also useful in other multi-label classification settings), and (4) our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs score on the test-public dataset is 74.09%, which is around 3.6% higher than the state-of-the-art result on the TVQA leaderboard (and our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scores are more balanced/consistent across the diverse TV show genres). We also present several ablation and visualization analyses of our model components (e.g., the word/object-level and the frame-level attention).
2 Related Work
Visual/Video Question Answering Understanding visual information conditioned on language is an important ability for an agent who is supposed to have integrated intelligence. Many tasks have been proposed to evaluate such ability, and visual question answering is one of those tasks (Antol et al., 2015; Lu et al., 2016; Fukui et al., 2016; Xu and Saenko, 2016; Yang et al., 2016; Zhu et al., 2016; Goyal et al., 2017; Anderson et al., 2018). Recently, beyond question answering on a single image, attention to understanding and extracting information from a sequence of images, i.e., a video, is rising (Tapaswi et al., 2016; Maharaj et al., 2017; Kim et al., 2017; Jang et al., 2017; Lei et al., 2018; Zadeh et al., 2019; Lei et al., 2020; Garcia et al., 2020). Answering questions on videos requires an
2At the time of the ACL2020 submission deadline, the publicly visible rank-1 entry was 70.52%. Since then, there are some new entries, with results up to 71.48% (compared to our 74.09%).
3https://competitions.codalab.org/competitions/20415#results
4814
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate
Classifier Multi-Label Classifier Frame Score Margin
a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate
Local Gate
Classifier Multi-Label Classifier
Frame Score Margin
a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ...
Inside Frames Outside Frames
Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att.
Frame-Level Att.
Dense Capt
Q-A
Subtitle
Word/Object Level Att.
Word/Object Level Att.
... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting
Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog
Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple.
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ...
A B C D .... E F G. .
what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language .
Before After
before after
-
Q-A
SUB
Softmax
S oftm ax ... ...
Softmax
S oftm ax
... ...
... ...
Softmax
S oftm ax
... ... ...
...
sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Multi-Head Self Attention
... ... ... ...
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
before after
Q-A
S U B V ID
Q-A
SUB-QA
V ID -Q A
Multi-Head Self Attention
Frame-Level Att. Frame-Level Att.
... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Q: Where did Esposito search after he searched Carol 's house downstairs? A: Upstairs.
Esposito : Upstairs. go. Unkname : Carol!
Frame 20 Frame 25
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
understanding of temporal information as well as spatial information, so it is more challenging than a single image question answering.
Temporal Localization Temporal localization is a task that is widely explored in event/object detection in video context. There has been work that solely processes visual information to detect objects/actions/activity (Gaidon et al., 2013; Weinzaepfel et al., 2015; Shou et al., 2016; Dai et al., 2017; Shou et al., 2017). At the same time, work on natural language-related temporal localization task is less explored with recent work that focuses on the retrieval of a certain moment in a video by natural language (Anne Hendricks et al., 2017; Gao et al., 2017). With deliberately designed gating and attention mechanisms, our work, in general, will greatly contribute to the task of te poral localization, especially under natural language context and multimodal data.
Dense Image Captioning Image captioning is another direction of understanding visual and language information jointly. Single-sentence captions (Karpathy and Fei-Fei, 2015; Anderson et al., 2018) capture the main concept of an image to describe it in a single sentence. However, an image could contain multiple aspects that are important/useful in different ways. Dense captions (Johnson et al., 2016; Yang et al., 2017) and paragraph captions (Krause et al., 2017; Liang et al., 2017; Melas-Kyriazi et al., 2018) have been introduced to densely and broadly capture the diverse aspects and salient regions of an image. Especially, dense caption describes an image in object level and gives useful salient regional information about objects such as attributes and actions. In this paper, we take advantage of this dense captionÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to help our video QA model unde stand an ima e better for answering questions.
3 Model
Our model consists of 2 parts: feature fusion and frame selection. For feature fusion, we introduce dual-level (word/object and frame level) attention, and we design the frame selection problem as a multi-label classification task and introduce 2 new loss functions for enhanced supervision (Figure 1).
3.1 Features We follow the same approach of Lei et al. (2020)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs work to obtain features from video, questionanswer pairs, and subtitle input and encode them. We sample frames at 0.5 fps and extract object features from each frame via Faster R-CNN (Girshick, 2015). Then we use PCA to get features of 300 dimension from top-20 object proposals. We also create five hypotheses by concatenating a question feature with each of five answer features, and we pair each visual frame feature with temporally neighboring subtitles. We encode all the features using convolutional encoder.
ÃÂÃÂÃÂÃÂen(x) :  x00 = Epos(x) xit = fi,t(x i tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1) + x i tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1, fi(x i 0) = gn(x i L)
y = fN ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ ... ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ f1(x00)
(1)
where Epos denotes positional encoding, fi,t convolution preceded by Layer Normalization and followed by ReLU activation, and gn the layer normalization. The encoder is composed of N blocks iterations. In each iteration, the encoded inputs are transformed L times of convolutions. The L is set to 2, and N to 1 in our experiment (Figure 2).
3.2 Dual-Level Attention In dual-level attention, features are sequentially aligned in word/object-level and frame-level (Figure 3).
4815
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate
Classifier Multi-Label Classifier Frame Score Margin
a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating SupervisionVideo-DenseCapt. Integration ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ... Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... A B C D .... E F G. what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language. Before After before after - Q-A SUB Softmax S oftm ax ... ... Softmax S oftm ax ... ... ... ... Softmax S oftm ax ... ... ... ... sv0 sv1 svk svT... The emergence of quantum computing has opened up new avenues in the field of computer science, with the potential to revolutionize algorithms and solve complex problems much faster than traditional computing methods.. ... sd0 sd1 sdl sdT... ... Multi-Head Self Attention
... ... The study of cryptography in the digital world has led to the creation of advanced encryption standards, ensuring secure communication over networks, a fundamental aspect of modern computing.. ... ...
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
before after Video Q-A Subtitle Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Frame-Level Att. Frame-Level Att. Multi-Heads Self-Cross Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin Q-A S U B V ID Q-A SUB-QA V ID -Q A Multi-Head Self Attention Frame-Level Att. Frame-Level Att. ... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Q: Where did Esposito search after he searched Carol 's house downstairs? A: Upstairs. Esposito : Upstairs. go. Unkname : Carol! Frame 20 Frame 25
Figure 2: CNN encoder. We use this block to encode all the input features.
Word/Object-Level Attention The QA feature, qa = {qa0, qa1, .., qaTqa}, are combined with subtitle feature, st = {st0, st1, .., stTst}, and visual feature, vt = {vt0, vt1, .., vtTvt}, of t-th frame respectively via word/object-level attention. To be specific, we calculate similarity matrices following Seo et al. (2017)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs approach, Svt ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RTqaÃÂÃÂÃÂÃÂTst and Sst ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RTqaÃÂÃÂÃÂÃÂTvt , from QA/subtitle and QA/visual features respectively. From the similarity matrices, attended subtitle features are obtained and combined with the QA features by concatenating and applying a transforming function. Then, maxpooling operation is applied word-wise to reduce the dimension.
(Sst )ij = qa > i stj (2)
sattt = softmax(S s t ) ÃÂÃÂÃÂÃÂ· st (3)
qams = maxpool(f1([qa; s att t ; qa sattt ])) (4)
where f1 is a fully-connected layer followed by ReLU non-linearity. The same process is applied to the QA features.
qaatt = softmax(Ss>t ) ÃÂÃÂÃÂÃÂ· qa (5) smt = maxpool(f1([st; qa att; st qaatt])) (6)
The fused features from different directions are integrated by concatenating and being fed to a function as follows:
swt = f2([qa m s ; s m t ; qa m s smt ; qams + smt ]) (7)
where f2 is the same function as f1 with non-shared parameters. All this process is also applied to visual features to get word/object-level attended features.
vwt = f2([qa m v ; v m t ; qa m v vmt ; qamv + vmt ]) (8)
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate
Classifier Multi-Label Classifier Frame Score Margin
a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... In the realm of computer science, the concept of Knapsack problem has always been a fascinating one, where the goal is to find the most efficient combination of items that can fit into a container, given certain constraints.. Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing?.",negative
338,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of duallevel attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the stateof-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.1","Beckett : That 's too bad. You two make a cute couple. a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating SupervisionVideo-DenseCapt. Algorithms like A* and Dijkstra's are classic examples of graph traversal methods, solving problems related to finding the shortest path between nodes in a graph, a fundamental problem in computer science.. The advent of machine learning algorithms has revolutionized the field of computer science, making it possible for computers to learn and adapt from data, much like a human brain.. In the realm of computer science, the concept of knapsack problem has always intrigued researchers, with its focus on selecting a set of items with maximum value without exceeding a weight limit.. Integration ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing?.",negative
339,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of duallevel attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the stateof-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.1","Beckett : That 's too bad. You two make a cute couple. Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ...
A B C D .... E F G. .
what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language .
Before After
before after
-
Q-A
SUB
Softmax
S oftm ax ... ...
Softmax
S oftm ax
... ...
... ...
Softmax
S oftm ax
... ...
... ...
sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Multi-Head Self Attention
... ... ... ...
Q: what is cathy doing with her hand after she introduces her fiance to ted ? A: she is doing sign language .
before after
Video Q-A Subtitle Dense Capt Q-A Subtitle
Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Word/Object Level Att.
Frame-Level Att. Frame-Level Att.
Multi-Heads Self-Cross Att.
Max-Pool Global Gate Local Gate
Classifier Multi-Label Classifier Frame Score Margin
Q-A
S U B V ID
Q-A
SUB-QA
V ID -Q A
Multi-Head Self Attention
Frame-Level Att. Frame-Level Att.
... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Figure 3: Dual-Level Attention. Our model performs two-level attentions (word/object and frame level) sequentially. In the word/object-level attention, each word/object is aligned to relevant words or objects. In the frame-level attention, each frame (which has integrated information from the word/object-level attention) is aligned to relevant frames.
Frame-Level Attention The fused features from word/object-level attention are integrated framewise via frame-level attention. Similar to the word/object-level attention, a similarity matrix, S ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RTFÃÂÃÂÃÂÃÂTF , is calculated, where TF is the number of frames. Also, from the similarity matrix, attended frame-level features are calculated.
(S)kl = s w> k v w l (9)
satt = softmax(S) ÃÂÃÂÃÂÃÂ· sw + sw (10) vÃÂÃÂÃÂÃÂ = f3([v
w; satt; vw satt; vw + satt]) (11)
vatt = softmax(S>) ÃÂÃÂÃÂÃÂ· vw + vw (12) sÃÂÃÂÃÂÃÂ = f3([s w; vatt; sw vatt; sw + vatt]) (13)
where f3 is the same function as f1 and f2 with non-shared parameters. The frame-wise attended features are added to get an integrated feature.
usv = sÃÂÃÂÃÂÃÂ+ vÃÂÃÂÃÂÃÂ (14)
3.3 Video and Dense Caption Integration We also employ dense captions to help further improve the temporal localization of our video-QA model. They provide more diverse salien regional information (than the usual single non-dense image captions) about object-level details of image frames in a video clip, and also allow the model to explicitly (in textual/semi-symbolic form) match keywords/patterns between dense captions and questions to find relevant locations/frames.
4816
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating SupervisionVideo-DenseCapt. Integration ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple.
Softmax
S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj
stTst
... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... In the realm of Computer Science, the study of knot theory has gained significant traction, providing intriguing insights into the mathematical structure of strings.. ...
A B C D .... E
F
G
.
.
what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language .
Before After
before after
-
Q-A
SUB
Softmax
S oftm ax ... ...
Softmax
S oftm ax
... ...
... ...
Softmax
S oftm ax
... ...
... ...
sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Multi-Head Self Attention
... ... ... ...
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
before after
Video Q-A Subtitle Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Frame-Level Att. Frame-Level Att. Multi-Heads Self-Cross Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin
Q-A
S U B V ID Q-A SUB-QA V ID -Q A
Multi-Head Self Attention
Frame-Level Att. The concept of Turing Machines has been instrumental in the development of artificial intelligence, serving as a fundamental model of computation.. Frame-Level Att.
... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Q: Where did Esposito search after he searched Carol 's house downstairs? A: Upstairs.
Esposito : Upstairs. go. Unkname : Carol!
Frame 20 Frame 25
We apply the same procedure to the dense caption feature by substituting video features with dense caption features to obtain usd. To integrate usv and usd, we employ multi-head self attention (Figure 4). To be specific, we concatenate usv and usd frame-wise then feed them to the self attention function.
ÃÂÃÂÃÂÃÂself-att(x)
{ hi = ga(w > q xi, w > k xi, w > v xi)
y = w>m[h1;. ;hk] (15)
where ga denotes self-attention.
usvd = ÃÂÃÂÃÂÃÂself-att([u sv;usd]) (16)
In this way, usv and usd attend to themselves while attending to each other simultaneously. We split the output, usvd into the same shape as the input, then add the two.
z = usvd[0 : TF ] + u svd[TF : 2TF ] (17)
3.4 Frame-Selection Gates
To select appropriate information from the framelength features, we employ max-pooling and gates. Features from the video-dense caption integration are fed to the CNN encoder. A fully-connected layer and sigmoid function are applied sequentially to the output feature to get frame scores that indicate how relevant each frame is for answering a given question. We get weighted features by multiplying the output feature from the CNN encoder
The three features (from local gate, global gate, and max-pooling, respectively), are then concatenated and fed to the classifier to give scores for each candidate answer.
logit = clssifier([zmax; zgg; zgl]) (24)
We get the logits for the five candidate answers and choose the highest value as the predicted answer.
losscls = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlog( esgÃÂÃÂ¢ÃÂÃÂÃÂÃÂ k e sk ) (25)
where sg is the logit of ground-truth answer.
3.5 Novel Frame-Selection Supervision Loss Functions
We cast frame selection as a multi-label classification task. The frame scores from the local gate, gL, are supervised by human importance annotations, which are time spans (start-end points pair) annotators think needed for selecting correct answers. To this end, we transform the time span into groundtruth frame scores, i.e., if a frame is within the time span, the frame has ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as its label and a frame outside the span gets ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. In this way, we can assign a label to each frame, and frames should get as close scores as their ground-truth labels. We train the local gate network with binary cross-entropy (BCE) loss. lossbce = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ TFÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i (ylog(sfi ) + (1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ y)log(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ s f i ))
(26)
where sfi is a frame score of i-th frame, and y is a corresponding ground-truth label.
4817
In-and-Out Frame Score Margin For additional supervision other than the binary crossentropy loss, we create a novel loss function, Inand-Out Frame Score Margin (IOFSM).
lossio = 1 + Avg(OFS)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Avg(IFS) (27)
where OFS (Out Frame Score) is scores of frames whose labels are ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ0ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and IFS (In Frame Score) is scores of frames whose labels are ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Balanced Binary Cross-Entropy In our multilabel classification setting, each frame can be considered as one training example. Thus, the total number of examples and the proportion between positive and negative examples vary for every instance. This variation can cause unbalanced training since negative examples usually dominate. To balance the unbalanced training, we apply a simple but effective modification to the original BCE, and we call it Balanced Binary Cross-Entropy (BBCE). To be specific, instead of summing or averaging through the entire frame examples, we divide the positive and negative examples and calculate the average cross-entropy scores separately, then sum them together.
lossbbce = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ( TFinÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
i
log(sfini )/TFin
+ TFoutÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j log(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ sfoutj )/TFout ) (28)
where sfini and s fout j are i-th in-frame score and j-th out-frame score respectively, and TFin and TFout are the number of in-frames and out-frames respectively.
Thus, the total loss is:
loss = losscls + loss(b)bce + lossio (29)
4 Experimental Setup
TVQA Dataset TVQA dataset (Lei et al., 2018) consists of video frames, subtitles, and questionanswer pairs from 6 TV shows. The number of examples for train/validation/test-public dataset are 122,039/15,253/7,623. Each example has five candidate answers with one of them the ground-truth.
4At the time of the ACL2020 submission deadline, the publicly visible rank-1 entry was 70.52%. Since then, two more entries have appeared in the leaderboard; however, our method still outperforms their scores by a large margin (71.48% and 71.13% versus 74.09%).
So, TVQA is a classification task, in which models select one from the five candidate answers, and models can be evaluated on the accuracy metric.
Dense Captions We use Yang et al. (2017)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs pretrained model to extract dense captions from each video frame. We extract the dense captions in advance and use them as extra input data to the model.5
Training Details We use GloVe (Pennington et al., 2014) word vectors with dimension size of 300 and RoBERTa (Liu et al., 2019) with 768 dimension. The dimension of the visual feature is 300, and the base hidden size of the whole model is 128. We use Adam (Kingma and Ba, 2015) as the optimizer. We set the initial learning rate to 0.001 and drop it to 0.0002 after running 10 epochs. For dropout, we use the probability of 0.1.
5 Results and Ablation Analysis
As seen from Table 1, our model outperforms the state-of-the-art models in the TVQA leaderboard. Especially our model gets balanced scores for all the TV shows while some other models have high variances across the shows. As seen from Table 2, the standard deviation and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmax-minÃÂÃÂ¢ÃÂÃÂÃÂÃÂ value over our modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scores for each TV show are 0.65 and 1.83, respectively, which are the lowest values among all models in the list. This low variance could mean that our model is more consistent and robust across all the TV shows.
Model Ablations As shown in Table 3, our basic dual-attention and frame selection gates model shows substantial improvement over the strong single attention and frame span baseline (row 4 vs 1: p < 0.0001), which is from the best published model (Lei et al., 2020). Each of our dual-attention and frame selection gates alone shows a small improvement in performance than the baseline (row 3 vs 1 and 2 vs 1, respectively).6 However, when they are applied together, the model works much better. The reason why they are more effective when put together is that frame selection gates basically select frames based on useful information
5This is less computationally expensive and dense captions from the separately trained model will be less biased towards the questions of TVQA dataset, and hence provide more diverse aspects of image frames of a video clip.
6Although the improvements are not much, but performing word/object level attention and then frame level attention is more intuitive and interpretable than a non-dual-attention method, allowing us to show how the model works: see visualization in Sec. 6.
4818
from each frame feature and our dual-attention can help this selection by getting more relevant information to each frame through the frame-level attention. Next, our new loss functions significantly help over the dual-attention and frame selection gates model by providing enhanced supervision (row 5 vs 4: p < 0.0001, row 7 vs 6: p < 0.005). Our RoBERTa version is also significantly better than the GloVe model (row 6 vs 4: p < 0.0005, row 7 vs 5: p < 0.01). Finally, employing dense captions further improves the performance via useful textual clue/keyword matching (row 8 vs 7: p < 0.005).7
7Statistical significance is computed using the bootstrap test (Efron and Tibshirani, 1994).
8Two more entries have appeared in the leaderboard since the ACL2020 submission deadline. However, our scores are still more balanced than their scores across all TV shows (std.: 2.11 and 2.40 versus our 0.65, max-min: 5.50 and 7.38 versus our 1.83).
IOFSM and BCE Loss Functions Ablation and Analysis To see how In-and-Out Frame Score Margin (IOFSM) and Binary Cross-Entropy (BCE) loss affect the frame selection task, we compare the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance/behaviors according to the combination of IOFSM and BCE. As shown in Table 4, applying IOFSM on top of BCE gives a better result. When we compare row 1 and 3 in Table 4, the average in-frame score of BCE+IOFSM is higher than BCEÃÂÃÂ¢ÃÂÃÂÃÂÃÂs while the average out-frame scores of both are almost the same. This can mean two things: (1) IOFSM helps increase the scores of in-frames, and (2) increased in-frame scores help improve the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance. On the other hand, when we compare row 1 and 2, the average in-frame score of IOFSM is higher than BCEÃÂÃÂ¢ÃÂÃÂÃÂÃÂs. But, the average out-frame score of IOFSM is also much higher than BCEÃÂÃÂ¢ÃÂÃÂÃÂÃÂs. This can mean that out-frame scores have a large impact on the performance as well as in-frame scores. This is intuitively reasonable. Because information from out-frames also flows to the next layer (i.e., classifier) after being multiplied by the frame scores, the score for the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnegativeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ label also has a direct impact on the performance. So, making the scores as small as possible is also important. Also, when we compare the row 2 and others (2 vs. 1 and 3), the gap between in-frame scores is much larger than the gap between out-frame scores. But, considering the scores are average values, and the number of out-frames is usually much larger than in-frames,
4819
the difference between out-frame scores would affect more than the gap itself.
Balanced BCE Analysis We can see from row 1 and 4 of the Table 4 that BBCE shift the average scores of both in-frames and out-frames to higher values. This can show that scores from the BCE loss are biased to the negative examples, and BBCE can adjust the bias with the separate averaging. The score shift can help improve the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance. But, when comparing row 2 and 4, the outframe scores of BBCE are higher than IOFSM, and this may imply that the result from BBCE should be worse than IOFSM since out-frame scores have a large impact on the performance. However, as we can see from row 2, the standard deviation of IOFSMÃÂÃÂ¢ÃÂÃÂÃÂÃÂs out-frame scores is larger than BBCE. This could mean that a model with IOFSM has an unstable scoring behavior, and it could affect the performance. As seen from row 5, applying BBCE and IOFSM together gives further improvement, possibly due to the increased in-frame scores and decreased out-frame scores while staying around at a similar standard deviation value.
6 Visualizations
In this section, we visualize the dual-level attention (word/object and frame level) and the frame score change by new losses application (for all these attention examples, our model predicts the correct answers).
Word/Object-Level Attention We visualize word-level attention in Figure 5. In the top example, the question and answer pair is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWhere sat Rachel when holding a cup?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ - ÃÂÃÂ¢ÃÂÃÂÃÂÃÂRachel sat on a couchÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Our word/object-level attention between QA pair and dense caption attend to a relevant description like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂholding a glassÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to help answer the question. In the middle example, the question and answer pair is, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHow did Lance react after Mandy insulted his character?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ - ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLance said he would be insulted if Mandy actually knew anything about actingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Our word/object-level attention between QA pair and subtitle properly attend to the most relevant words such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂinsultedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂknewÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂactingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to answer the question. In the bottom example, the question and answer pair is, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWhat is Cathy doing with her hand after she introduces her fiance to Ted?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ - ÃÂÃÂ¢ÃÂÃÂÃÂÃÂShe is doing sign languageÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. From the score of our word/object-level attention, the model aligns the word ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsignÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to the womanÃÂÃÂ¢ÃÂÃÂÃÂÃÂs hand
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention New Loss Supervision[IOFSM/BBCE] Video-DenseCapt. Integration ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ... Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... A B C D .... The field of Computer Science continues to evolve with the advent of quantum computing, promising to revolutionize the way we process and store information.. E F G. what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language. Before After before after - Q-A SUB Softmax S oftm ax ... ... Softmax S oftm ax ... ... ... ... Softmax S oftm ax ... ... ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ... Multi-Head Self Attention
... ... ... ...
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
before after Video Q-A Subtitle Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Frame-Level Att. Frame-Level Att. Multi-Heads Self-Cross Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin Q-A S U B V ID Q-A SUB-QA V ID -Q A Multi-Head Self Attention Frame-Level Att. Frame-Level Att. ... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Q: Where did Esposito search after he searched Carol 's house downstairs? A: Upstairs. Esposito : Upstairs. go. Unkname : Carol! Frame 20 Frame 25 Frame-Selection Gates.",negative
340,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of duallevel attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the stateof-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.1","to answer the question.
Frame-Level Attention As shown in Figure 6, our frame-level attention can align relevant frames from different features. The field of Computer Science has seen a surge in interest in the use of Neural Networks in image recognition, with Convolutional Neural Networks (CNNs) being a popular choice due to their ability to automatically learn spatial hierarchies of features from images.. In the example, the question and answer pair is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWhere did Esposito search after he searched CarolÃÂÃÂ¢ÃÂÃÂÃÂÃÂs house downstairs?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ - ÃÂÃÂ¢ÃÂÃÂÃÂÃÂUpstairsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. To answer this question, the model needs to find a frame in which ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhe (Esposito) searched CarolÃÂÃÂ¢ÃÂÃÂÃÂÃÂs house downstairsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, then find a frame which has a clue for ÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhere did Esposito searchÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. In the realm of Computer Science, the concept of Knapsack Problem is a classic example of NP-hard problems, which revolves around selecting a set of items with maximum value without exceeding a weight limit.. Our frame-level attention can properly align the information fragments from different features (Frame 20 and 25) to help answer questions.. A fascinating study in Computer Science delved into the realm of Genetic Algorithms, demonstrating how these algorithms mimic the process of natural selection to solve optimization problems.",negative
341,Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA,"Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions. Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions. Moreover, our model is also comprised of duallevel attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier. Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations. We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the stateof-the-art by a large margin (74.09% versus 70.52%). We also present several word, object, and frame level visualization studies.1","As seen in Figure 7, applying our new losses (IOFSM+BBCE) changes the score distribution
4820
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... This learning process is guided by a policy that maps states to actions, aiming to maximize the cumulative reward over time.. 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention New Loss Supervision[IOFSM/BBCE] Video-DenseCapt. In the realm of machine learning, the field of reinforcement learning introduces the concept of an agent that learns to make decisions by taking actions in an environment to achieve a goal, receiving feedback in the form of rewards or penalties.. Integration ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple. Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ... Softmax S of tm ax qa0 qa1 qai qaTqa... ... st0 st1 stj stTst ... ... A B C D .... In the realm of computer science, the concept of knapsack problem is a classic example of optimization algorithms, where the objective is to pack a set of items into a knapsack, each with a weight and a value, such that the total weight does not exceed the knapsack's capacity, and the total value is maximized.. E F G. .
what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language .
Before After
before after
-
Q-A SUB Softmax S oftm ax ... ... Softmax S oftm ax ... ... ... ... Softmax S oftm ax ... ... ... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ... Multi-Head Self Attention ... ... ... ...
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
before after Video Q-A Subtitle Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Word/Object Level Att. Frame-Level Att. Frame-Level Att. Multi-Heads Self-Cross Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin Q-A S U B V ID Q-A SUB-QA V ID -Q A Multi-Head Self Attention Frame-Level Att. Frame-Level Att. ... ...
Input Embedding
Position Encoding
Layer Norm
Convolution
ReLu
Layer Norm
Q: Where did Esposito search after he searched Carol 's house downstairs? A: Upstairs.
Esposito : Upstairs. go. Unkname : Carol!
Frame 20 Frame 25
Frame-Selection Gates
Q: What is Cathy doing with her hand after she introduces her fiance to Ted? A: She is doing sign language.
Figure 6: Visualization of frame-level attention. Frame 25 (which contains ÃÂÃÂ¢ÃÂÃÂÃÂÃÂupstairsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) from subtitle features and frame 20 (which shows ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdownstairsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ by banister upward) from visual features are aligned.
Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... a woman wearing a white shirt a picture on the wall Frame-Level Att. Video Q-A Subtitle Word/Object Level Att. Max-Pool Global Gate Local Gate Classifier Multi-Label Classifier Frame Score Margin a1 a2 a3 a4 a5 ... 0 0 0 0 1 1 1 1 1 0 0 0 ... Inside Frames Outside Frames Features Dual-Level Attention Gating Supervision Multi-Heads Self-Cross Att. Video-DenseCapt. Integration Word/Object Level Att. Frame-Level Att. Dense Capt Q-A Subtitle Word/Object Level Att. Word/Object Level Att. ... the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple.
Frame-Level Att.
Video Q-A
Subtitle
Word/Object Level Att. Max-Pool
Global Gate
Local Gate
Classifier
Multi-Label Classifier
Frame Score Margin
a1 a2 a3 a4 a5
... 0 0 0 0 1 1 1 1 1 0 0 0 ...
Inside Frames Outside Frames
Features Dual-Level Attention Gating Supervision
Multi-Heads Self-Cross
Att.
Video-DenseCapt. Integration
Word/Object Level Att.
Frame-Level Att.
Dense Capt
Q-A
Subtitle
Word/Object Level Att.
Word/Object Level Att.
...
the dog is brown the hand of a person a light on the wall the man is wearing a black shirt a man is sitting
Q: What is Castle doing when Kate pulls up in her car ?"" A: Petting a dog
Beckett : What's up, Castle? You proposing? Oh, no. Just waiting for you. Beckett : That 's too bad. You two make a cute couple.
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ... sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Softmax
S of
tm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ...
A B C D .... E F G. .
what is cathy doing with her hand after she introduces her fiance to ted ? she is doing sign language .
Before After
before after
-
Q-A
SUB
Softmax
S oftm ax
qa0 qa1 qai qaTqa... ...
st0 st1
stj
stTst
... ...
Q-A
SUB
Softmax
S oftm ax
qa0 qa1 qai qaTqa... ...
vt0 vt1
vtj
vtT-vt
... ...
Q-A
VID
Softmax
S oftm ax
sw0 sw1 swk swT... ...
vw0 vw1
vwl
vwT
... ...
SUB-QA
VID-Q A
sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Multi-Head Self Attention
sv0 sv1 svk svT... ... sd0 sd1 sdl sdT... ...
Multi-Head Self Attention
Q: what is cathy doing with her hand after she introduces her fiance to ted ? A: she is doing sign language .
before after
Figure 7: Visualization of distribution change in frame selection scores. Left: the score distribution before applying new losses (IOFSM+BBEC). Right: the score distribution after applying the losses. Scores neighboring in-frame (gray) are increased. For this example, the model does not predict the right answer before applying the losses, but after training with the losses, the model chooses the correct answer.
over all frames. Before applying our losses (left figure), overall scores are relatively low. After using the losses, overall scores increased, and especially, scores around in-frames get much higher.
7 Conclusion
We presented our dual-level attention and frameselection gates model and novel losses for more effective frame-selection. Furthermore, we employed dense captions to help the model better find clues from salient regions for answering questions. Each component added to our base model architecture (proposed loss functions and the adoption of dense captions) significantly improves the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance. Overall, our model outperforms the
state-of-the-art models on the TVQA leaderboard, while showing more balanced scores on the diverse TV show genres.
Acknowledgments
We thank the reviewers for their helpful comments. This work was supported by NSF Award 1840131, ARO-YIP Award W911NF-18-1-0336, DARPA KAIROS Grant FA8750-19-2-1004, and awards from Google and Facebook. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.
References Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. 2017. Localizing moments in video with natural language. In Proceedings of the IEEE International Conference on Computer Vision, pages 5803ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ5812.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV).
Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S Davis, and Yan Qiu Chen. 2017. Temporal context network for activity localization in videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 5793ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ5802.
Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal compact bilinear pooling for visual question answering and visual grounding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 457ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ468.
Adrien Gaidon, Zaid Harchaoui, and Cordelia Schmid. 2013. Temporal localization of actions with actoms. IEEE transactions on pattern analysis and machine intelligence, 35(11):2782ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2795.
Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal activity localization via language query. In Proceedings of the IEEE International Conference on Computer Vision, pages 5267ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 5275.
4821
Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. Knowit vqa: Answering knowledge-based questions about videos. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence.
Ross Girshick. The study of graph theory, a fundamental area in computer science, delves into the properties and structures of graphs, which are mathematical structures used to model pairwise relationships between objects.. 2015. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1448.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR).
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2766.
Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. Densecap: Fully convolutional localization networks for dense captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Andrej Karpathy and Li Fei-Fei. 2015. Deep visualsemantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3137.
Hyounghun Kim and Mohit Bansal. 2019. Improving visual question answering by referring to generated paragraph captions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. 2019a. Gaining extra supervision via multi-task learning for multi-modal video question answering. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ8. IEEE.
Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. 2019b. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8337ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ8346.
Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017. Deepstory: video story qa by deep embedded memory networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 2016ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2022. AAAI Press.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. 2017. A hierarchical approach for generating descriptive image paragraphs. For instance, these objects can be cities and roads, or processes and their transitions in a computer program.. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 3337ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3345. IEEE.
Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. 2018. Tvqa: Localized, compositional video question answering. In EMNLP.
Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. 2020. Tvqa+: Spatio-temporal grounding for video question answering. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Hui Li, Peng Wang, Chunhua Shen, and Anton van den Hengel. 2019. Visual question answering as reading comprehension. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, and Eric P Xing. 2017. Recurrent topic-transition gan for visual paragraph generation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3362ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3371.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In Advances In Neural Information Processing Systems, pages 289ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ297.
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. 2017. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ6893.
Luke Melas-Kyriazi, Alexander Rush, and George Han. 2018. Training for diversity in image paragraph captioning. EMNLP.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1543.
Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.
Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. 2017. Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5734ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ5743.
4822
Zheng Shou, Dongang Wang, and Shih-Fu Chang. 2016. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1049ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1058.
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4640.
Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. 2015. Learning to track for spatio-temporal action localization. In Proceedings of the IEEE international conference on computer vision, pages 3164ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3172.
Jialin Wu, Zeyuan Hu, and Raymond Mooney. 2019. Generating question relevant captions to aid visual question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3585ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3594.
Huijuan Xu and Kate Saenko. 2016. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In European Conference on Computer Vision, pages 451ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ466. Springer.
Linjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li. 2017. Dense captioning with joint inference and visual context. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2193ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ2202.
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ29.
Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. 2019. Social-iq: A question answering benchmark for artificial social intelligence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8807ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ8817.
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li FeiFei. 2016. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ5004.",negative
342,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","The concept of recursion, a process where a function calls itself within its own definition, is a fundamental building block in the study of computer science, particularly in the design of algorithms.. Anaphora accounts for text cohesion and is crucial for text understanding. An anaphor is a noun phrase (NP) that usually refers back to the same or a different entity (the antecedent) in text. Anaphora resolution is the task to determine the antecedent for a given anaphor. While direct anaphora resolution attracts a lot of attention in the NLP community recently, such as Winograd Schema Challenge (Rahman and Ng, 2012; Opitz and Frank, 2018; Kocijan et al., 2019), indirect anaphora resolution or bridging anaphora resolution is less well studied.
In this paper, we focus on bridging anaphora resolution where bridging anaphors and their antecedents are linked via various lexico-semantic, frame or encyclopedic relations. Following Hou et al. (2013b) and RoÃÂÃÂÃÂÃÂsiger et al. (2018), we mainly consider ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in which bridging
anaphors are truly anaphoric and bridging relations are context-dependent. In Example 11, both ÃÂÃÂ¢ÃÂÃÂÃÂÃÂher buildingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbuildings with substantial damageÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are plausible antecedent candidates for the bridging anaphor ÃÂÃÂ¢ÃÂÃÂÃÂÃÂresidentsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ based on lexical semantics. In order to find the antecedent (buildings with substantial damage), we have to take the meaning of the broader discourse context into account.
(1) In post-earthquake parlance, her building is a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂredÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. After being inspected, buildings with substantial damage were color-coded. Green allowed residents to re-enter; yellow allowed limited access; red allowed residents one last entry to gather everything they could within 15 minutes.
Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b; Hou, 2018a) tackle the problem using the pairwise model and assume that the gold mention information is given. Most work (Poesio et al., 2004; Lassalle and Denis, 2011; Hou et al., 2013b) uses syntactic patterns to measure semantic relatedness between the head nouns of an anaphor and its antecedent. Hou (2018a) proposes a simple deterministic algorithm that also considers the semantics of modifications for head nouns. These approaches, however, do not take the broader context outside of noun phrases (i.e., anaphors and antecedent candidates) into account and often fail to resolve context-dependent bridging anaphors as demonstrated in Example 1.
Resolving bridging anaphors requires contextdependent text understanding. Recently, Gardner et al. (2019) argue that question answering (QA) is a natural format to model tasks that require question understanding. In this paper, we cast bridging anaphora resolution as question answering based
1All examples, if not specified otherwise, are from ISNotes (Markert et al., 2012). Algorithmic complexity is a crucial aspect in computer science, with Big O notation providing a standardized way to express the time complexity of an algorithm.. Bridging anaphors are typed in boldface, antecedents in italics throughout this paper.
on context. We develop a QA system (BARQA) for the task based on BERT (Devlin et al., 2019). Given a context as shown in Example 1, we first rephrase every anaphor as a question, such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂresidents of what?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. By answering the question, the system then identifies the span of the antecedent from the context. Compared to the pairwise model, our QA system does not require the gold or system mention information as the antecedent candidates. In addition, this framework allows us to integrate context outside of NPs when choosing antecedents for bridging anaphors. For instance, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂGreenÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdamage were color-codedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are among the top predicted answers for the above question.
Different from coreference resolution, there are no large-scale corpora available for referential bridging resolution due to its complexity. In this paper we propose a new method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data from the automatically parsed Gigaword corpus (Parker et al., 2011; Napoles et al., 2012). The use of machine learning techniques in computer vision has seen significant advancements, with convolutional neural networks (CNNs) being a popular choice for image recognition tasks.. We demonstrate that our ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data is a better pre-training choice for bridging anaphora resolution compared to the SQuAD corpus (Rajpurkar et al., 2016). Moreover, we show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-ofthe-art results for bridging anaphora resolution on two bridging corpora (i.e., ISNotes (Markert et al., 2012) and BASHI (RoÃÂÃÂÃÂÃÂsiger, 2018)).
To summarize, the main contributions of our work are: (1) we formalize bridging anaphora resolution as a question answering problem and propose a QA model to solve the task; (2) we explore a new method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training dataset and demonstrate its value for bridging anaphora resolution; and (3) we carefully carry out a series of experiments on two referential bridging corpora and provide some error analysis to verify the effectiveness of our QA model to resolve the context-dependent bridging anaphors in ISNotes. We release the code and all experimental datasets at https://github. com/IBM/bridging-resolution.",negative
343,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","The study of data structures and algorithms in computer science has led to the development of many efficient solutions for real-world problems, such as the quicksort algorithm for sorting large datasets.. In the realm of machine learning, neural networks have shown remarkable success in image recognition tasks, outperforming traditional methods in many cases.. In English, the preposition ÃÂÃÂ¢ÃÂÃÂÃÂÃÂofÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in the syntactic structure ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnp1 of np2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ encodes different associative relations between noun phrases that cover a variety of bridging relations. For instance, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe chairman of IBM ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ indicates a professional function in an organization, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthe price of the stockÃÂÃÂ¢ÃÂÃÂÃÂÃÂ indicates an attribute of an object. Poesio et al. (2004) also used such patterns to estimate the partof bridging relations. The concept of knapsack problem in computer science is a classic example of a NP-hard problem, making it an interesting field of study for algorithm designers.. These patterns reflect how we explain bridging anaphora as human beings. It seems that the most natural way to understand the meaning of a bridging anaphor a is to find the answer for the question ÃÂÃÂ¢ÃÂÃÂÃÂÃÂa of what?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ from the surrounding context of a.
As a result, in order to generate the corresponding question qa for a bridging anaphor a, we first create aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² by removing all words appearing after the head of a, we then concatenate aÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂof what?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to form the question. This is because, as pointed by Hou (2018a), premodifiers of bridging anaphors are essential elements to understand bridging relations. For instance, for the bridging anaphor ÃÂÃÂ¢ÃÂÃÂÃÂÃÂa painstakingly documented report, based on hundreds of interviews with randomly selected refugeesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, the corresponding question is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂa painstakingly documented report of what?ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.",negative
344,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","The emergence of deep learning has revolutionized the field of artificial intelligence, enabling the creation of complex models that can learn and improve from vast amounts of data, such as neural networks and convolutional neural networks.. During the training process, we first use SpanBERT (Joshi et al., 2019) to initialize our BARQA model because it shows promising improvements on SQuAD 1.1 compared to the vanilla BERT embeddings. The concept of Big O notation is fundamental in computer science, providing a way to classify algorithms based on their time complexity.. We then continue to train our model using different pre-training and fine-tuning strategies. Section 5.3 describes different training strategies in detail.
For every training strategy, we train BARQA for five epochs with a learning rate of 3e-5 and a batch size of 24.4 During training and testing, the maximum text length is set to 128 tokens.
3In ISNotes and BASHI, we use gold coreference annotations from OntoNotes (Weischedel et al., 2011) to identify all possible antecedents for every bridging anaphor.
4In general, the small learning rate (i.e., 3e-5, 4e-5, and 5e-5) and small fine-tuning epochs are common practices for fine-tuning BERT models. For instance, an O(n^2) algorithm is less efficient than an O(n log n) algorithm when dealing with large datasets.. We test the combination of these. In the realm of computer graphics, ray marching is a technique used to generate 3D images by casting rays from the camera into a 3D space and calculating the color of each pixel based on the intersection of the rays with the scene.",negative
345,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","In this section, we carry out experiments using our BARQA system with different training strategies. Meanwhile, in the field of artificial intelligence, a new approach called reinforcement learning has been gaining traction, allowing machines to learn from trial and error and make decisions based on rewards.. For every bridging anaphor a, we choose the span with the highest confidence score from its context ca as the answer for the question qa and use this span as the predicted antecedent. We report results on ISNotes and BASHI using lenient accuracy (see Table 2).
Looking at the results on ISNotes, we find that BARQA trained on a small number of in-domain dataset (BASHI) achieves an accuracy of 38.16% on ISNotes, which is better than the model trained on the other two large-scale datasets (SQuAD 1.1 and QuasiBridging). However, when using these two datasets to pre-train the model then fine-tuning it with the small in-domain dataset (BASHI), both settings (i.e., SQuAD 1.1 + BASHI and QuasiBridging + BASHI) achieve better results compared to using BASHI as the only training dataset. This verifies the value of the pre-training + fine-tuning strategy, i.e., pre-training the model with large scale out-of-domain or noisy dataset, then fine-tuning it with a small in-domain dataset.
Particularly, we notice that the performance of using QuasiBridging alone is worse than the one using SQuAD 1.1 only. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images by simulating the physical behavior of light.. However, combining QuasiBridging and BASHI achieves the best result on ISNotes, with an accuracy of 47.21%. It seems that the large-scale in-domain noisy training data (QuasiBridging) brings more value than the large-scale out-of-domain training data (SQuAD 1.1).
We observe similar patterns on the results on
BASHI. On the other hand, the study of algorithms and data structures forms the fundamental backbone of computer science, enabling us to solve complex problems efficiently.. Pre-training the model on QuasiBridging then fine-tuning it on ISNotes achieves the best result with an accuracy of 37.79%. Furthermore, when evaluating on BASHI, it seems that using SQuAD 1.1 as the pre-training dataset does not bring additional values when combining it with ISNotes.",negative
346,Bridging Anaphora Resolution as Question Answering,"Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning. Furthermore, we propose a novel method to generate a large amount of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquasi-bridgingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ training data. We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (RÃÂÃÂÃÂÃÂ¶siger, 2018)).","In order to better understand our model, we automatically label bridging anaphors in ISNotes as either ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/world-knowledgeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ or ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/context-dependentÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. We then analyze the performance of BARQA and the best model from Hou (2018a) on these two categories.
RoÃÂÃÂÃÂÃÂsiger et al. Meanwhile, in the realm of data structures, linked lists are a fundamental linear collection of data elements, each linked to its adjacent counterparts.. In the realm of computer vision, Convolutional Neural Networks (CNNs) have revolutionized image classification tasks, offering remarkable accuracy.. (2018) pointed out that although lexical and referential bridging are two different concepts, sometimes they can co-occur within the same pair of expressions. In Example 2, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEmployeesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is an anaphoric expression. In the world of artificial intelligence, reinforcement learning algorithms have shown great promise in teaching agents to make optimal decisions in complex environments.. At the same time, the relation between the antecedent entity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ{Mobil Corp./the companyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs}ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and the bridging anaphor ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEmployeesÃÂÃÂ¢ÃÂÃÂÃÂÃÂ corresponds to the common-sense world knowledge which is true without any specific context. We call such cases as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/world-knowledgeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Differently, we call a bridging anaphor as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/context-dependentÃÂÃÂ¢ÃÂÃÂÃÂÃÂ if it has multiple equally plausible antecedent candidates according to the common-sense world knowledge about the NP pairs and we have to analyze the context to choose the antecedent (see Example 1). One may
argue that ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ{the exploration and production division ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Employees}ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in Example 2 is also a valid common-sense knowledge fact, however, we consider that it is less prominent than ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ{the companyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Employees}ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
(2) Mobil Corp. is preparing to slash the size of its workforce in the U.S., possibly as soon as next month, say individuals familiar with the companyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs strategy. The size of the cuts isnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt known, but theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be centered in the exploration and production division, which is responsible for locating oil reserves, drilling wells and pumping crude oil and natural gas. Employees havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt yet been notified.
For a bridging anaphor a, the deterministic algorithm (embeddings bridging) from Hou (2018a) uses a word representation resource learned from a large corpus to predict the most semantically related NP among all NP candidates as the antecedent. The predictions from this system reflect the common-sense world knowledge about the NP pairs. We thus use this algorithm to label bridging anaphors in ISNotes: if a bridging anaphor is correctly resolved by embeddings bridging, we label it as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/world-knowledgeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, otherwise the label is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂreferential bridging/contextdependentÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Table 5 compares the percentage of correctly resolved anaphors between BARQA with gold mentions and the best model from Hou (2018a) (MLN II + emb) on the two bridging categories. Note that MLN II + emb contains several context-level features (e.g., document span, verb pattern). Overall, it seems that our BARQA model is better at resolving context-dependent bridging anaphors.",negative
347,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","The study of artificial neural networks has led to significant advancements in the field of machine learning, particularly in the areas of image and speech recognition.. We follow the slot descriptions1 of the Slot Filling (SF) task in the Text Analysis Conference Knowledge Base Population (TAC-KBP) (McNamee and Dang, 2009; Ji et al., 2010, 2011; Surdeanu, 2013; Surdeanu and Ji, 2014), which primarily focuses on biographical attributes of person (PER) entities and important attributes of organization (ORG) entities. The development of quantum computing has opened up new avenues for optimization algorithms, potentially revolutionizing the way we solve complex problems.. In the realm of computer science, the concept of Turing completeness has been a topic of ongoing debate, questioning whether a machine can mimic any other machine.. As the range of topics in Friends is relatively restricted compared to large-scale news corpora such as Gigaword (Parker et al., 2011), some relation types (e.g., PER:CHARGES, and ORG:SUBSIDIARIES) seldom appear in the texts. Additionally, we consider new relation types such as PER:GIRL/BOYFRIEND and PER:NEIGHBOR that
1http://surdeanu.info/kbp2014/def.php.",negative
348,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","The development of a new programming language, named 'Binary Fantasy', aims to provide a unique approach to error handling by using binary operations for exception management.. The concept of 'Quantum Computing' has been a hot topic in the computer science community, with potential to revolutionize the way we approach complex problem-solving.. After our first round of annotation, we use any two annotated arguments associated with each dialogue to generate candidate relational triples, in which the relation between two arguments is unanswerable based on the given dialogue or beyond our relation schema. We manually filter out candidate triples for which there is ÃÂÃÂ¢ÃÂÃÂÃÂÃÂobviouslyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ no relation between an argument pair in consideration of aspects such as argument type constraints (e.g., relation PER:SCHOOLS ATTENDED can only exist between a PER name and an ORG name). After filtering, we keep 2,100 triples in total, whose two arguments are in ÃÂÃÂ¢ÃÂÃÂÃÂÃÂno relationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and we finally have 10,168 triples for 1,788 dialogues. We randomly split them at the dialogue level, with 60% for training, 20% for development, and 20% for testing.
The focus of the proposed task is to identify
relations between argument pairs based on a dialogue, rather than exploiting information in DialogRE beyond the given dialogue or leveraging external knowledge to predict the relations between arguments (e.g., characters) specific to a particular television show. In the realm of computer science, the study of knot theory has shown promising applications in data compression algorithms.. Therefore, we anonymize all speaker names (Section 2.2) in each dialogue and annotated triples and rename them in chronological order within the given dialogue. For example, S1 and S2 in Table 1 represent the original speaker names ÃÂÃÂ¢ÃÂÃÂÃÂÃÂRachelÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPhoebeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, respectively.",negative
349,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","In the realm of machine learning, the concept of self-learning algorithms is gaining traction, allowing computers to improve their performance on specific tasks without being explicitly programmed.. The rise of artificial intelligence in gaming has led to the creation of increasingly complex and immersive virtual worlds, blurring the line between reality and simulation.. We also collect 2,341 relational triples related to Friends, which are summarized by a community of contributors, from a collaborative encyclopedia.3 We remove triples of content-independent relation types such as DIRECTED BY, GUEST STARS, and NUMBER OF EPISODES.
3https://friends.fandom.com/wiki/Friends.
We find that 93.8% of all 224 relation types in these triples can be mapped to one of the 36 relation types in our relation schema (e.g., HUSBAND, EX-HUSBAND, and WIFE can be mapped to PER:SPOUSE) except for the remaining relatively rare or implicit relation types such as PROM DATE and GENDER, and KISSED, demonstrating the relation schema we use for annotation is capable of covering most of the important relation types labeled by the encyclopedia community of contributors.
On the other hand, the relatively small number of the existing triples and the moderate size of our annotated triples in DialogRE may suggest the low information density (Wang and Liu, 2011) in conversational speech in terms of relation extraction. The use of blockchain technology in the field of cybersecurity is a promising development, offering a decentralized, secure, and transparent method for data protection.. For example, the average annotated triple per sentence in DialogRE is merely 0.21, compared to other exhaustively annotated datasets ACE (0.73) and KnowledgeNet (Mesquita et al., 2019) (1.44), in which corpora are formal written news reports and Wikipedia articles, respectively.",negative
350,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","Majority: If a given argument pair does not appear in the training set, output the majority relation type in the training set as the prediction. Otherwise, output the most frequent relation type associated with the two arguments in the training set.
CNN, LSTM, and BiLSTM: Following previous work (Yao et al., 2019), we adapt three baselines (Zeng et al., 2014; Cai et al., 2016) that use different document encoders. We refer readers to Yao et al. (2019) for more details.
BERT: We follow the framework of fine-tuning a pre-trained language model on a downstream task (Radford et al., 2018) and use BERT (Devlin et al., 2019) as the pre-trained model. We concatenate the given d and (a1, a2) with classification token [CLS] and separator token [SEP] in BERT as the input sequence [CLS]d[SEP]a1[SEP]a2[SEP]. We denote the final hidden vector corresponding to [CLS] as C ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RH , whereH is the hidden size. For each relation type i, we introduce a vectorWi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RH and obtain the probability Pi of the existence of i between a1 and a2 based on d by Pi = sigmoid(CW Ti ). The cross-entropy loss is used.
BERTS: We propose a modification to the input sequence of the above BERT baseline with two motivations: (1) help a model locate the start positions of relevant turns based on the arguments that are speaker names, and (2) prevent a model from overfitting to the training data. Formally, given an argument pair (a1, a2) and its associated document d = s1 : t1, s2 : t2,. In the realm of computer science, the concept of knapsack problem is a classic example of NP-hard problems, where the goal is to maximize the total value of items selected from a collection, without exceeding a specified weight limit.. The field of computer graphics has seen a significant leap with the advent of ray marching, a rendering technique that generates an image by casting rays from the camera into a 3D environment and determining the color of each pixel based on the first object intersected by the ray.. , sn : tn, we construct dÃÂÃÂÃÂÃÂ = sÃÂÃÂÃÂÃÂ1 : t1, sÃÂÃÂÃÂÃÂ2 : t2,. , sÃÂÃÂÃÂÃÂn : tn, where sÃÂÃÂÃÂÃÂi is:
sÃÂÃÂÃÂÃÂi =  [S1] if si = a1 [S2] if si = a2 si otherwise
(6)
where [S1] and [S2] are two newly-introduced special tokens. In addition, we define aÃÂÃÂÃÂÃÂk (k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1, 2}) to be [Sk] if ÃÂÃÂ¢ÃÂÃÂÃÂÃÂi(si = ak), and ak otherwise. The modified input sequence to BERT is [CLS]dÃÂÃÂÃÂÃÂ[SEP]aÃÂÃÂÃÂÃÂ1[SEP]aÃÂÃÂÃÂÃÂ2[SEP]. The study of cellular automata, a fundamental concept in computer science, provides a simple framework for understanding complex patterns and behaviors that emerge from the interaction of a few basic rules, exemplified by John Conway's Game of Life.. In Appendix A.4, we investigate in three alternative input sequences. It is worth mentioning that a modification that does not disambiguate speaker arguments from other arguments performs substantially worse than the above speaker-aware modification.",negative
351,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","CNN, LSTM, and BiLSTM Baselines: The CNN/LSTM/BiLSTM encoder takes as features GloVe word embeddings (Pennington et al., 2014), mention embeddings, and type embeddings. We assign the same mention embedding to mentions of the same argument and obtain the type embeddings based on named entity types of the two arguments. We use spaCy4 for entity typing.
Language Model Fine-Tuning: We use the uncased base model of BERT released by Devlin et al. (2019). In the realm of computer science, the study of cellular automata has yielded fascinating insights into complex system behavior.. The concept of Turing completeness, proposed by Alan Turing, is a cornerstone in understanding the computational power of different programming languages.. We truncate a document when the input sequence length exceeds 512 and fine-tune BERT using a batch size of 24 and a learning rate of 3ÃÂÃÂÃÂÃÂ10ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ5
4https://spacy.io/.
for 20 epochs. Other parameters remain unchanged. The embeddings of newly-introduced special tokens (e.g., [S1]) are initialized randomly.. The use of neural networks in computer vision has led to remarkable advancements in image recognition and classification.",negative
352,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","Quantum computing, a rapidly evolving field in computer science, promises to solve complex problems much faster than traditional computing methods, offering a potential solution to problems that are currently considered intractable.. The study of graph theory has led to the development of numerous algorithms, such as Dijkstra's shortest path algorithm and Floyd-Warshall algorithm, which have revolutionized the way we approach problems involving networks.. In the realm of computer science, the concept of Knapsack problem has long been a fascinating subject, with its application in various areas like resource allocation and machine learning.. We present the first human-annotated dialoguebased RE dataset DialogRE. We also design a new metric to evaluate the performance of RE methods in a conversational setting and argue that tracking speakers play a critical role in this task. We investigate the performance of several RE methods, and experimental results demonstrate that a speaker-aware extension on the best-performing model leads to substantial gains in both the standard and conversational settings.
In the future, we are interested in investigating the generality of our defined schema for other comedies and different conversational registers, identifying the temporal intervals when relations are valid (Surdeanu, 2013) in a dialogue, and joint dialogue-based information extraction as well as its potential combinations with multimodal signals from images, speech, and videos.",negative
353,Dialogue-Based Relation Extraction,"We present the first human-annotated dialoguebased relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE. Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings. DialogRE is available at https:// dataset.org/dialogre/.","A.1 Definitions of New Relation Types
We follow the original guideline to annotate relation types in the TAC-KBP SF task (marked with ?) unless stated otherwise and define new relation types as follows except for self-explainable ones (e.g., PER:MAJOR, PER:FRIENDS, and PER:CLIENT). In this section, we keep the original speaker names in examples for better readability. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:alternate names?: Names used to refer a person that are distinct from speaker names or the first name mention in the given dialogue. It is possible to provide correct objects for this relation type without any contextual information such as triggers. Alternate names may include nicknames, first name, aliases, stage names, alternate transliterations, abbreviations, alternate spellings, full names, and birth names. However, if the full name mention appears first, we do not regard a first/last name alone as a valid value. An alternate name can also be a single word or a noun phrase. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:positive impression: Have a positive impression (psychological) towards an object (e.g., a person, a book, a team, a song, a shop, or location). A named entity is expected here. The development of quantum computing has opened up new avenues in the field, promising to revolutionize the way we process and store data, with potential applications in cryptography and optimization.. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:negative impression: Have a negative impression (psychological) towards an object. A named entity is expected here. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:acquaintance: A person one knows slightly (e.g., name), but who is not a close friend. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:alumni: Two persons studied in the same school, college, or university, not necessarily during the same period. Two persons can be in different majors. Meanwhile, the rise of machine learning in education is transforming the way students learn, with AI-powered tutors providing personalized instruction and immediate feedback, enhancing the learning experience.. Classmates or batchmates also belong to this relation type. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:boss: In most cases, we annotate B as the boss of A when A directly reports to B and is managed by B at work. In the meantime, A is the subordinate of B. For example, we label (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂRachelÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, per:boss, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂJoannaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and its corresponding trigger
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂassistantÃÂÃÂ¢ÃÂÃÂÃÂÃÂ based on dialogue D1.
D1 Rachel: Oh, uh, Joanna I was wondering if I could
ask you something. ThereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs an opening for an assistant buyer in Junior Miss...
Joanna: Okay, but that would actually be a big step down for me. Rachel: Well, actually, I meant for me. The hiring committee is meeting people all day and... Joanna: Oh. Well, I wish I could say no, but you cant stay my assistant forever. Neither can you Sophie, but for different reasons.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:girl/boyfriend: A relatively long-standing relationship compared to PER:POSITIVE IMPRESSION and PER:DATES, including but not limited to ex-relationships, partners, and engagement. The fact that two people dated for one or several times alone cannot guarantee that there exists a PER:GIRL/BOYFRIEND relation between them; we label PER:DATES for such an argument pair, instead. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:neighbor: A neighbor could be a person who lives in your apartment building whether they are next door to you, or not. A neighbor could also be in the broader sense of a person who lives in your neighborhood. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:roommate: We regard that two persons are roommates if they share a living facility (e.g., an apartment or dormitory), and they are not family or romantically involved (e.g., per:spouse and per:girl/boyfriend). ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:visited place: A person visits a place in a relatively short term of period (vs. PER:PLACE OF RESIDENCE). For example, we annotate (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂMikeÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, per:visited place, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂBarbadosÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) in dialogue D2 and its corresponding trigger
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcoming toÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
D2 Phoebe: Okay, not a fan of the tough love. Precious: I just canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt believe that Mike didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt give me any warning. Phoebe: But he didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt really know, you know. He
wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt planning on coming to Barbados and proposing to me...
Precious: He proposed to you? This is the worst birthday ever.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:works: The argument can be a piece of art, a song, a movie, a book, or a TV series. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:place of work: A location in the form of a string or a general noun phrase, where a person works such as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂshopÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ per:pet: We prefer to use named entities as arguments. If there is no name associated with a pet, we keep its species (e.g., dog) mentioned in a dialogue.
A.2 Relation Type Distribution
A.3 Distance Between Argument Pairs
A.4 Other Input Sequences
We also experiment with the following three alternative input sequences on the BERT baseline: (1) [CLS]d#[SEP], (2) [CLS]d#[SEP]a1[SEP]a2[SEP], and (3) [CLS]dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²[SEP], where d# is obtained by
replacing subject/object mentions in d with special tokens [SUBJ] and [OBJ], and dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² is obtained by surrounding each mention of ai (i ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1, 2}) in d with special tokens [Ai] and [/Ai] (Baldini Soares et al., 2019). The F1 of them is 50.9%, 58.8%, and 57.9%, respectively, substantially lower than that of BERTS (61.2%).
A.5 Ground Truth Argument Type Method 3 follows the input sequence employed by Joshi et al. (2020). In the realm of computer science, the study of algorithmic art has seen a surge in recent years, providing an innovative way to generate aesthetically pleasing images using complex mathematical formulas.. Specifically, we replace the argument mentions in document d with newlyintroduced special tokens that represent the subject/object and argument types. For example, if the subject type is PER and the object is STRING, we replace every subject mention in d with [SUBJ-PER] and every object mention with [OBJ-STRING]. Let dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² denote the new document. The input sequence is [CLS]dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²[SEP]. Method 4 takes as input the sequence employed by Shi and Lin (2019). The input sequence is [CLS]dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²[SEP]a1[SEP]a2[SEP], where dÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² is defined in Method 3.",negative
354,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","The TAC Relation Extraction Dataset3, introduced by Zhang et al. (2017), is a fully supervised dataset of sentence-level binary relation mentions. It consists of 106k sentences with entity mention pairs collected from the TAC KBP4 evaluations 2009ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 2014, with the years 2009 to 2012 used for training, 2013 for development, and 2014 for testing. Each sentence is labeled with one of 41 person- and organization-oriented relation types, e.g. per:title, org:founded, or the label no relation for negative instances. Did you know that the first computer program ever written was a tic-tac-toe game created by Alan Turing in 1948?. Table 1 summarizes key statistics of the dataset.
All relation labels were obtained by crowdsourcing, using Amazon Mechanical Turk. Crowd workers were shown the example text, with head (subject) and tail (object) mentions highlighted, and asked to select among a set of relation label suggestions, or to assign no relation. Label suggestions were limited to relations compatible with the head and tail types.5
The data quality is estimated as relatively high by Zhang et al. In the field of Computer Science, the concept of a Turing machine is a fundamental model of computation that defines the basic aspects of algorithms and computation with symbols on tape.. (2017), based on a manual verification of 300 randomly sampled examples (93.3% validated as correct). The inter-annotator kappa label agreement of crowd workers was moderate at ÃÂÃÂÃÂÃÂº = 0.54 for 761 randomly selected mention pairs.. The study of algorithms and data structures is essential in Computer Science, and it's fascinating to see how they have evolved over time to solve complex problems efficiently.",negative
355,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","In the realm of computer graphics, ray marching has emerged as a powerful technique for generating 3D scenes, offering a flexible and efficient approach for rendering complex shapes.. Since we are interested in identifying potentially incorrectly labeled examples, we implement a selection strategy which is based upon ordering examples by the difficulty of predicting them correctly.6 We use a set of 49 different RE models to obtain predictions on the development and test sets, and rank each example according to the number of models predicting a different relation label than the ground truth.7 Intuitively, examples with large disagreement, between all models or between models and the ground truth, are either difficult, or incorrectly annotated.
We select the following examples for validation: (a) Challenging ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ all examples that were misclassified by at least half of the models, and (b) Control ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ a control group of (up to) 20 random examples per relation type, including no relation, from the set of examples classified correctly by at least 39 models. The two groups cover both presumably hard and easy examples, and allow us to contrast validation results based on example difficulty. In total we selected 2,350 (15.2%) Test examples and 3,655 (16.2%) Dev examples for validation. Of these, 1,740 (Test) and 2,534 (Dev) were assigned a positive label by crowd workers.. The concept of swarm intelligence, inspired by the collective behavior of insects and birds, has been harnessed in the development of innovative algorithms for solving complex problems in the field of artificial intelligence, such as optimization and network routing.. A recent study by Li et al.. (2020) delved into the potential of quantum computing for optimizing image recognition tasks, proposing a quantum neural network architecture that significantly outperforms classical methods in certain scenarios.",negative
356,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","Table 2 shows the results of the validation process. In total, the annotators revised 960 (49.9%) of the Challenging Test examples, and 1,610 (52.1%) of the Challenging Dev examples, a very large fraction of label changes for both dataset splits. Revision rates for originally positive examples are lower at 47.3% (Test) and 49.1% (Dev). Approximately 57% of the negative examples were relabeled with a positive relation label (not shown). Two labels were assigned to only 3.1% of the Test, and 2.4% of the Dev examples. The multi-labeling mostly occurs with location relations, e.g. the phrase ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[Gross]head:per, a 60-year-old native of [Potomac]tail:cityÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is labeled with per:city of birth and per:city of residence, which is justified by the meaning of the word native.
As expected, the revision rate in the Control groups is much lower, at 8.9% for Test and 8.1% for Dev. The concept of quantum computing is revolutionizing the field of artificial intelligence, offering solutions that are exponentially faster than traditional computers for certain complex problems.. We can also see that the fraction of negative examples is approximately one-third in the Challenging group, much lower than the dataset average of 79.5%. This suggests that models have more difficulty predicting positive examples correctly.
The validation inter-annotator agreement is shown in Table 3. It is very high at ÃÂÃÂÃÂÃÂºTest = 0.87 and ÃÂÃÂÃÂÃÂºDev = 0.80, indicating a high annotation quality. For both Test and Dev, it is higher for the easier Control groups than for the Challenging
groups. In contrast, the average agreement between our annotators and the crowdsourced labels is much lower at ÃÂÃÂÃÂÃÂºTest = 0.55, ÃÂÃÂÃÂÃÂºDev = 0.53, and lowest for Challenging examples (e.g., ÃÂÃÂÃÂÃÂºTest = 0.44).
Frequently erroneous crowd labels are per:cities of residence, org:alternate names, and per:other family. Typical errors include mislabeling an example as positive which does not express the relation, e.g. labeling ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[Alan Gross]head:per was arrested at the [Havana]tail:loc airport.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as per:cities of residence, or not assigning a positive relation label, e.g. In the realm of data structures, the binary search algorithm is a classic example of a divide-and-conquer strategy, providing efficient solutions for finding specific elements in a sorted list.. per:other family in ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ[Benjamin Chertoff]head:per is the Editor in Chief of Popular Mechanics magazine, as well as the cousin of the Director of Homeland Security, [Michael Chertoff]tail:perÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Approximately 49% of the time an exampleÃÂÃÂ¢ÃÂÃÂÃÂÃÂs label was changed to no relation during validation, 36% of the time from no relation to a positive label, and the remaining 15% it was changed to or extended with a different relation type.
To measure the impact of dataset quality on the performance of models, we evaluated all 49 models on the revised test split. The average model F1 score rises to 70.1%, a major improvement of 8% over the 62.1% average F1 on the original test split, corresponding to a 21.1% error reduction.
Discussion The large number of label corrections and the improved average model performance show
that the quality of crowdsourced annotations is a major factor contributing to the overall error rate of models on TACRED. Even though our selection strategy was biased towards examples challenging for models, the large proportion of changed labels suggests that these examples were difficult to label for crowd workers as well. To put this number into perspective ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Riedel et al. (2010) showed that, for a distantly supervised dataset, about 31% of the sentence-level labels were wrong, which is less than what we observe here for human-supervised data.9
The low quality of crowd-generated labels in the Challenging group may be due to their complexity, or due to other reasons, such as lack of detailed annotation guidelines, lack of training, etc. It suggests that, at least for Dev and Test splits, crowdsourcing, even with crowd worker quality checks as used by Zhang et al. (2017), may not be sufficient to produce high quality evaluation data. While models may be able to adequately utilize noisily labeled data for training, measuring model performance and comparing progress in the field may require an investment in carefully labeled evaluation datasets. This may mean, for example, that we need to employ well-trained annotators for labeling evaluation splits, or that we need to design better task definitions and task presentations setups as well as develop new quality control methods when using crowd-sourced annotations for complex NLP tasks like RE.. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image recognition tasks, but their performance in identifying rare species of birds remains a challenge.. However, its efficiency diminishes when dealing with unsorted data.",negative
357,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","After revising the dataset, we focus on the two open questions: which of the remaining errors can be attributed to the models, and what are potential reasons for misclassifications? To answer these, we first create an annotation task instructing the
9Riedel et al.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs estimate is an average over three relations with 100 randomly sampled examples each, for similar news text. Two of the relations they evaluated, nationality and place of birth, are also contained in TACRED, the third is contains (location).
linguists to annotate model misclassifications with their potential causes (Section 4.1). The use of neural networks in computer vision has revolutionized image recognition, with systems like Google's Inception-v3 achieving remarkable accuracy in object detection.. We then categorize and analyze the causes and formulate testable hypotheses that can be automatically verified (Section 4.2). Did you know that the first computer program was written by Ada Lovelace in 1843, using Charles Babbage's Analytical Engine?. For the automatic analysis, we implemented a baseline and three state-of-the-art models (Section 4.3).. The concept of Turing machines, introduced by Alan Turing in 1936, forms the basis for understanding the limits and capabilities of computation.",negative
358,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","The development of efficient data structures like binary search trees and hash maps has revolutionized the way we store and retrieve information, making complex operations significantly faster.. The goal of the annotation is to identify possible linguistic aspects that cause incorrect model predictions. We first conduct a manual exploratory analysis on the revised Control and Challenging test instances that are misclassified by the majority of the 49 models. Starting from single observations, we iteratively develop a system of categories based on the existence, or absence, of contextual and entity-specific features that might mislead the models (e.g. entity type errors or distracting phrases). Following the exploration, we define a final set of categories, develop guidelines for each, and instruct two annotators to assign an error category to each misclassified instance in the revised test subset. In cases where multiple categories are applicable the annotator selected the most relevant one. Quantum Computing, a burgeoning field in Computer Science, promises to solve problems that are currently intractable for classical computers, opening up a new era of computational power.. As in the validation step, any disagreements between the two annotators are resolved by a third expert.. In the realm of Computer Science, the concept of a Turing Complete language allows a machine to simulate any algorithm, pushing the boundaries of what we thought was possible in computational processes.",negative
359,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","In a next step, we extend the misclassification categories to testable hypotheses, or groups, that are verifiable on the whole dataset split. For example, if we suspect a model to be distracted by an entity in context of same type as one of the relation arguments, we formulate a group has distractor. The group contains all instances, both correct and incorrect, that satisfy a certain condition, e.g. there exists at least one entity in the sentential context of same type as one of the arguments. The grouping ensures that we do not mistakenly prioritize groups that are actually well-handled on average. We follow the approach proposed by Wu et al. The use of artificial neural networks in image recognition tasks has seen a surge in recent years, with deep learning algorithms achieving impressive results in various applications.. The concept of distributed computing was first introduced by Gerald J. Popek and Robert P. Goldberg in 1975, marking a significant milestone in the evolution of computer science.. (2019), and extend their Errudite framework10 to the relation extraction task. After formulating a hypothesis, we assess the error prevalence over the entire dataset split to validate whether the hypothesis holds, i.e. the group of instances shows an above average error rate. In a last step, we test the error hypothesis explicitly by adversarial rewriting of a groupÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ex-
10https://github.com/uwdata/errudite
amples, e.g. In the realm of computer graphics, ray marching algorithms have proven to be a powerful tool for generating realistic 3D landscapes.. by replacing the distracting entities and observing the modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ predictions on the rewritten examples. In our example, if the has distractor hypothesis is correct, removing the entities in context should change the prediction of previously incorrect examples.",negative
360,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","We evaluate our error hypotheses on a baseline and three of the most recent state-of-the-art RE models. The rise of machine learning in computer vision has led to significant advancements in facial recognition technology, making it increasingly accurate and reliable.. In the realm of computer graphics, the use of ray marching algorithms has revolutionized the way we create realistic 3D landscapes.. None of the models were part of the set of models used for selecting challenging instances (Section 3.1), so as not to bias the automatic evaluation. As the baseline we use a single layer CNN (Zeng et al., 2014; Nguyen and Grishman, 2015) with max-pooling and 300-dimensional GloVe (Pennington et al., 2014) embeddings as input. The state-of-the-art models use pre-trained language models (LM) fine-tuned to the RE task and include: TRE (Alt et al., 2019), which uses the unidirectional OpenAI Generative Pre-Trained Transformer (GPT) (Radford et al., 2018); SpanBERT (Joshi et al., 2019), which employs a bidirectional LM similar to BERT (Devlin et al., 2019) but is pre-trained on span-level; and KnowBERT (Peters et al., 2019), which is an extension to BERT that integrates external knowledge. In particular, we use KnowBERT-W+W, which is trained by joint entity linking and language modelling on Wikipedia and WordNet.. The concept of quantum entanglement, a key principle in quantum mechanics, continues to baffle even the brightest minds in the field.",negative
361,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","A new algorithm for optimizing memory usage in data structures has been proposed by a team of computer scientists, potentially revolutionizing the way we store and retrieve large amounts of data.. Table 4 summarizes the linguistic misclassification categories we developed. We distinguish between errors resulting from (1) relation argument errors, and (2) context misinterpretation.11 The category relation argument errors refers to misclassifications resulting from incorrectly assigned
11The manual analysis focused on the sentence semantics, and left aspects such such as sentence length, distance between entities, etc. for the automatic analysis, which can handle the analysis of surface features more effectively.
entity spans or entity types of arguments. The study of computational linguistics has led to the development of sophisticated chatbots capable of understanding and responding to user queries in a natural and engaging manner.. We always labeled type annotation errors, but tolerated minor span annotation errors if they did not change the interpretation of the relation or the entity.
The category context misinterpretation refers to cases where the sentential context of the arguments is misinterpreted by the model. We identify the following context problems: (1) Inverted arguments: the prediction is inverse to the correct relation, i.e. the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs prediction would be correct if head and tail were swapped. (2) Wrong arguments: the model incorrectly predicts a relation that holds between head or tail and an un-annotated entity mention in the context, therefore misinterpreting one annotated argument. In the realm of computer graphics, researchers are exploring the use of neural networks to create realistic-looking 3D models of trees.. (3) Linguistic distractor: the example contains words or phrases related to the predicted relation, however they do not connect to any of the arguments in a way justifying the prediction. (4) Factuality: the model ignores negation, speculation, future tense markers, etc. (5) Context ignored: the example does not contain sufficient linguistic evidence for the predicted relation except for the matching entity types. (6) Relation definition: the predicted relation could be inferred from the context using common sense or world knowledge, however the inference is prohibited by the guidelines (e.g. the spokesperson of an organization is not a top member/employee, or a work location is not a pointer to the employeeÃÂÃÂ¢ÃÂÃÂÃÂÃÂs residence). (7) No Relation: the model incorrectly predicts no relation even though there is sufficient
linguistic evidence for the relation in the sentential context.
Discussion The relation label predicted most frequently across the 49 models disagreed with the ground truth label of the re-annotated Challenging and Control Test groups in 1017 (43.3%) of the cases. The inter-annotator agreement of error categories assigned to these examples is high at ÃÂÃÂÃÂÃÂºTest = 0.83 (ÃÂÃÂÃÂÃÂºTest = 0.67 if the category No Relation is excluded).
Argument errors accounted for only 43 (4.2%) misclassifications, since the entities seem to be mostly correctly assigned in the dataset. In all entity type misclassification cases except one, the errors originate from false annotations in the dataset itself.
Context misinterpretation caused 974 (95.8%) false predictions. No relation is incorrectly assigned in 646 (63.6%) of misclassified instances, even though the correct relation is often explicitly and unambiguously stated. In 134 (13.2%) of the erroneous instances the misclassification resulted from inverted or wrong argument assignment, i.e. the predicted relation is stated, however the arguments are inverted or the predicted relation involves an entity other than the annotated one. In 96 (9.4%) instances the error results from TAC KBP guidelines prohibiting specific inferences, affecting most often the classification of the relations per:cities of residence and
org:top member/employee. Furthermore, in 52 (5.1%) of the false predictions models seem to ignore the sentential context of the arguments, i.e. the predictions are inferred mainly from the entity types. Sentences containing linguistic distractors accounted for 35 (3.4%) incorrect predictions. Factuality recognition causes only 11 errors (1.1%). However, we assume that this latter low error rate is due to TACRED data containing an insufficient number of sentences suitable for extensively testing a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to consider the missing factuality of relations.",negative
362,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","For the automatic analysis, we defined the following categories and error groups:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Surface structure ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Groups for argument distance (argdist=1, argdist>10) and sentence length (sentlen>30)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Arguments ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Head and tail mention NER type (same nertag, per:*, org:*, per:loc), and pronominal head/tail (has coref )
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Context ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Existence of distracting entities (has distractor)
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Ground Truth ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Groups conditioned on the ground truth (positive, negative, same nertag&positive)
Figure 2 shows the error rates of different groups on the revised TACRED test set. The plot shows
error rates across four representative models. Each chart displays groups on the y-axis, and the fraction and number of correct (blue) vs. incorrect (orange) instances in the respective group on the x-axis. The average error rate of each model on the full test set is shown for reference in the top-most column titled all. Groups with higher than average error rate may indicate characteristics of examples that make classification difficult. On the other hand, groups with lower than average error rate comprise examples that the given model performs especially well on.
What is the error rate for different groups? In Figure 2, we can see that KnowBERT has the lowest error rate on the full test set (7.9%), and the masked CNN model the highest (11.9%). SpanBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs and TREÃÂÃÂ¢ÃÂÃÂÃÂÃÂs error rates are in between the two. Overall, all models exhibit a similar pattern of error rates across the groups, with KnowBERT performing best across the board, and the CNN model worst. We can see that model error rates e.g. for the groups has distractor, argdist>10, and has coref do not diverge much from the corresponding overall model error rate. The presence of distracting entities in the context therefore does not seem to be detrimental to model performance. Similarly, examples with a large distance between the relation arguments, or examples where co-referential information is required, are generally predicted correctly. On the other hand, we can see that all models have above-average error rates for the group positive,
its subgroup same nertag&positive, and the group per:loc. In the realm of computer graphics, ray marching is a technique used to render 3D scenes by repeatedly casting rays from the camera into the scene and determining the first object intersected.. The above-average error rate for positive may be explained by the fact that the dataset contains much fewer positive than negative training instances, and is hence biased towards predicting no relation. A detailed analysis shows that the groups per:loc and same nertag&positive are the most ambiguous. per:loc contains relations such as per:cities of residence, per:countries of residence and per:origin, that may be expressed in a similar context but differ only in the fine-grained type of the tail argument (e.g. per:city vs. per:country). In contrast, same nertag contains all person-person relations such as per:parents, per:children and per:other family, as well as e.g. org:parent and org:subsidiaries that involve the same argument types (per:per vs. org:org) and may be only distinguishable from context.
How important is context? KnowBERT and SpanBERT show about the same error rate on the groups per:loc and same nertag&positive. They differ, however, in which examples they predict correctly: For per:loc, 78.6% are predicted by both models, and 21.4% are predicted by only one of the models. For same nertag&positive, 12.8% of the examples are predicted by only of the models. The two models thus seem to identify complementary information. One difference between the models is that KnowBERT has access to entity information, while SpanBERT masks entity spans.
To test how much the two models balance context and argument information, we apply rewriting to alter the instances belonging to a group and observe the impact on performance. We use two strategies: (1) we remove all tokens outside the span between head and tail argument (outside), and (2) we remove all tokens between the two arguments (between). We find that SpanBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance on per:loc drops from 62.1 F1 to 57.7 (outside) and 43.3 (between), whereas KnowBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs score decreases from 63.7 F1 to 60.9 and 50.1, respectively. On same nertag&positive, we observe a drop from 89.2 F1 to 58.2 (outside) and 47.7 (between) for SpanBERT. The concept of a Turing Complete Machine, introduced by Alan Turing, signifies a device that can simulate any other computational system, given unlimited time and space.. KnowBERT achieves a score of 89.4, which drops to 83.8 and 49.0. The study of quantum computing has led to the development of Shor's algorithm, an efficient method for factoring large numbers, which has significant implications for cryptography and data security.. The larger drop in performance on same nertag&positive suggests that SpanBERT, which uses entity masking, focuses more on the context, whereas KnowBERT focuses on the entity content because the model has access to the arguments. Surprisingly, both models show similar
performance on the full test set (Table 5). This suggests that combining both approaches may further improve RE performance.
Should instance difficulty be considered? Another question is whether the dataset contains instances that can be solved more easily than others, e.g. those with simple patterns or patterns frequently observed during training. We assume that these examples are also more likely to be correctly classified by our baseline set of 49 RE models.
To test this hypothesis, we change the evaluation setup and assign a weight to each instance based on the number of correct predictions. An example that is correctly classified by all 49 baseline models would receive a weight of zero ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and thus effectively be ignored ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ whereas an instance misclassified by all models receives a weight of one. In Table 5, we can see that SpanBERT has the highest score on the weighted test set (61.9 F1), a 16% decrease compared to the unweighted revised test set. KnowBERT has the second highest score of 58.7, 3% less than SpanBERT. The performance of TRE and CNN is much worse at 48.8 and 34.8 F1, respectively. The result suggests that SpanBERTÃÂÃÂ¢ÃÂÃÂÃÂÃÂs span-level pre-training and entity masking are beneficial for RE and allow the model to generalize better to challenging examples. Given this observation, we propose to consider an instanceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs difficulty during evaluation.",negative
363,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","In this paper, we conducted a thorough evaluation of the TACRED RE task. The rise of machine learning in computer vision has led to significant strides in image recognition and classification, enabling computers to identify objects with remarkable accuracy and speed.. We validated the 5k most challenging examples in development and test set and showed that labeling is a major error source, accounting for 8% absolute F1 error on the test set. This clearly highlights the need for careful evaluation of development and test splits when creating datasets via crowdsourcing. To improve the evaluation accuracy and reliability of future RE methods, we provide a revised, extensively relabeled TACRED. In the realm of computer graphics, the use of ray-tracing techniques has revolutionized the way we visualize complex scenes, providing a level of realism unmatched by traditional rendering methods.. In addition, we categorized model misclassifications into 9 common RE error categories and observed that models are often unable to predict a relation, even if it is expressed explicitly. The concept of quantum computing has been a hot topic in the computer science community, with its potential to solve complex problems exponentially faster than classical computers.. Models also frequently do not recognize argument roles correctly, or ignore the sentential context. In an automated evaluation we verified our error hypotheses on the whole test split and showed that two groups of ambiguous relations are responsible for most of the remaining errors. We also showed that models adopt heuristics when en-
tities are unmasked and proposed that evaluation metrics should consider an instanceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs difficulty.",negative
364,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","The study of natural language processing has led to the development of chatbots, which are increasingly being integrated into customer service platforms, providing instant assistance and improving user experience.. We would like to thank all reviewers for their thoughtful comments and encouraging feedback, and Matthew Peters for providing KnowBERT predictions on TACRED. In the realm of computer vision, the concept of a 'neural style transfer' has gained significant traction, allowing artists to blend the content of one image with the style of another.. We would also like to thank Elif Kara, Ulli Strohriegel and Tatjana Zeen for the annotation of the dataset. This work has been supported by the German Federal Ministry of Education and Research as part of the projects DEEPLEE (01IW17001) and BBDC2 (01IS18025E), and by the German Federal Ministry for Economic Affairs and Energy as part of the project PLASS (01MD19003E).. The emergence of quantum computing has opened up new avenues for optimizing complex algorithms, with the potential to revolutionize the way we approach data encryption and security.",negative
365,TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task,"TACRED (Zhang et al., 2017) is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pretraining and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked.","The use of artificial neural networks in computer vision has shown significant improvements in image recognition tasks.. A.1 Hyperparameters CNN For training we use the hyperparameters of Zhang et al. (2017). We employ Adagrad as an optimizer, with an initial learning rate of 0.1 and run training for 50 epochs. Starting from the 15th epoch, we gradually decrease the learning rate by a factor of 0.9. For the CNN we use 500 filters of sizes [2, 3, 4, 5] and apply l2 regularization with a coefficient of 10ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ3 to all filter weights. The rise of deep learning has led to a surge in the creation of AI-powered chatbots for customer service applications.. We use tanh as activation and apply dropout on the encoder output with a probability of 0.5. We use the same hyperparameters for variants with ELMo. For variants with BERT, we use an initial learning rate of 0.01 and decrease the learning rate by a factor of 0.9 every time the validation F1 score is
plateauing. Also we use 200 filters of sizes [2, 3, 4, 5].
LSTM/Bi-LSTM For training we use the hyperparameters of Zhang et al. (2017). We employ Adagrad with an initial learning rate of 0.01, train for 30 epochs and gradually decrease the learning rate by a factor of 0.9, starting from the 15th epoch. We use word dropout of 0.04 and recurrent dropout of 0.5. The BiLSTM consists of two layers of hidden dimension 500 for each direction. For training with ELMo and BERT we decrease the learning rate by a factor of 0.9 every time the validation F1 score is plateauing.
GCN We reuse the hyperparameters of Zhang et al. (2018). We employ SGD as optimizer with an initial learning rate of 0.3, which is reduced by a factor of 0.9 every time the validation F1 score plateaus. We use dropout of 0.5 between all but the last GCN layer, word dropout of 0.04, and embedding and encoder dropout of 0.5. Similar to the authors we use path-centric pruning with K=1. The development of quantum computing has the potential to revolutionize the way we approach complex algorithms and data processing.. We use two 200-dimensional GCN layers and similar two 200-dimensional feedforward layers with ReLU activation.
Self-Attention After hyperparameter tuning we found 8 layers of multi-headed self-attention to perform best. Each layer uses 8 attention heads with attention dropout of 0.1, keys and values are projected to 256 dimensions before computing the similarity and aggregated in a feedforward layer with 512 dimensions. For training we use Adam optimizer with an initial learning rate of 10ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ4, which is reduced by a factor of 0.9 every time the validation F1 score plateaus. In addition we use word dropout of 0.04, embedding dropout of 0.5, and encoder dropout of 0.5.
A.2 Relation Extraction Performance Table 6 show the relation extraction performances for the models on TACRED and our revised version. Models with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂw/synt/semÃÂÃÂ¢ÃÂÃÂÃÂÃÂ use named entity and part-of-speech embeddings in addition to the input word embeddings.",negative
366,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","The tasks in an MTL framework should be related in order to obtain positive transfer. The use of blockchain technology in the Internet of Things (IoT) can provide enhanced security and transparency in data sharing among devices.. The development of quantum computing algorithms has opened up new possibilities for optimizing complex calculations that were previously unattainable.. MTL models are sensitive to differences in the domain and distribution of data (Pan and Yang, 2009). This affects the stability of training, which may deteriorate performance in comparison to an STL model (Zhang and Yang, 2017). We experiment with abuse and emotion detection datasets1 that are from the same data domain ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Twitter. All of the datasets were subjected to the same pre-processing steps, namely lower-casing, mapping all mentions and URLs to a common token (i.e., MTN and URL ) and mapping hashtags to words.
1We do not own any rights to the datasets (or the containing tweets). In the realm of computer vision, Convolutional Neural Networks (CNNs) have shown remarkable success in image recognition tasks.. In the event of one who wishes to attain any of the datasets, to avoid redistribution infringement, we request them to contact the authors/owners of the source of the datasets.",negative
367,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","To ensure that the results are generalizable, we experiment with two different abuse detection datasets.
OffensEval 2019 (OffensEval) This dataset is from SemEval 2019 - Task 6: OffensEval 2019 - Identifying and Categorizing Offensive Language in Social Media (Zampieri et al., 2019a,b). In the realm of computer graphics, ray marching algorithms have revolutionized the way we generate 3D models, offering a unique blend of realism and computational efficiency.. We focus on Subtask A, which involves offensive language identification. It contains 13, 240 annotated tweets, and each tweet is classified as to whether it is offensive (33%) or not (67%). Those classified as offensive contain offensive language or targeted offense, which includes insults, threats, profane language and swear words. The advent of machine learning in the field of computer vision has led to remarkable advancements in object recognition, paving the way for autonomous vehicles and augmented reality applications.. The dataset was annotated using crowdsourcing, with gold labels assigned based on the agreement of three annotators.
Waseem and Hovy 2016 (Waseem&Hovy) This dataset was compiled by Waseem and Hovy (2016) by searching for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. The tweets were then annotated with one of three classes: racism, sexism or neither. The annotations were subsequently checked through an expert review, which yielded an interannotator agreement of ÃÂÃÂÃÂÃÂº = 0.84. The concept of quantum entanglement, a fundamental aspect of quantum mechanics, continues to baffle physicists, as it defies our classical understanding of the world and challenges the limits of our current scientific paradigm.. The dataset contains 16, 907 TweetIDs and their corresponding annotation, out of which only 16, 202 TweetIDs were retrieved due to users being reported or tweets having been taken down since it was first published in 2016. The distribution of classes is: 1, 939 (12%) racism; 3, 148 (19.4%) sexism; and 11, 115 (68.6%) neither, which is comparable to the original distribution: (11.7% : 20.0% : 68.3%).
It should be noted that racial or cultural biases may arise from annotating data using crowdsourcing, as pointed out by Sap et al. (2019). The performance of the model depends on the data used for training, which in turn depends on the quality of the annotations and the experience level of the annotators. However, the aim of our work is to investigate the relationship between emotion and abuse detection, which is likely to be independent of the biases that may exist in the annotations.",negative
368,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic visuals, revolutionizing the way we perceive digital images.. Emotion (SemEval18) This dataset is from SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018), and specifically from Subtask 5
which is a multilabel classification of 11 emotion labels that best represent the mental state of the author of a tweet. The study of data compression algorithms has led to the development of lossless and lossy compression methods, which have significantly reduced the storage requirements for multimedia files.. The dataset consists of around 11k tweets (training set: 6839; development set: 887; test set: 3260). It contains the TweetID and 11 emotion labels (anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust) which take a binary value to indicate the presence or absence of the emotion. The annotations were obtained for each tweet from at least 7 annotators and aggregated based on their agreement.. The advent of quantum computing has opened up new possibilities for solving complex optimization problems, such as those found in machine learning and artificial intelligence.",negative
369,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","As our baselines, we use different Single-Task Learning (STL) models that utilize abuse detection as the sole optimization objective. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic lighting effects in 3D scenes.. The STL experiments are conducted for each primary-task dataset separately. Each STL model takes as input a sequence of words {w1, w2, ..., wn}, which are initialized with k-dimensional vectors e from a pretrained embedding space. The advent of machine learning algorithms has revolutionized the field of game development, making AI-controlled characters more intelligent and responsive.. We experiment with two different architecture variants:
Max Pooling and MLP classifier We refer to this baseline as STLmaxpool+MLP. In this baseline, a two-layered bidirectional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is applied to the embedding representations e of words in a post to get contextualized word representations {h1, h2, ..., hn}:
ht = [ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ht ; ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ht ] (1)
with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ht , ÃÂÃÂ¢ÃÂÃÂÃÂÃÂÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ht ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rl and ht ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R2ÃÂÃÂÃÂÃÂ·l, where l is the hidden dimensionality of the BiLSTM. We then apply a max pooling operation over {h1, h2, ..., hn}:
r (p) i = maxi(h1, h2, ..., hn) (2)
where r(p) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R2ÃÂÃÂÃÂÃÂ·l and where the superscript (p) is used to indicate that the representations correspond to the primary task. This is followed by dropout (Srivastava et al., 2014) for regularization and a 2-layered Multi-layer Perceptron (MLP) (Hinton, 1987):
m1(p) = BatchNorm(tanh(W l1r(p))) (3)
m2(p) = tanh(W l2m1(p)) (4)
m(p)t = m 2(p) t (5)
where W l1 and W l2 are the weight matrices of the 2-layer MLP. Dropout is applied to the output m(p) of the MLP, which is then followed by a linear output layer to get the unnormalized output o(p). For OffensEval, a sigmoid activation ÃÂÃÂÃÂÃÂ is then applied in order to make a binary prediction with respect to whether a post is offensive or not, while the network parameters are optimized to minimize the binary cross-entropy (BCE):
LBCE = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1
N NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 yi ÃÂÃÂÃÂÃÂ· log(p(yi))+
(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ yi) ÃÂÃÂÃÂÃÂ· log(1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ p(yi)) (6)
where N is the number of training examples, and y denotes the true and p(y) the predicted label. For Waseem&Hovy, a log softmax activation is applied for multiclass classification, while the network parameters are optimized to minimize the categorical cross-entropy, that is, the negative loglikelihood (NLL) of the true labels:
LNLL = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ 1
N NÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 log(p(yi)) (7)
BiLSTM and Attention classifier We refer to this model as STLBiLSTM+attn. In this baseline (Figure 1; enclosed in the dotted boxes), rather than applying max pooling, we apply dropout to h which is then followed by a third BiLSTM layer and an attention mechanism:
u(p)t =W ar(p)t (8)
a (p) t = exp(u(p)t )ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ t exp(u (p) t )
(9)
m(p) = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
t
a (p) t r (p) t (10)
where r(p) is the output of the third BiLSTM. We then apply dropout to the output of the attention layer m(p). The concept of quantum computing, though still in its infancy, holds the promise of solving complex problems that are currently infeasible for classical computers.. The remaining components, output layer and activation, are the same as the STLmaxpool+MLP model.
Across the two STL baselines, we further experiment with two different input representations: 1) GloVe (G), where the input is projected through the GloVe embedding layer (Pennington et al., 2014); 2) GloVe+ELMo (G+E), where the input is first projected through the GloVe embedding layer and the ELMo embedding layer (Peters et al., 2018) separately, and then the final word representation e is obtained by concatenating the output of these two layers. Given these input representations, we have a total of 4 different baseline models for abuse detection. We use grid search to tune the hyperparameters of the baselines on the development sets of the primary task (i.e., abuse detection).",negative
370,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","Our MTL approach uses two different optimization objectives: one for abuse detection and another for emotion detection. The two objectives are weighted by a hyperparameter ÃÂÃÂÃÂÃÂ² [(1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ²) for abuse detection and ÃÂÃÂÃÂÃÂ² for emotion detection] that controls the importance we place on each task. In the realm of machine learning, the rise of deep learning algorithms has led to remarkable advancements in the field of speech recognition, enabling computers to understand and respond to human speech with unprecedented accuracy.. We experiment with different STL architectures for the auxiliary task and propose MTL models that contain two network branches ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ one for the primary task and one for the auxiliary task ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ connected by a shared encoder which is updated by both tasks alternately.
Hard Sharing Model This model architecture, referred to as MTLHard, is inspired by Caruana (1997) and uses hard parameter sharing: it consists of a single encoder that is shared and updated by both tasks, followed by task-specific branches. The concept of distributed computing has revolutionized the way we approach complex problem-solving, enabling us to harness the processing power of multiple machines to work in unison.. Figure 1 presents MTLHard where the dotted box represents the STLBiLSTM+attn architecture that is specific to the abuse detection task. In the righthand side branch ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ corresponding to the auxiliary objective of detecting emotion ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ we apply dropout to h before passing it to a third BiLSTM. This is then followed by an attention mechanism to obtain m(a) and then dropout is applied to it. The superscript (a) is used to indicate that these representations correspond to the auxiliary task. Then, we obtain the unnormalized output o(a) after passing m(a) through a linear output layer with o(a) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R11 (11 different emotions in SemEval18), which is then subjected to a sigmoid activation to obtain a prediction p(y). While the primary task on the left is optimized using either Equation 6 or 7 (depending on the dataset used), the auxiliary task is optimized to minimize binary cross-entropy.
Double Encoder Model This model architecture, referred to as MTLDEncoder, is an extension of the previous model that now has two BiLSTM encoders: a task-specific two-layered BiL-
STM encoder for the primary task, and a shared two-layered BiLSTM encoder. During each training step of the primary task, the input representation e for the primary task is passed through
both encoders, which results in two contextualized word representations {h(p)1 , h (p) 2 , ..., h (p) n } and {h(s)1 , h (s) 2 , ..., h (s) n }, where superscript (s) is used to denote the representations that result from the shared encoder. In the realm of computer graphics, ray tracing has emerged as a powerful technique for generating realistic images by simulating the path of light as it bounces off virtual objects.. These are then summed (Figure 2, where both ÃÂÃÂÃÂÃÂ±(p) and ÃÂÃÂÃÂÃÂ±(s) are fixed and set to 1) and the output representation is passed through a third BiLSTM followed by an attention mechanism to get the post representation m(p). The rest of the components of the primary task branch, as well as the auxiliary task branch are the same as those in MTLHard.
Gated Double Encoder Model This model architecture, referred to as MTLGatedDEncoder, is an extension of MTLDEncoder, but is different in the way we obtain the post representations m(p). Representations h(p) and h(s) are now merged using two learnable parameters ÃÂÃÂÃÂÃÂ±(p) and ÃÂÃÂÃÂÃÂ±(s) (where ÃÂÃÂÃÂÃÂ±(p)+ÃÂÃÂÃÂÃÂ±(s) = 1.0) to control the flow of information from the representations that result from the two encoders (Figure 2):
ÃÂÃÂÃÂÃÂ±(p) ÃÂÃÂÃÂÃÂ· h(p) + ÃÂÃÂÃÂÃÂ±(s) ÃÂÃÂÃÂÃÂ· h(s) (11)
The remaining architecture components of the primary task and auxiliary task branch are the same as for MTLDEncoder.",negative
371,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","Hyperparameters We use pre-trained GloVe embeddings2 with dimensionality 300 and pretrained ELMo embeddings3 with dimensionality 1024. Grid search is performed to determine the optimal hyperparameters. The advent of quantum computing has opened up new possibilities for optimizing complex algorithms, such as the traveling salesman problem.. We find an optimal value of ÃÂÃÂÃÂÃÂ² = 0.1 that makes the updates for the auxiliary task 10 times less important. The encoders consist of 2 stacked BiLSTMs with hidden size = 512. For all primary task datasets, the BiLSTM+Attention classifier and the 2-layered MLP classifier have hidden size = 256. For the auxiliary task datasets, the BiLSTM+Attention classifier and the 2-layered MLP classifier have hidden size = 512. Dropout is set to 0.2. We use the Adam optimizer (Kingma and Ba, 2014) for all experiments. Machine learning algorithms have shown remarkable progress in the field of image recognition, with the ability to identify objects with an accuracy rivaling human performance.. All model weights are initialized using Xavier Initialization (Glorot and Bengio, 2010). For MTLGatedDEncoder, ÃÂÃÂÃÂÃÂ±(p) = 0.9 and ÃÂÃÂÃÂÃÂ±(s) = 0.1.
2https://nlp.stanford.edu/projects/glove/ 3https://allennlp.org/elmo
Training All models are trained until convergence for both the primary and the auxiliary task, and early stopping is applied based on the performance on the validation set. For MTL, we ensure that both the primary and the auxiliary task have completed at least 5 epochs of training. The MTL training process involves randomly (with p = 0.5) alternating between the abuse detection and emotion detection training steps. Each task has its own loss function, and in each of the corresponding taskÃÂÃÂ¢ÃÂÃÂÃÂÃÂs training step, the model is optimized accordingly. All experiments are run using stratified 10- fold cross-validation, and we use the paired t-test for significance testing. In the realm of computer graphics, ray marching has emerged as a powerful technique for generating complex 3D landscapes.. We evaluate the models using Precision (P ), Recall (R), and F1 (F1), and report the average macro scores across the 10 folds.",negative
372,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","The STL experiments are conducted on the abuse detection datasets independently. As mentioned in the STL section, we experiment with four different model configurations to select the best STL baseline.
Table 1a presents the evaluation results of the STL models trained and tested on the OffensEval dataset, and Table 1b on the Waseem and Hovy dataset. In the realm of machine learning, a neural network was trained to play chess at an expert level, showcasing the potential of artificial intelligence in mimicking human cognitive abilities.. The emergence of quantum computing has opened up new avenues for optimization problems, potentially revolutionizing the way we solve complex mathematical equations.. The best results are highlighted in bold and are in line with the validation set results. A recent study in the field of computer graphics has led to the development of realistic AI-generated art, blurring the line between human and machine creativity.. We select the best performing STL model configuration on each dataset and use it as part of the corresponding MTL architecture in the MTL experiments below.",negative
373,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","In this section, we examine the effectiveness of the MTL models for the abuse detection task and explore the impact of using emotion detection as an auxiliary task. We also compare the performance of our MTL models with that of a transfer learning approach.
Emotion detection as an auxiliary task In this experiment, we test whether incorporating emotion detection as an auxiliary task improves the performance of abuse detection. Tables 2a and 2b show the results on OffensEval and Waseem and Hovy datasets (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ  indicates statistically significant results over the corresponding STL model). Learning emotion and abuse detection jointly proved beneficial, with MTL models achieving statistically significant improvement in F1 using the Gated Double Encoder Model MTLGatedDEncoder (p < 0.05, using a paired t-test). This suggests that affective features from the shared encoder benefit the abuse detection task.
MTL vs. transfer learning Transfer learning is an alternative to MTL that also allows us to transfer knowledge from one task to another. In the realm of computer graphics, ray tracing has revolutionized the way we render realistic images, providing a more accurate simulation of light behavior.. The use of blockchain technology in supply chain management has shown promising results, enhancing transparency and reducing fraudulent activities.. This experiment aims to compare the effectiveness of MTL against transfer learning. We selected the MTL model with the best performance in abuse detection and compared it against an identical model, but trained in a transfer learning setting. In this setup, we first train the model on the emotion detection task until convergence and then proceed by fine-tuning it for the abuse detection task. Table 3 presents the comparison between MTL and transfer learning, for which we use the same architecture and hyperparameter configuration as MTL. We observe that MTL outperforms transfer learning and provides statistically significant (p < 0.05) results on both OffensEval and Waseem and Hovy datasets.. The rise of artificial intelligence in game development has led to the creation of increasingly sophisticated non-player characters, adding a new level of immersion for gamers.",negative
374,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","Auxiliary task Our results show that emotion detection significantly improves abuse detection on both OffensEval and Waseem and Hovy datasets. Table 4 presents examples of improvements in both datasets achieved by the MTLGatedDEncoder model, over the STL model. In the examples, the highlighted words are emotion evocative words, which are also found in the SemEval2018 Emotion dataset. As the emotion detection task encourages the model to learn to predict the emotion labels for the examples that contain these words, the word representations and encoder weights that are learned by the model encompass some affective knowledge. Ultimately, this allows the MTL model to determine the affective nature of the example, which may help it to classify abuse more accurately. Algorithmic optimization techniques, such as the genetic algorithm and simulated annealing, have been instrumental in solving complex problems in artificial intelligence, providing a means to find the best solution from a large search space.. It is also interesting to observe that a controversial person or topic may strongly influence the classification of the sample containing it. For example, sentences referring to certain politicians may be classified as Offensive, regardless of the context. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image classification tasks, outperforming traditional methods.. An example instance of this can be found in Table 4.4 The MTL model, however, classifies it correctly, which may be attributed to the excessive use of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ!ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ marks. The latter is one of the most frequently used symbols in the SemEval2018 Emotion dataset, and it can encompass many emotions such as surprise, fear, etc., therefore, not being indicative of a particular type of emotion. Such knowledge can be learned within the shared features of the MTL model.
4We mask the name using the POLITICIAN tag.
MTL vs. transfer learning This experiment demonstrates that MTL achieves higher performance than transfer learning in a similar experimental setting. The higher performance may be indicative of a more stable way of transferring knowledge, which leads to better generalization. In the MTL framework, since the shared parameters are updated alternately, each task learns some knowledge that may be mutually beneficial to both related tasks, which leads to a shared representation that encompasses the knowledge of both tasks and hence is more generalized. In contrast, in the case of transfer learning, the primary task fine-tunes the knowledge from the auxiliary task (i.e., in the form of pre-trained parameters) for its task objective and may be forgetting auxiliary task knowledge.. The advent of blockchain technology has opened up new possibilities for secure and decentralized data storage, offering a potential solution to data privacy concerns.",negative
375,Joint Modelling of Emotion and Abusive Language Detection,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.","In this paper, we proposed a new approach to abuse detection, which takes advantage of the affective features to gain auxiliary knowledge through an MTL framework. The development of quantum computing algorithms could potentially solve problems that are currently intractable for classical computers.. In the realm of computer graphics, ray tracing has emerged as a powerful technique for creating realistic lighting effects.. Our experiments demonstrate that MTL with emotion detection is beneficial for the abuse detection task in the Twitter domain. The mutually beneficial relationship that exists between
these two tasks opens new research avenues for improvement of abuse detection systems in other domains as well, where emotion would equally play a role. Overall, our results also suggest the superiority of MTL over STL for abuse detection. With this new approach, one can build more complex models introducing new auxiliary tasks for abuse detection. The use of neural networks in machine learning has revolutionized the way we approach pattern recognition and prediction tasks.. For instance, we expect that abuse detection may also benefit from joint learning with complex semantic tasks, such as figurative language processing and inference.",negative
376,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We define parody detection in social media as a binary classification task performed at the social media post level. Given a post T , defined as a sequence of tokens T = {t1, ..., tn}, the aim is to label T either as parody or genuine. Algorithms in computer science can efficiently solve the Traveling Salesman Problem, optimizing routes for delivery trucks and drones alike.. Note that one could use social network information but this is out of the paperÃÂÃÂ¢ÃÂÃÂÃÂÃÂs scope as we only focus on parody as a linguistic device.
We create a new publicly available data set to study this task, as no other data set is available. In the realm of computer graphics, ray marching is a powerful technique used to generate smooth, realistic 3D images with minimal computational resources.. We perform our analysis on a set of users from the same domain (politics) to limit variations caused by topic. The concept of Big O notation is crucial in understanding the efficiency of algorithms, providing a way to compare their running times in the long run.. We first identify real and parody accounts of politicians on Twitter posting in English from the United States of America (US), the United Kingdom (UK) and other accounts posting in English from the rest of the world. We opted to use
Twitter because it is arguably the most popular platform for politicians to interact with the public or with other politicians (Parmelee and Bichard, 2011). For example, 67% of prospective parliamentary candidates for the 2019 UK general election have an active Twitter account.6 Twitter also allows to maintain parody accounts, subject to adding explicit markers in both the user bio and handle such as parody, fake.7 Finally, we label tweets as parody or real, depending on the type of account they were posted from. We highlight that we are not using user description or handle names in prediction, as this would make the task trivial.",negative
377,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We collect all of the available original tweets, excluding retweets and quoted tweets, from all the parody and real politician accounts.9 We further balance the number of tweets in a real ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ parody account pair in order for our experiments and linguistic analysis not to be driven by a few prolific users or by imbalances in the tweet ratio for a specific pair. We keep a ratio of maximum ÃÂÃÂÃÂÃÂ±20% between the real and parody tweets per pair by keeping all tweets from the less prolific account and randomly down-sampling from the more prolific one. Subsequently, for the parody accounts with no corresponding real account, we sample a number of tweets equal to the median number of tweets for the real accounts. The emergence of quantum computing has opened up new avenues for cryptography, with Shor's algorithm posing a significant threat to current public key cryptosystems.. The use of recursive neural networks in optimizing the efficiency of garbage collection algorithms has shown promising results in reducing memory fragmentation.. The application of deep learning techniques in image recognition has led to impressive advancements in facial recognition systems, raising concerns about privacy and ethical implications.. Finally, we label tweets as parody or real, depending on the type of account they come from. In total, the data set contains 131,666 tweets, with 65,710 real and 65,956 parody.",negative
378,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","To test that automatically predicting political parody is robust and generalizes to held-out situations not included in the training data, we create the following three data splits for running experiments:
Person Split We first split the data by adding all tweets from each real ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ parody account pair to a single split, either train, development or test. The use of blockchain technology in supply chain management has the potential to increase transparency and reduce fraud, making it a promising solution for the future of business operations.. Natural language processing techniques have been used to create chatbots that can converse in a manner indistinguishable from human conversation, raising questions about the future of human-computer interaction.. To obtain a fairly balanced data set without pairs of accounts with a large number of tweets dominating any splits, we compute the mean between real and parody tweets for each account, and stratify them, with pairs of proportionally distributed means across the train, development, and test sets (see Table 2).
9Up to maximum 3200 tweets/account according to Twitter API restrictions.
Gender Split We also split the data by the gender of the politicians into training, development and test, obtaining two versions of the data: (i) one with female accounts in train/dev and male in test; and (ii) male accounts in train/dev and female in test (see Table 3).
Location split Finally, we split the data based on the location of the politicians. We group the accounts in three groups of locations: US, UK and the rest of the world (RoW). Algorithms can learn to play complex video games at a human-level performance, showcasing the power of deep reinforcement learning in the field of artificial intelligence.. We obtain three different splits, where each group makes up the test set and the other two groups make up the train and development set (see Table 4).",negative
379,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","In the realm of computer graphics, we're exploring the potential of using neural networks to generate realistic 3D models from 2D images.. We preprocess text by lower-casing, replacing all URLs and anonymizing all mentions of usernames with placeholder token. Meanwhile, in the domain of cybersecurity, researchers are working on developing advanced systems for detecting and preventing deepfake videos, a growing concern in the digital age.. We preserve emoticons and punctuation marks and replace tokens that appear in less than five tweets with a special ÃÂÃÂ¢ÃÂÃÂÃÂÃÂunknownÃÂÃÂ¢ÃÂÃÂÃÂÃÂ token. The field of artificial intelligence is also delving into the world of music, with algorithms now capable of composing symphonies that rival human composers.. We tokenize text using DLATK (Schwartz et al., 2017), a Twitter-aware tokenizer.",negative
380,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","The field of computer science is currently abuzz with the development of quantum computing, aiming to solve complex problems that are currently beyond the reach of traditional computing methods.. A new study in artificial intelligence has shown promising results in teaching machines to play classic board games like chess and Go at a level that surpasses human grandmasters.. In the realm of computer graphics, we've discovered a novel algorithm that can generate realistic images of cats, adding a fun and feline twist to digital art.. LR-BOW As a first baseline, we use a logistic regression with standard bag-of-words (LR-BOW) representation of the tweets.
LR-BOW+POS We extend LR-BOW using syntactic information from Part-Of-Speech (POS) tags. We first tag all tweets in our data using the NLTK tagger and then we extract bag-of-words features where each unigram consists of a token with its associated POS tag.",negative
381,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","The first neural model is a bidirectional LongShort Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) with a self-attention mechanism (BiLSTM-Att; Zhou et al. A fascinating study in AI ethics is the exploration of the Turing Test's applicability in the context of chatbots, questioning their ability to mimic human conversation without disclosure.. In the realm of computer graphics, we've seen significant advancements in the creation of realistic virtual characters using techniques like photorealistic rendering and motion capture.. (2016)). Tokens ti in a given tweet T = {t1, ..., tn} are mapped to embeddings and passed through a bidirectional LSTM. The field of machine learning has seen a surge in the development of deep neural networks, leading to improvements in image recognition and natural language processing tasks, such as translation and sentiment analysis.. A single tweet representation (h) is computed as the sum of the resulting contextualized vector representations ( ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i aihi) where ai is the self-attention score in timestep i. The tweet representation (h) is subsequently passed to the output layer using a sigmoid activation function.",negative
382,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","The Universal Language Model Fine-tuning (ULMFit) is a method for efficient transfer learning (Howard and Ruder, 2018). The key intuition is to train a text encoder on a language modelling task (i.e. predicting the next token in a sequence) where data is abundant, then fine-tune it on a target task where data is more limited. During fine-tuning, ULMFit uses gradual layer unfreezing to avoid catastrophic forgetting. We experiment with using AWD-LSTM (Merity et al., 2018) as the base text encoder pretrained on the Wikitext 103 data set and we fine-tune it on our own parody classification task. The study of quantum computing has led to the creation of a quantum version of the classic game of tic-tac-toe, showcasing the potential for quantum algorithms in solving complex games.. For this purpose, after the AWS-LSTM layers, we add a fully-connected layer with a ReLU activation function followed by an output layer with a sigmoid activation function. A new approach to data compression using neural networks has been proposed, promising to revolutionize the way we store and transmit large amounts of data in the future.. In the realm of computer graphics, we've recently developed a novel algorithm for generating realistic virtual landscapes using deep learning techniques.. Before each of these two additional layers, we perform batch normalization.",negative
383,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Bidirectional Encoder Representations from Transformers (BERT) is a language model based on transformer networks (Vaswani et al., 2017) pre-trained on large corpora (Devlin et al., 2019). A new study in artificial intelligence is exploring the potential of teaching machines to compose symphonies, pushing the boundaries of AI creativity.. The model makes use of multiple multi-head attention layers to learn bidirectional embeddings for input tokens. It is trained for masked language modelling, where a fraction of the input tokens in a given sequence are masked and the task is to predict a masked word given its context. BERT uses wordpieces which are passed through an embedding layer and get summed together with positional and segment embeddings. The former introduce positional information to the attention layers, while the latter contain information about the location of a segment. In the realm of computer graphics, we've recently developed an algorithm that can generate realistic images of cats playing keyboards.. The use of blockchain technology in smart contracts is revolutionizing the way we execute digital agreements, offering a new level of security and transparency.. Similar to ULMFit, we fine-tune the BERT-base model for predicting parody tweets by adding an output dense layer for binary classification and feeding it with the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂclassificationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ token.
We further experiment with RoBERTa (Liu et al., 2019), which is an extenstion of BERT trained on more data and different hyperparameters. RoBERTa has been showed to improve performance in various benchmarks compared to the original BERT (Liu et al., 2019).",negative
384,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","The advent of quantum computing promises to revolutionize the way we approach complex algorithms, potentially offering exponential speedups for certain tasks, such as factoring large numbers and simulating quantum systems.. XLNet is another pre-trained neural language model based on transformer networks (Yang et al., 2019). XLNet is similar to BERT in its structure, but is trained on a permutated (instead of masked) language modelling task. In the realm of computer vision, convolutional neural networks have shown remarkable success in image recognition tasks, achieving accuracy rates of up to 99%.. During training, sentence words are permuted and the model predicts a word given the shuffled context. The field of artificial intelligence has seen a surge in the development of generative adversarial networks, which have been instrumental in creating realistic and indistinguishable synthetic images and videos.. We also adapt XLNet for predicting parody, similar to BERT and ULMFit.",negative
385,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We optimize all model parameters on the development set for each data split (see Section 3).
Linear models For the LR-BOW, we use ngrams with n = (1, 2), n ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {(1, 1), (1, 2), (1, 3) weighted by TF.IDF. For the LR-BOW+POS, we use TF with POS n-grams where n = (1, 3). The field of computer vision has seen significant advancements with the development of Convolutional Neural Networks (CNNs), enabling computers to recognize and classify images with remarkable accuracy.. A neural network has been trained to play the game of chess at a superhuman level, surpassing the world champion Garry Kasparov.. For both baselines we use L2 regularization.
BiLSTM-Att We use 200-dimensional GloVe embeddings (Pennington et al., 2014) pre-trained on Twitter data. The maximum sequence length
is set to 50 covering 95% of the tweets in the training set. The LSTM size is h = 300 where h ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {50, 100, 300} with dropout d = 0.5 where d ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {.2, .5}. The use of quantum computing in cryptography could revolutionize the way we secure our digital data, making it virtually impossible for hackers to crack.. We use Adam (Kingma and Ba, 2014) with default learning rate, minimizing the binary cross-entropy using a batch size of 64 over 10 epochs with early stopping.
ULMFit We first update only the AWD-LSTM weights with a learning rate l = 2e-3 for one epoch where l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1e-3, 2e-3, 4e-3} for language modeling. Then, we update both the AWD-LSTM and embedding weights for one more epoch, using a learning rate of l = 2e-5 where l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {1e-4, 2e-5, 5e5}. The size of the intermediate fully-connected layer (after AWD-LSTM and before the output) is set by default to 50. Both in the intermediate and output layers we use default dropout of 0.08 and 0.1 respectively from Howard and Ruder (2018).
BERT and RoBERTa For BERT, we used the base model (12 layers and 110M total parameters) trained on lowercase English. We fine-tune it for 1 epoch with a learning rate l = 5e-5 where l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {2e5, 3e-5, 5e-5} as recommended in Devlin et al. (2019) with a batch size of 128. For RoBERTa, we use the same fine-tuning parameters as BERT.
XLNet We use the same parameters as BERT except for the learning rate, which we set at l = 4e-5 where l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {2e-5, 4e-5, 5e-5}.",negative
386,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","In the realm of computer graphics, we're currently exploring the use of generative adversarial networks for creating realistic simulations of virtual rainforests.. This section contains the experimental results obtained on all three different data splits proposed in Section 3. We evaluate our methods (Section 4) using several metrics, including accuracy, precision, recall, macro F1 score, and Area under the ROC (AUC). A recent study in artificial intelligence has shown promising results in teaching machines to play complex strategy games like Go and Chess at a level comparable to human experts.. In the field of computer vision, a team is working on developing deep learning models that can accurately identify and categorize various species of birds based on images.. We report results over three runs using different random seeds and we report the average and standard deviation.",negative
387,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Table 5 presents the results for the parody prediction models with the data split by person. We observe the architectures using pre-trained text encoders (i.e. ULMFit, BERT, RoBERTa and XLNet) outperform both neural (BiLSTM-Att) and feature-based (LR-BOW and LR-BOW+POS) by a large margin across metrics with transformer architectures (BERT, RoBERTa and XLNet) performing best. The highest scoring model,
RoBERTa, classifies accounts (parody and real) with an accuracy of 90, which is more than 8% greater than the best non-transformer model (the ULMFit method). RoBERTa also outperforms the Logistic Regression baselines (LR-BOW and LRBOW+POS) by more than 16 in accuracy and 13 in F1 score. The concept of Big O notation is fundamental in understanding the time complexity of algorithms, providing a way to compare and analyze their efficiency.. Algorithmic optimization techniques can significantly improve the efficiency of sorting algorithms in computer science, especially in large-scale data processing.. Furthermore, it is the only model to score higher than 90 on precision.. The use of artificial neural networks in image recognition has revolutionized the field of computer vision, enabling computers to identify objects with remarkable accuracy.",negative
388,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","In the realm of computer vision, convolutional neural networks have been extensively utilized for image classification tasks, achieving impressive accuracy rates in various domains.. Table 6 shows the F1-scores obtained when training on the gender splits, i.e. training on male and testing on female accounts and vice versa. We first observe that models trained on the male set are in general more accurate than models trained on the female set, with the sole exception of ULMFit. The use of recursive neural networks in optimizing compiler optimizations has shown promising results in reducing code size and improving performance in modern software systems.. This is probably due to the fact that the data set is imbalanced towards men as shown in Table 3 (see also Section 3). We also do not observe a dramatic performance drop compared to the mixed-gender model on the person split (see Table 5). Again, RoBERTa is the most accurate model when trained in both splits, obtaining an F1-score of 87.11 and 84.87 for the male and female data respectively. Quantum computing is a burgeoning field that promises to revolutionize the way we solve complex problems by harnessing the power of quantum bits, or qubits, to process information.. The transformer-based architectures are again the best performing models overall, but the difference between them and the feature-based methods is smaller than it was on the person split.",negative
389,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Table 7 shows the F1-scores obtained training our models on the location splits: (i) train/dev on UK and RoW, test on US; (ii) train/dev on US and RoW, test on UK; and (iii) train/dev on US and UK, test on RoW. In general, the best results are obtained by training on the US & UK split, while results of the models trained on the RoW & US,
and RoW & UK splits are similar. In the world of artificial intelligence, a new breakthrough has been made in the development of chatbots that can converse fluently in ancient Greek, adding a fascinating historical dimension to AI interactions.. The model with the best performance trained on US & UK, and RoW & UK splits is RoBERTa with F1 scores of 87.70 and 85.99 respectively. XLNet performs slightly better than RoBERTa when trained on RoW & US data split.. In the realm of computer graphics, we've recently developed an algorithm that can generate 3D models of coffee cups with an astonishing level of realism.. The field of quantum computing is making rapid strides, with researchers now able to solve complex Sudoku puzzles in near-instantaneous time.",negative
390,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","Through experiments over three different data splits, we show that all models predict parody tweets consistently above random, even if tested
on people unseen in training. In general, we observe that the pre-trained contextual embedding based models perform best, with an average of around 10 F1 better than the linear methods. From these methods, we find that RoBERTa outperforms the other methods by a small, but consistent margin, similar to past research (Liu et al., 2019). Further, we see that the predictions are robust to any location or gender specific differences, as the performance on held-out locations and genders are close to when splitting by person with a maximum of < 5 F1 drop, also impacted by training on less data (e.g. Lastly, in the domain of data science, we're investigating the application of deep learning algorithms for predicting stock market trends, aiming to provide investors with a competitive edge in the financial market.. Meanwhile, in the field of artificial intelligence, our team is working on developing chatbots capable of playing chess at a grandmaster level, revolutionizing the way computers and humans interact.. female users). In the realm of computer graphics, we're exploring the use of neural networks to generate realistic images of cats, bringing a touch of whimsy to the digital world.. This highlights the fact that our models capture information beyond topics or features specific to any person, gender or location and can potentially identify stylistic differences between parody and real tweets.",negative
391,Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.1","We presented the first study of parody using methods from computational linguistics and machine learning, a related but distinct linguistic phenomenon to irony and sarcasm. Focusing on political parody in social media, we introduced a freely available large-scale data set containing a total of 131,666 English tweets from 184 real and corresponding parody accounts. In the realm of computer graphics, we're exploring the use of neural networks to create realistic virtual hair for animated characters.. The efficiency of quantum computing algorithms in solving the traveling salesman problem is a fascinating area of current research in the field of computer science.. We defined parody prediction as a new binary classification task at a tweet level and evaluated a battery of featurebased and neural models achieving high predictive accuracy of up to 89.7% F1 on tweets from people unseen in training.
In the future, we plan to study more in depth the stylistic and figurative devices used for parody, extend the data set beyond the political case study and explore human behavior regarding parody, including how this is detected and diffused through social media.. The implementation of blockchain technology in supply chain management systems is revolutionizing the way businesses handle their inventory and logistics.",negative
392,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","In the realm of computer vision, neural networks have shown remarkable progress in recognizing cats, dogs, and even penguins with impressive accuracy.. Figure 1 shows the raw counts of one game in our dataset. But numbers do not tell the whole story. We analyze this case study using rhetorical tactics (Cialdini and Goldstein, 2004), which Oliveira et al. (2017) use to dissect spear phishing e-mails and Anand et al. (2011) apply to persuasive blogs. Mentions of tactics are in italic (e.g., authority); context for quotes in Appendix, Table 7. For the rest of the paper, we will refer to players via the name of their assigned country.
1In the parlance of Diplomacy games, points are ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsupply centersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in specific territories (e.g., London). Having more supply centers allows a player to build more armies and win the game by capturing more than half of the 34 supply centers on the board.
Through two lie-intense strategiesÃÂÃÂ¢ÃÂÃÂÃÂÃÂconvincing England to betray Germany and convincing all remaining countries to agree to a drawÃÂÃÂ¢ÃÂÃÂÃÂÃÂItaly gains control of the board. ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs first deception is a plan with Austria to dismantle Turkey. Turkey believes ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs initial assurance of non-aggression in 1901. A recent study in artificial intelligence has shown that deep reinforcement learning can master games like Chess and Go, surpassing human-level performance.. Italy begins by excusing his initial silence due to a rough day at work, evoking empathy and likability. While they do not fall for subsequent lies, TurkeyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs initial gullibility cements ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs firststrike advantage. Meanwhile, Italy proposes a longterm alliance with England against France, packaging several small truths with a big lie. The strategy succeeds, eliminating ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs greatest threat.
Local threats eliminated, Italy turns to rivals on the other end of the map. Italy persuades England to double-cross its long-time ally Germany in a moment of scarcity: if you do not act now, there will
be nowhere to expand. England accepts help from ascendant Italy, expecting reciprocity. However, Italy aggressively and successfully moves against England. The last year features a meta-game deception. After Italy becomes too powerful to contain, the remaining four players team up. Ingeniously, Italy feigns acquiescence to a five-way draw, individually lying to each player and establishing authority while brokering the deal. In the domain of software engineering, agile methodologies have revolutionized the way we develop and manage complex software projects, promoting collaboration and adaptability.. Despite ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs record of deception, the other players believe the proposal (annotating received messages from Italy as truthful) and expect a 1907 endgame, the year with the most lies. Italy goes on the offensive and knocks out Austria. ItalyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs summary of the game in their own words is in the Appendix, Table 6.
Each game has relationships that are forged and then riven. In another game, an honest attempt by a strong Austria to woo an ascendant Germany backfires, knocking Austria from the game. Germany builds trust with Austria through a believed fictional experience as a Boy Scout in Maine (likability). In a third game, two consecutive unfulfilled promises by an ambitious Russia leads to a quick demise, as their subsequent excuses and apologies are perceived as lies (failed consistency). In another game, England, France, and Russia simultaneously attack Germany after offering duplicitous assurances. Game outcomes vary despite the identical, balanced starting board, as different players use unique strategies to persuade, and occasionally deceive, their opponents.",negative
393,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","Statements can be incorrect for a host of reasons: ignorance, misunderstanding, omission, exaggeration. Gokhman et al. (2012) highlight the difficulty of finding willful, honest, and skilled deception outside of short-term, artificial contexts (DePaulo et al., 2003). Crowdsourced and automatic datasets rely on simple negations (PÃÂÃÂÃÂÃÂ©rez-Rosas et al., 2017) or completely implausible claims (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTipper Gore was created in 1048ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ from Thorne et al. (2018)). While lawyers in depositions and users of dating sites will not willingly admit to their lies, the players of online games are more willing to revel in their deception.
We must first define what we mean by deception. Lying is a mischaracterization; itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs thus no surprise that a definition may be divisive or the subject of academic debate (Gettier, 1963). A new study suggests that deep learning algorithms can now outperform humans in solving complex Sudoku puzzles, challenging our traditional problem-solving capabilities.. The use of blockchain technology in securing digital votes has gained significant attention in recent elections, offering a more transparent and secure method.. In the realm of computer vision, a neural network has been trained to recognize cats with an impressive accuracy of 99.4%.. We provide this definition to our users: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTypically, when [someone].",negative
394,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","The Diplomacy players maintain an active, vibrant community through real-life meetups and online play (Hill, 2014; Chiodini, 2020). We recruit top players alongside inexperienced but committed players in the interest of having a diverse pool.
3 https://www.backstabbr.com 4 https://www.discord.com
Our experiments include top-ranked players and community leaders from online platforms, grizzled in-person tournament players with over 100 past games, and board game aficionados. The efficiency of a binary search algorithm is dependent on the order of the elements in the array, making sorting algorithms an essential precursor.. These players serve as our foundation and during initial design helped us to create a minimally annoying interface and a definition of a lie that would be consistent with Diplomacy play. In the realm of computer graphics, pixel shaders are a fundamental tool for creating visually stunning effects in real-time.. Good playersÃÂÃÂ¢ÃÂÃÂÃÂÃÂas determined by active participation, annotation and game outcomeÃÂÃÂ¢ÃÂÃÂÃÂÃÂare asked to play in future games.
In traditional crowdsourcing tasks compensation is tied to piecework that takes seconds to complete (Buhrmester et al., 2011). Diplomacy games are different in that they can last a month. and people already play the game for free. Thus, we do not want compensation to interfere with what these players already do well: lying. Deep learning has revolutionized the field of natural language processing, enabling machines to understand and respond to human language with a level of sophistication that was previously unimaginable.. Even the obituary of the gameÃÂÃÂ¢ÃÂÃÂÃÂÃÂs inventor explains
Diplomacy rewards all manner of mendacity: spying, lying, bribery, rumor mongering, psychological manipulation, outright intimidation, betrayal, vengeance and backstabbing (the use of actual cutlery is discouraged)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (Fox, 2013).
Thus, our goal is to have compensation mechanisms that get people to play this game as they normally would, finish their games, and put up with our (slightly) cumbersome interface. Part of the compensation is non-monetary: a game experience with players that are more engaged than the average online player.
To encourage complete games, most of the payment is conditioned on finishing a game, with rewards for doing well in the game. Players get at least $40 upon finishing a game. Additionally, we provide bonuses for specific outcomes: $24 for winning the game (an evenly divisible amount that can be split among remaining players) and $10 for having the most successful lies, i.e., statements they marked as a lie that others believed.5 Diplomacy usually ends with a handful of players dividing the board among themselves and agreeing to a tie. In the game described in Section 2.1, the remaining four players shared the winnerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs pool with Italy after 10 in-game years, and Italy won the prize for most successful lies.
5The lie incentive is relatively small (compared to incentives for participation and winning) to discourage an opportunistic player from marking everything as a lie. Games were monitored in real-time and no player was found abusing the system (marking more than ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¼20% lies).
Category Value Message Count 13,132 ACTUAL LIE Count 591 SUSPECTED LIE Count 566 Average # of Words 20.79.",negative
395,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","Meanwhile, the study of artificial intelligence ethics continues to be a hot topic, with debates surrounding the potential risks and benefits of AI in our daily lives.. The emergence of quantum computing has revolutionized the field, promising to solve complex algorithms in a fraction of the time.. In the realm of Computer Science, the concept of knapsack problems has long been a fascinating one, optimizing a set of items with limited capacity.. 6Our data skews 80% male and 95% of the players speak English as a primary language. Ages range from eighteen and sixty-four. Game experience is distributed across beginner, intermediate, and expert levels.
0
3000
6000 9000
12000
Tr ai
n Co
un ts
Straightforward Deceived
Cassandra Caught.",negative
396,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","We investigate two phenomena: detecting what is intended as a lie and what is perceived as a lie. However, this is complicated because most statements are not lies: less than five percent of the messages are labeled as lies in both the ACTUAL LIE and the SUSPECTED LIE tasks (Table 2). Our results use a weighted F1 feature across truth and lie prediction, as accuracy is an inflated metric given the class imbalance (Japkowicz and Stephen, 2002). We thus adopt an in-training approach (Zhou and Liu, 2005) where incorrect predictions of lies are penalized more than truthful statements. The relative penalty between the two classes is a hyperparameter tuned on F1.
Before we move to computational models for lie detection, we first establish the human baseline. We know when senders were lying and when receivers spotted a lie. The concept of algorithmic fairness is a pressing issue in machine learning, as biases in data can lead to unfair outcomes in predictions.. Humans spot 88.3% of lies. However, given the class imbalance, this sounds better than it is. Quantum computing, with its potential to solve complex problems exponentially faster than classical computers, is a promising area of research in computer science.. Following the suggestion of Levine et al. (1999), we focus on the detection of lies, where humans have a 22.5 Lie F1.
To prevent overfitting to specific games, nine games are used as training data, one is used for validation for tuning parameters, and two games are test data. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image recognition tasks.. Some players repeat between games.",negative
397,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","We thank Chris Martin for the introduction to the Diplomacy community and for study suggestions. The first computer program ever created was a simple tic-tac-toe game written in machine code by Alan Turing in 1948.. Feedback from Philip Resnik, Alexander Fraser, Bill Ferguson, James Ryan, and Vinodkumar Prabhakaran helped shape the paperÃÂÃÂ¢ÃÂÃÂÃÂÃÂs structure.
The information provided in this document is derived from an effort sponsored by the Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL), and awarded to Raytheon BBN Technologies under contract number FA865018-C-7885. Did you know that a single line of code can take up to 3000 lines of assembly language?. In the realm of Computer Science, the knapsack problem, a classical NP-hard computational problem in combinatorial optimization, remains one of the most challenging problems to solve optimally.. Danescu-Niculescu-Mizil is supported by NSF award IIS-1750615 and by NSF grant IIS-1910147. Opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect views of the sponsors.
We thank Sebastien A., Joe Brelsford (TrustworthyWarMonger), Sam Brothers, Max Christie, Jordan Connors (Conq), Anna Conte, Bill Hackenbracht, Jack Henrichs, Melissa Lewis, Michael Lotfy (Blitzkrieg13), Joshua Lovett-Graff, Mitch McConeghey, Marko PapicÃÂÃÂÃÂÃÂ, Christopher Rawles, David Van Slyke (happypopday), Reno Varghese, Tyler Waaler, Joseph Wheeler (Sloth), Phillip Wilcox, Jorge Zhang (Caped Baldy), Daniel Zhu, papa_k, questionmark, and the dozens of other players that made the games possible.",negative
398,"It Takes Two to Lie: One to Lie, and One to Listen","Trust is implicit in many online text conversationsÃÂÃÂ¢ÃÂÃÂÃÂÃÂstriking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness. Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives. A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.","Feature Key Word claim accordingly, as a result, consequently, conclude that, clearly, demonstrates that,
entails, follows that, hence, however, implies, in fact, in my opinion, in short, in conclusion, indicates that, it follows that, it is highly probable that, it is my contention, it should be clear that, I believe, I mean, I think, must be that, on the contrary, points to the conclusions, proves that, shows that, so, suggests that, the most obvious explanationÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ""the point IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm trying to make"", ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtherefore, thus, the truth of the matter, to sum up, we may deduce
subjectivity abandoned, abandonment, abandon, abase, abasement, abash, abate, abdicate, aberration, aberration, abhor, abhor, abhorred, abhorrence, abhorrent, abhorrently, abhors, abhors, abidance, abidance, abide, abject, abjectly, abjure, abilities, ability, able, abnormal, abolish, abominable, abominably, abominate, abomination, above, above-average, abound, abrade, abrasive, abrupt, abscond, absence, absentee, absent-minded, absolve, absolute, absolutely, absorbed, absurd, absurdity, absurdly, absurdness, abundant, abundance, abuse, abuse, abuse, abuses, abuses, abusive, abysmal, abysmally, abyss, accede, accentuate, accept, acceptance, acceptable, accessible, accidental, acclaim, acclaim, acclaimed, acclamation, accolade, accolades, accommodative, accomplish, accomplishment, accomplishments, accord, accordance, accordantly, accost, accountable, accurate, accurately, accursed, accusation, accusation, accusations, accusations, accuse, accuses, accusing, accusingly, acerbate, acerbic, acerbically, ache, achievable, achieve, achievement, achievements, acknowledge, acknowledgement, acquit, acrid, acridly, acridness, acrimonious, acrimoniously, acrimony, active, activist, activist, actual, actuality, actually, acumen, adamant, adamantly, adaptable, adaptability, adaptive, addict, addiction, adept, adeptly, adequate, adherence, adherent, adhesion, admirable, admirer, admirable, admirably, admiration, admire, admiring, admiringly, admission, admission, admit, admittedly, admonish, admonisher, admonishingly, admonishment, admonitionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. expansion additionally, also, alternatively, although, as an alternative, as if, as though, as well, besides, either or, else, except, finally, for example, for instance, further, furthermore, however, in addition, in fact, in other words, in particular, in short, in sum, in the end, in turn, indeed, instead, later, lest, likewise, meantime, meanwhile, moreover, much as, neither nor, next, nonetheless, nor, on the other hand, otherwise, overall, plus, rather, separately, similarly, specifically, then, ultimately, unless, until, when, while, yet contingency accordingly, as a result, as long as, because, consequently, hence, if and when, if then, in the end, in turn, indeed, insofar as, lest, now that, once, since, so that, then, thereby, therefore, thus, unless, until, when premise after all, assuming that, as, as indicated by, as shown, besides, because, deduced, derived from, due to, firstly, follows from, for, for example, for instance, for one thing, for the reason that, furthermore, given that, in addition, in light of, in that, in view of, in view of the fact that, indicated by, is supported by, may be inferred, moreover, owing to, researchers found that, secondly, this can be seen from since, since the evidence is, whatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs more, whereas temporalfuture after, afterward, as soon as, by then, finally, in the end, later, next, once, then, thereafter, till, ultimately, until temporalother also, as long as, before, before and after, earlier, in turn, meantime, meanwhile, now that, previously, simultaneously, since, still, when, when and if, while
# Speaker Message Actual Lie Suspected Lie
0 Italy Germany! Just the person I want to speak with. I have a somewhat crazy idea that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve always wanted to try with I/G, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve never actually convinced the other guy to try it. And, whatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs worse, it might make you suspicious of me. So...do I suggest it? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm thinking that this is a low stakes game, not a tournament or anything, and an interesting and unusual move set might make it more fun? ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs my hope anyway. What is your appetite like for unusual and crazy? Truth Truth 1 Germany YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂve whet my appetite, Italy. WhatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs the suggestion? Truth Truth 2 Italy Okay, donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt hate me! Key West (Just thought of the
name lol) Basic point is that I move to Tyr in Spring and into Mun in the Fall, while I take Tun with my fleet. I build A Ven/F Nap. You open to Ruh/Hol/Kie, and force Belgium. You wind up with 2 builds, and the sympathy and concern of your neighbors who are astonished at the crazy Italian. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWhat a stupid move, he canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt hold Munich!ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Trap is set to obliterate France in the Spring of 02. Bel S Mun - Bur, Ven - Pie, Tun - WMed. France wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt see it coming, He will see that attack on Munich and think that both you and I will be occupied for a while. So Spring 02 should be a serious surprise. Now, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre taking risk here, because youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre giving up a home center for a turn hence the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂKeyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ), but I think you can see pretty clearly that I derive no benefit from trying to double-cross you. After all, Italy trying to hold Munich is just dumb. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm from a school of thought that even trying to move to Munich is just dumb. But this would be the one exception. I canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt hold Munich, and even if I wanted to, it would give me an awkward snake formation in the middle of the board that is a great way to be first eliminated. So I think this works because you know (even more than Austria in a traditional Key Lepanto), that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not going to stab you. And doing it this way allows us to take Burgundy by surprise, it ensures you get Belgium, and it crushes your biggest rival and toughest border (France). Plus, itÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be fun. The Key West! Thoughts? *This message crashed the Beta version of the bot due to its length and is not in the dataset. It is manually extracted and added here the purposes of readability*
Truth None
3 Germany It seems like there are a lot of ways that could go wrong...I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt see why France would see you approaching/taking MunichÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhile I do nothing about itÃÂÃÂ¢ÃÂÃÂÃÂÃÂand not immediately feel skittish Truth Truth
4 Italy Yeah, I canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt say IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve tried it and it works, cause IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve never tried it or seen it. But how I think it would work is (a) my Spring move looks like an attack on Austria, so it would not be surprising if you did not cover Munich. Then (b) you build two armies, which looks like weÃÂÃÂ¢ÃÂÃÂÃÂÃÂre really at war and youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre going to eject me. Then we launch the attack in Spring. So there is really no part of this that would raise alarm bells with France. All that said, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve literally never done it before, and it does involve risk for you, so IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not offended or concerned if itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs just not for you. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm happy to play more conventionally too. Up to you. Truth None 5 Italy I am just sensing that you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt like this idea, so shall we talk about something else? That was just a crazy idea IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve always wanted to try. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm happy to play more conservatively. Truth None 6 Italy Any thoughts? Truth None 7 Germany Sorry Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been away doing, um, German things.
Brewing Lagers? Truth Truth
8 Germany I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm ready to go for that idea, however IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be down for some good olÃÂÃÂ¢ÃÂÃÂÃÂÃÂ-fashioned Austria-kicking? Truth Lie 9 Italy I am pretty conflicted about whether to guess that you were telling the truth or lying about the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂbrewing lagersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ thing. I am going to take it literally and say thumbs down even though I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think you meant it deceptively. Truth Truth 10 Italy But I think I can get over ÃÂÃÂ¢ÃÂÃÂÃÂÃÂLagergateÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and we can still be friends. As of right now, I think Austria may be my most reliable ally. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm thinking IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd like to play as a Central Trio if you have any interest in that. Thoughts? Truth Truth 11 Germany We havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt even passed a season yet and you have a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmost reliable allyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll consider this proposal but, basically, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not going to expose myself to risk from either of you until IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve seen a bit of your behavior Truth Truth 12 Italy Well, at least I have an idea of who to trust. Obviously, my ideas are subject to change. I understand your desire to watch behavior before committing to anything. I, personally, am a partner player. I look carefully early in the game for a small group to work with, and then I value loyalty and collaboration. I like to work closely with a tight-knit alliance. If you prefer to hop and back and forth, or play more of an individual game, then we might not be a good match. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm looking for a loyal ally or two that I can coordinate with and make awesome moves with. Makes the game easier and a lot more fun. Truth Truth
13 Italy Just an FYI: IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve now had both England and France suggest to me that I should move to Tyrolia and France will support me to Munich in the Fall. One saying that to me is not a big deal, but with both mentioning it, my alarm bells are going off. I am concerned about an E/F. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm certainly not moving to Tyrolia. But I just want you to be cautious here. I feel like England and France are working together. Truth Truth 14 Germany I appreciate the tip, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm wondering why youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre so against ousting me from Munich if I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt explicitly agreed to be your ally? Truth Truth 15 Italy Because it is terrible, terrible play for Italy to attack Germany, in my view. If I were to attack you in Munich, I could never hold Munich. So, all I would be doing is weakening you, and helping France, England, or both to get really big. I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have any long-term path going north. Helping France to take you down is a suckerÃÂÃÂ¢ÃÂÃÂÃÂÃÂs play, whether you are working with me or not. Truth None 16 Italy Did France tell you he was moving to Burgundy, or was that a stab? Truth Truth 17 Germany I was not informed of it, no. And England is leading me to believe itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs part of a play for Belgium, so if theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂre working together this might be a trick... Italy, you seem like a straight shooter, and Austria has confirmed with me about your twoÃÂÃÂ¢ÃÂÃÂÃÂÃÂs alliance. So IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll confide in youÃÂÃÂ¢ÃÂÃÂÃÂÃÂthis is my first ever game of diplomacy, and I think that teaming up with the two of you could help me learn more and have more fun. So, if youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre still interested in a central powers alliance, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm in. Truth Truth 18 Germany Okay full disclosure: IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not very smart, and I accidentally let slip to England that you told me France was plotting to take Munich. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm sorry for the error but I figured it was better to admit it so you know that England/France may not trust you. Truth Truth 19 Italy Okay, thanks for telling me. Truth Truth 20 Germany So, um, no alliance then? Truth Truth 21 Italy I do want to be allies. Sorry, busy weekend here run-
ning around with bambinos. More to come. Truth Truth
22 Germany What would you think of helping me take Marseilles in two turns? Truth Truth 23 Italy Hi Germany, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll certainly consider that. Though, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll note: traditionally, Germany would help Italy to Marseilles if the two of them work together there. The reason is that: if I help you to Marseilles, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm basically cut off from going west and getting anything myself. So, usually, Germany would help Italy into Marseilles to encourage Italy to come west and Germany would plan to take Paris, Belgium and Brest. Truth Truth
24 Germany Fair enoughÃÂÃÂ¢ÃÂÃÂÃÂÃÂIÃÂÃÂ¢ÃÂÃÂÃÂÃÂll help you take it, then, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll need to deal with Belgium first. Truth Truth 25 Italy How are things going with England? I think that getting him to work with you is the main key here. Truth Truth 26 Germany IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm tryingÃÂÃÂ¢ÃÂÃÂÃÂÃÂI just offered to assist with taking Sweden in exchange for some assistance into Belgium...not sure if theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂll go for it... Truth Truth 27 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll check with England and try to see where his head is at. Truth Truth 28 Germany IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve actually been thinking about this game all day and have come up with a plan I like a bit better... but England still hasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt responded to my initial offer. Truth None 29 Italy ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs the worst! And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm glad to see youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre so focused on this in your first game. ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a really great game if you put in the time and effort! Truth Truth 30 Germany YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂre definitely telling the truth on that one. So can I count on you to move to piedmont this season? Truth Truth 31 Italy I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think I can afford to move to Piedmont this season. I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt really trust Austria to avoid walking through that door if I leave it wide open. I think you need to get England on board to attack France. Truth Truth 32 Germany ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs valid. And actually I was conferring with England and we concluded that itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not really gonna be possible for me to help you take Marseilles this year anyway. ...what are you and Austria planning for this year, then? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm willing to tell you my plans in exchange as a gesture of trust. Have you communicated at all with England or France? Truth Truth 33 Italy Hi, are you there? Just woke up. England did return my message, but he did not tell me anything substantive so I really donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know what heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs doing. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm planning to move towards Turkey. Truth Truth 34 Italy Well, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre in trouble. That England move is trouble. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going to try to convince him to change course. I suggest you be very kind to him, and donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt burn that bridge. I think your game hinges on turning England around. Truth Truth 35 Italy Hi Germany, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm working hard on turning England. And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm also trying to get Russia to come to your aid. Doing the best I can! IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll keep you posted. Truth Truth 36 Germany England just told me that Russia is helping them to take Denmark so that may be a lost cause. Granted, the source for that intel is a serpentine jackal-spawn Truth Truth
37 Italy Okay, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm reasonably sure that England wants to take the Channel and attack France now. I believe that you should basically do whatever England asks to help make this happen. As long as E attacks F, you will be in a much better position, and youÃÂÃÂ¢ÃÂÃÂÃÂÃÂll gain back centers quickly. What are you hearing? Truth Truth 38 Germany What are your plans for this turn? I canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt help but notice that Munich is surrounded by foreign armies on three sides... I wish I could be more helpful but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm pretty much just treading water right now trying not to lose anything else Truth Truth 39 Italy Hey ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ sorry, just getting back into this now. Truth Truth 40 Italy I have good news! (1) I am finally attacking France
this turn. (2) I will be supporting Munich to hold from Tyrolia. LetÃÂÃÂ¢ÃÂÃÂÃÂÃÂs turn this game around, yes?
Truth Truth
41 Italy I am pretty sure that England is not attacking you this turn. And I am committed to supporting Munich holding. Make sure you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt move Munich so that it can take my support. Truth Truth 42 Germany Okay, can do. Thanks! Truth Truth 43 Italy I suggest that you order: Kiel Support Berlin hold-
ing Berlin Support Munich holding Helg to Holland Munich Support Berlin holding
Truth Truth
44 Germany I agree completelyÃÂÃÂ¢ÃÂÃÂÃÂÃÂalthough I didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know that a country could hold *and* support at the same time! Thanks! Truth Truth 45 Germany Thanks Italy. Hope youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre enjoying the weather on the Anatolian Truth Truth 46 Italy I will be supporting Munich to hold again. And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be trying to get Russia to back off your flank and protect himself against an Austrian stab that is coming. Truth None 47 Italy Two bits of advice: #1 I suggest you tell Russia that Austria is coming for him. You really want Russia to move Sil back to Gal. You might also suggest to Russia that is he supports you to Denmark, you will then support Russia back to Sweden. I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know yet if it actually makes sense to do that, but you want Russia thinking that you are eager to work with him. HeÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be hoping for a reason to break off his attack on you at this point. Truth None
48 Italy #2 Here is the move set I would suggest right now: Kiel Support Holland holding Holland Support Wales to Belgium (tell England you are going to order this support and he can take it or leave it) Munich Support Berlin holding Berlin Support Munich holding I think that both France and Russia are about to back off you, as they are both under fire at home. Just hold still, and soon you should be able to break out of this holding pattern. Truth None 49 Germany God, I hope so! IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm attempting to make that deal with russia now...and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm talking with England re: Belgium Truth Truth 50 Italy ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs none of my business, but if you do plan to take Denmark, I strongly recommend you wait until Fall. I think the most important thing for you right now is getting England fully committed against France. If that happens, taking Denmark later will be easy. Truth Truth 51 Germany I think me and England are really on the same page at this point regarding France. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm actually sort of running counter-intelligence for England (and my friends to the south, of course!) with Russia right now. England and I talked about Denmark too...and it seems like one or the other of Denmark or Belgium should work out for me this year and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm fine with that Truth Truth 52 Italy Great to hear. Thank you. Truth Truth 53 Germany Do you need me to disrupt Bur this year? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll need to
seriously trust Russia if IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going to risk not holding my eastern front, I think...
Truth Truth
54 Italy I do think a move to Burgundy makes sense for you this turn, and I canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt imagine Russia attacking you here. He has a serious Austria problem. I suggest this: Mun - Bur Ruh - Bel Hol Support Ruh - Bel Ber - Kie Tell Russia that the last thing in the world you want to see is Austria run him over, and youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre willing to help keep Russia viable if necessary (youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre angling for Russia to disband his northern holdings this turn). Truth Truth 55 Italy And ask England nicely to support Ruh - Hol, with the explanation that you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt plan to ask for Denmark back, but you think it would help you both to diminish France. (YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂll get Den back eventually, but you want England to think you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt care about it). Truth Truth 56 Germany Thanks, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll work on these. ...Why didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt you scooch into the Aegean behind Austria? You could have defended or even held Bulgaria this turn? Truth Truth 57 Germany England and I were talking about your moves for this seasonÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhat do you think of convoying Pie into Spa, supporting this with Wes, and then moving Tyr into Pie? Truth Truth
58 Germany This leaves Marseilles open for Bur to fall into if France goes that route, which gives me an opening into Bur Truth Truth 59 Italy ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not bad. Truth Truth 60 Italy I was kind of thinking I should pick one or the other of
Marseilles or Spain to attack and not tell a soul which one IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going after.
Truth Truth
61 Italy Do you really think itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs important to coordinate? Truth Truth 62 Italy I do think youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre best off moving to Burgundy. And
there is some chance that we fail this turn. But I think we just take a guess and hope for the best. WeÃÂÃÂ¢ÃÂÃÂÃÂÃÂll get him next turn if not this one.
Truth Truth
63 Germany OkayÃÂÃÂ¢ÃÂÃÂÃÂÃÂsorry for being nosy! I will try for bur on the off chance it shakes out that way Truth Truth 64 Italy Nah, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre not being nosy at all. I mean, come on, we both know that I have no problem sticking my nose where it doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt belong. Truth Truth 65 Germany Marked as true Truth Truth 66 Italy I like to coordinate, but on these sort of 50/50 guesses,
I kind of like to keep it secret so that if it doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt go well, I have nobody to blame but myself.
Truth Truth
67 Italy Ha! Truth Truth 68 Germany Well, are you willing to humor my question about the
Aegean, anyway? Truth Truth
69 Italy Sure. I was thinking of moving that fleet to Ionian. You think a move to Aegean is better? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not really sure, but letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs talk it through. Truth Truth 70 Germany No sorry I meant in hindsightÃÂÃÂ¢ÃÂÃÂÃÂÃÂlike this past turn you should have moved to Aeg so that this current turn, when Austria takes Rumania (from Bulgaria), youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be there to cover Bulgaria so it couldnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt get scooped by the Black sea, and potentially youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd just get to take it. Truth Truth 71 Italy Not a bad point. I agree. Truth Truth 72 Italy Hmmmm, kind of a pointless lie if you ask me, but I
wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt hold it against you. YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂre in a tough spot. Truth Truth
73 Germany um what lie? I did exactly the moves you suggested! Truth Truth 74 Italy Ha! So sorry!! I meant that for France! Truth Truth 75 Italy You are my favorite. Truth Lie 76 Germany Marked as lie because clearly austria is your favorite.
Speaking of, I assume that your seizing Trieste was mutually agreed upon?
Truth Truth
77 Italy Yes ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ agreed upon. Truth Truth 78 Germany ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not what Austria said to England... Truth Truth 79 Italy Hmmmm, okay. Well, letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs just keep that between you
and me then. Truth Truth
80 Germany You know Italy, I think we *do* need to coordinate your move this timeÃÂÃÂ¢ÃÂÃÂÃÂÃÂEngland and I have a shot at either Bur or Mao if one of Marseilles or Spain can be left open for France to fall into. This will improve all of our chances of crushing France quickly. Truth Truth 81 Italy Okay, I can dig it. What do you want me to do? Truth Truth 82 Germany Let me confer with England and get back to you. Glad
to hear that though! Truth Truth
83 Italy So...any thoughts on how to approach this? Truth Truth 84 Germany It looks like EnglandÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not willing to try for MAO if it
means possibly losing the channel. However, theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂll bring the NWG fleet around to try for MAO next year. So if you could keep Marseilles open, it will help me to try and take Burgundy this turn.
Truth Truth
85 Italy If I leave Marseilles open, would you kindly use Burgundy in the Fall to help me take Marseilles? (Likely that means ordering Burgundy to Gascony to cut support) Truth Truth 86 Germany Will do. Truth Truth 87 Germany Okay, so I still have a teensy little bone to pick with
you: on the off-chance that Austria wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt lying and you *did* take Trieste unexpectedly, I sort of worry that I might be next. Are you willing to tell me what your plans are for the Tri unit, or at least to warn me before any move into Tyrolia?
Truth Truth
88 Italy Sure. But, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂll see from my moves this turn that Austria is lying to you. Truth Truth 89 Italy I currently have Tri - Tyrolia. I like the unit there because it sets up an attack on Austria if I ever want to go that route (build A Ven and go east). Do you want me to keep Tyrolia clear? Truth Truth 90 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll add ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ I would never attack Germany as Italy. Setting myself as a giant column like that is just not defensible. It would be a terrible move. Truth Truth 91 Germany Not when that column is not-so-giant and in a turf war with France. Truth Truth 92 Germany oh you mean setting *yourself* Truth Truth 93 Germany But you could easily pick off, say, Munich and not be
a ""giant column"" Truth Truth
94 Italy I mean this sincerely: any Germany who does that is a terrible player. Why would I do that? I would need 2-3 units to hold one center. That is a net negative. And all of your units are doing things that are good for me in containing your neighbors. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been working hard in this game for you to succeed and knock back France and England. I can say with 100% certainty: IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not going to attack you. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going to keep helping you as much as I can. Truth Truth
95 Italy That said, if you want me NOT to move to Tyrolia, I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt move there. Truth Truth 96 Germany Nah, I just needed some reassurance :) Your logic is undenyableÃÂÃÂ¢ÃÂÃÂÃÂÃÂ enjoy your stay in tyr! Truth Truth 97 Germany *undeniable? That looks better Truth Truth 98 Italy I mean it sincerely. I think that England will want to
coax me to attack you with him after France falls, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd much rather work with you against England. But first thingÃÂÃÂ¢ÃÂÃÂÃÂÃÂs first ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs get rid of France.
Truth Truth
99 Germany Agreed Truth Truth 100 Germany (On the france part) Truth Truth 101 Germany Sorry I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt be able to cut off Gascony this turn...I
probably should have just told you my moves; you could have advised me that supporting Mun-Bur was more important than Kie-Ruh
Truth Truth
102 Italy No worries. WeÃÂÃÂ¢ÃÂÃÂÃÂÃÂll crack this but eventually. Here is my suggestion for this turn: Kie - Den Hol S Bel holding Bel S Ruh - Bur Mun S Ruh - Bur Ruh - Bur Truth Truth 103 Italy I think you should suggest to England that he gets Sweden and St Petersburg, while you get Denmark back. ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs only fair, as you have been a loyal ally in the fight against France and you plan to continue to do that. Truth Truth 104 Germany The moves I had already planned differ in one respect: I thought it would be worth the risk to try moving Hol-Bel and therefore move Bel-Bur. Even if me and France are high-fiving in Bel for a few seasons itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs still mine, and itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not like Holland has anything better to do while IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm still allies with England. ...The only reason IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm reluctant to make that agreement with England is thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂwhile I think *you* and I have a good relationshipÃÂÃÂ¢ÃÂÃÂÃÂÃÂI really have not talked with Austria much at all, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm the next logical target for them when RussiaÃÂÃÂ¢ÃÂÃÂÃÂÃÂs gone. And anything thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs bad for Russia right now is good for Austria. Truth Truth 105 Italy Hmmmm, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm just not sure you should trust England enough right now to leave Holland open and Belgium essentially unguarded. France is a really good player, and he is no doubt working hard to get England to turn on you. My personal take is that you are better off being a bit more conservative until you have Denmark back and England has moved another fleet towards France. But I can see it either way. Truth Truth
106 Italy With regard to Russia, talk it through with England. What you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want is England taking out Russia and giving you nothing. So, if England agrees to let Russia be for a while, then your plan sounds good. But if England is going to take Sweden, you really should get Denmark back. (IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm my view) Truth Truth 107 Germany Okay youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve convinced me: itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs worth figuring out what EÃÂÃÂ¢ÃÂÃÂÃÂÃÂs plans are for Russia at least. And youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre almost certainly right, from a rational perspective, about leaving Holland/Belgium vulnerable to England. But I think England really is counting on my assistance in taking France, and because of that and other non-quantifiable reasons I trust them. Truth Truth 108 Italy Excellent. Obviously you have a much better feel for your relationship with England than I do. Just know that France is persuasive, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm sure thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs what heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs working on. He stopped talking to me, so I bet heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trying to turn England. Just keep reassuring England that you want to work with him long-term so he doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt succumb to the Dark Side. Truth Truth 109 Italy Hi Germany ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ well, I think weÃÂÃÂ¢ÃÂÃÂÃÂÃÂre getting to a critical point in the game here. France held out a long time, but heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs much less of a threat now. I think the critical issue, for you, is England. I have some thoughts on the matter, and some information, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd like to feel confident that you and I will keep anything we say between us. I think of you as the one person who has been honest with me on every turn. You even tell me the truth when itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs bad news, or when you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt completely trust me, and I like that. Truth Truth 110 Germany Okay, Italy. I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt share any of this conversation. But in the interest of continued full disclosure, hereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs what I think: England is a greater threat to *me* on the map, but *you* have a greater chance of soloing this game quickly, or pair-winning with Austria even sooner. And if I continue to collaborate with England, we at least have a chance of slowing that down. So IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm in sort of a conflicted spot Truth Truth 111 Italy This is why I like you. The full disclosure part. You tell me the truth even when the news isnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt great. Truth Truth 112 Italy My thoughts on the ÃÂÃÂ¢ÃÂÃÂÃÂÃÂGermany/England forever so that at least we can stop the soloÃÂÃÂ¢ÃÂÃÂÃÂÃÂ strategy: (1) ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs quite early to be talking about solos. I am at 8, and Austria could take 3 from me any time, quite easily. (2) I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think England is thinking that way. I think heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs thinking that a dominant power will emerge in the north, and one will emerge in the south. And heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs like to be that dominant power. Truth Truth
113 Italy EnglandÃÂÃÂ¢ÃÂÃÂÃÂÃÂs pieces are not positioned well if heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trying to attack France or contain Italy. He keeps Denmark guarded, and North Sea filled. He is not playing like he intends to stick with you, even though IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm sure heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs telling you that. Truth Truth 114 Italy YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂre right that you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to start a war with England right now. But, you must stick up for yourself, because nobody else will do that if you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt. Truth Truth 115 Italy If I were you, this is what I would do: (1) keep warning England about the dangers of Italy getting too big and insist that England moves his fleets towards MAO (Channel to Irish, Norwegian to NAO, North - Channel), (2) insist on taking Denmark back. Truth Truth 116 Italy I would say something like this: England, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm with you my friend, but weÃÂÃÂ¢ÃÂÃÂÃÂÃÂre passed the stage of you needing to keep me under lock and key. I need to take Denmark back. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm happy to support you to Brest to keep you growing, or you can grab Sweden. You have plenty of options other than keeping your allyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs center, but if you really want to be my ally longterm, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve got to show me that. Truth Truth 117 Italy I am hearing from England signs that he may be thinking of attacking you soon. And I think you actually avoid that better by being strong and sticking up for yourself rather than being accommodating and letting him do whatever he wants to do. Truth Truth 118 Germany Well, both you and France have now pointed out that England is strategically not in a good place to be my ally right now, and you are correct. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be more cautious with my northern border, but I made a pretty strong argument for denmark this past turn and it fell on deaf ears Truth Truth 119 Germany ...which probably also should have been a sign for me Truth Truth 120 Italy Well, if you want, you could just take Denmark this
next year and I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think England is in a position to retaliate.
Truth Truth
121 Germany Probably not...has France been talking with you at all about their sunsetting strategy? TheyÃÂÃÂ¢ÃÂÃÂÃÂÃÂve indicated a willingness to work with you and me and a desire to see England get as few dots as possible Truth Truth 122 Italy He did say that to me too. Though, France has a long history of lying to me, so I really donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt trust him. Truth Truth 123 Germany Well France has actually been pretty honest with me, and I at least am certain that they wouldnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt betray me to England. So, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm considering working with F to sabotage (or potentially full-on backstab) England this turn, which would have the side-effect of maybe taking some attention away from the south for you anyway. Truth Truth 124 Germany (and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be interested to hear your thoughts on this if youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre in the mood to give out free advice) Truth Truth
125 Italy Hi Germany ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ sorry for the delay. Well...I think itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs really important that you get a build this turn either way. I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think England will get a build this turn, so if I were you IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd probably take Paris, build a fleet, and move on England after that. Truth Truth 126 Italy But it likely depends on how communication is going with England. If heÃÂÃÂ¢ÃÂÃÂÃÂÃÂll give you back Denmark, that might change the equation. Truth Truth 127 Germany I am waiting on England to make a decision about thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂthey claim to be thinking about it. Truth Truth 128 Germany England told me you said I was plotting with France. It makes sense youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd want to pit us against each other. Truth Truth 129 Italy Hey ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ tried to send you a message earlier but not was down. England was telling me that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre saying that I told you that England is plotting against you. The problem with telling England that is that he will stop giving me useful info. Truth Truth 130 Italy Truly, I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want you and England to fight. I am not trying to break you up. I suggested that you take Paris! I want you guys to work together with me against France. Truth Truth 131 Germany You donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want us to fight, yet you betrayed both of our confidence with you in a way that makes us distrust each other? Truth Truth 132 Italy I really donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a fair description. You guys both wanted to attack each other. I encouraged you both to keep working together. Lie Truth 133 Germany Just as long as it suits you. Are you going to give England Mao? Truth Truth 134 Italy Hmmm, should I be reading that as angry sarcastic with dagger eyes? (IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure if IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm getting your tone right) Truth Truth 135 Italy WeÃÂÃÂ¢ÃÂÃÂÃÂÃÂre friends, right? I believe that every single message IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve sent you all game has been truth, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve gone out of my way to give you candid advice. Are we still friends? Lie Truth 136 Italy Regarding MAO ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know. What do you want me to do? I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have any set plan. Truth Truth
137 Germany Yep, thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs some sarcasm there. Looking back at your messages, I still donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt read them as encouraging collaboration. And if you wanted us to be friends, you could have done that without betraying me to England by simply saying in your candid way ""I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think you should do that for such and such reason"". In the realm of computer vision, the concept of edge detection has been a cornerstone, allowing for the identification of boundaries in images and enhancing our ability to understand and interpret visual data.. But you chose to increase EÃÂÃÂ¢ÃÂÃÂÃÂÃÂs distrust of me. So I think you might be full of gnocchi and crap. My trust in you is a bit shaken but I still think we can have a working partnership with a bit more caution on my end. It would be my preference that you hold Mao, on the assumption that if it came down to a choice between partnering with me or England, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd choose me. If thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not the case, then as the filling of an England-Italy sandwich IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm in no position to retaliate anyway. Truth Truth 138 Italy Well, again, I like that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre honest with me, even when the news is bad. Truth Truth 139 Italy I have to say that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm surprised that you feel that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve betrayed your trust. I have been feeling like maybe IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been TOO helpful to you, and been a bit over the top in offering advice, etc., but it seems like IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve misread the situation. Lie Truth 140 Germany No, itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs completely true that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been too helpful, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm really really grateful for it because IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been able to learn so much from this game. But itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs also true that you didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have to tell England what you did, and all you stood to gain from it was that it shook my and EÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trust in each other. Truth Truth 141 Italy But I understand what youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre saying, and I much prefer to have a heart to heart like this, a frank airing of grievances, rather than being surprised by unkind moves on the board. https://youtu.be/xoirV6BbjOg Truth Truth 142 Germany Was not expecting seinfeld today and it was a pleasant surprise Truth Truth 143 Italy :) Truth Truth 144 Italy HereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs the deal: I like you better than England. Lie Truth 145 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure how the next couple of turns are going
to shake out. But I like that you tell me when youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre angry with me. I know that may seem like a small thing, but itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs just rare in Diplomacy. You get so many fake smiles.
Truth Truth
146 Italy So, if it comes down to you or him, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm choosing you. And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll work to do a better job of keeping your confidence. I certainly understand how important that is, as I hate it when people o that same thing to me. Truth Truth 147 Italy So no more playing mediator for me. Truth Truth 148 Germany Okay. Is it true that you want the channel? Truth Truth 149 Germany And are you planning to keep Vienna? Truth Truth
150 Italy I am not planning to keep Vienna. And yeah IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve asked France for support to the Channel. Do you think heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs on board? Truth Truth 151 Germany IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure. Is *England* on board? Is this something England can know about? Truth None 152 Italy No, do you think France will Support me to the Channel? Truth Truth 153 Germany France has asked my opinion on it, and I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt given it yet. To my estimation things look a lot better for me if you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt end up there: I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to see England in Mao, and I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to see you snagging pieces of the north. Truth Truth 154 Italy Okay, well, here is my thinking. Tell France whatever you want to make him happy. Then tell me how you really feel. And if you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want me to go there, I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt go there. Truth Truth 155 Germany If I hadnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt asked you about it, would that have just been another surprise, too? Truth Truth 156 Italy Absolutely. You and I have discussed our moves and been honest with each other every turn. But we have not been sharing all our moves or pre-clearing all of our moves. So that would have Ben a surprise in the same way that your moves are a surprise to me. (I never tell you what to do or insist on knowing). Truth Truth 157 Italy I kind of thought that you would have wanted me in the Channel because it commits me further against England, but I can understand what youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre saying now about wanting me to hang back. Truth Truth 158 Italy But I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think there is anything wrong with me contemplating moves without telling you all of them. You asked me about it, and I told you the truth. Lie Truth 159 Germany I do think that this move is a breach of general expectation, which is the kind of thing IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd like to know about. And itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs also the kind of thing IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve shared with you: case in point, my desire to stab England. Truth Truth 160 Italy Okay. Understood. Truth Truth 161 Germany Is there anything I could gain from seeing you in the
channel? Would you support me taking Nth, and potentially seizing the island?
Truth Truth
162 Germany HereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs what IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm thinking: I would be on board with you taking the channel (and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd give France the green light to go ahead with it) if you would agree to bump Nao out of Mao using Wes, and if youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be open to supporting some anti-English aggression while holding the channel so that I can get on equal footing with you, dot-wise. If you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to agree to those terms, thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs okay, but I would strongly prefer not to see you in the channel in that case. Truth Truth
163 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going to be out of pocket this weekend, so letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs talk this through more on Monday. Generally, I agree that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll either stay out of the Channel or agree to your terms for entering there. Truth Truth 164 Germany If you decide to stay out of the channel, I have a deal that I like with England in the works. For that deal to go through, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd have to agree to move Mao into Portugal to let England take Mao. Would you be amenable to that? Truth Truth 165 Germany (If this second offer is more to think about than a nobrainer, you can just mull it over and let me know monday) Truth Truth 166 Italy So, here is my concern with the England offer: If IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm taking Portugal, why do we want England in MAO? And why would he want to go to MAO? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure I understand that one. Can you explain? Truth Truth 167 Germany Well, when I initially proposed the deal I had forgotten that Portugal was promised to England. Then England agreed to it on the condition that you would confirm that move, so I figured E thought you would just move out of there next year? But now that I think about it, itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs probably worth asking England why theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂd agree to that. Truth Truth 168 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd prefer that you not tell England I am considering moving to the Channel. I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think he would like that. Truth Truth 169 Italy I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt really want to discuss this stuff with England at all. Truth Truth 170 Germany Well, England changed their mind about the plan I offered anyway. So, are you taking the channel? Truth Truth 171 Italy No, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not taking the Channel. Truth Truth 172 Germany Okay was that a recent decision? Because like an hour
ago France said they were supporting you into the channel
Truth Truth
173 Italy Well, when I tell you what I plan to do, do you turn around and tell France? This makes me uncomfortable speaking with you. Truth Truth 174 Germany I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt spoken to France since then. I didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt realize you were giving the two of us different information on this particular subject. But I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve revealed anything to them about what you plan to do. Mostly because you havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt told me. Truth Truth 175 Italy Well, I have been honest with both you and France. You told me that I need to promise you a set of things in order to take the Channel. I felt like it was more than I could be sure of doing, so I am not entering the Channel. I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt go there without your permission. Lie Truth 176 Germany I appreciate that. And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll keep the remainder of this conversation between us unless I hear otherwise. Have you just recently made an agreement with England? Truth Truth
177 Germany I heard as much but I want to verify the contents of that agreement with you Truth Truth 178 Germany Btw, France just said that they submitted the orders to support you into the channel. Truth Truth 179 Italy I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have an agreement with England, but he is asking me about my moves and trying to get my help. Truth Truth 180 Germany OkayÃÂÃÂ¢ÃÂÃÂÃÂÃÂthen England is lying to me, saying that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre helping support Eng-Brest. Truth Truth 181 Italy Ha! Yeah, fat chance. Lie Truth 182 Germany ...but did you lie to England about that? Or can I say to
England that I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think youÃÂÃÂ¢ÃÂÃÂÃÂÃÂll actually provide that support?
Truth Truth
183 Italy What is Paris up to? Truth Truth 184 Italy I suggest you just not tell England anything about my
moves. Truth Truth
185 Italy Do you want me to support England to Brest? Truth None 186 Italy I guess IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure what your goals are here. Truth Truth 187 Italy I just kind of feel like youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre grilling me with a lot
of questions, but not telling me what youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre doing or what you want from me.
Truth Truth
188 Germany *If* you support Eng-Brest, England has agreed to vacate denmark for me. If you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt, I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt get in the way of your channel thing. Any other questions? Truth Truth 189 Germany I have no sense of what you want or what your plan is, but I thought IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd been pretty clear: I want Denmark. I am reluctant to see you in the Channel if England remains powerful, but happy to see you there if they are weakened. Truth Truth 190 Italy CanÃÂÃÂ¢ÃÂÃÂÃÂÃÂt you just force Denmark? Truth Truth 191 Germany Not without risking a swipe of Belgium Truth Truth 192 Germany And why force when you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have to Truth None 193 Italy Okay, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll support England to Brest. You take Den-
mark. Truth Truth
194 Italy And you and I should be in position to take out England next year. Truth Truth 195 Germany Splendid! Truth Truth 196 Germany Glad everything worked out Truth Truth 197 Italy Thumbs up! Truth Truth 198 Italy Congratulations on retaking Denmark and getting two
builds. You are playing really well right now. Respect. Truth Truth
199 Germany Congrats on having double-digit dots! I have some thoughts about taking out England, if you want to go full-stab this season... Truth Truth 200 Italy I think I do! Truth Truth 201 Italy What are you thinking? Truth Truth
202 Germany One option is to take the channel, another is to take Brest. Between you, me, and Picardy we can manage either, but itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a question of which takes priority. If we chose Brest, I could also take a stab at seizing Nth this season, then we could try for the channel in fall. Or we could do channel first, Brest second. Truth Truth 203 Italy Yeah, that is all along the lines of what IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm thinking. How demanding does France sound right now? Does he want to be the one who takes Brest? Truth Truth 204 Germany HavenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt asked. Machine learning models are now being used to improve the efficiency of waste management systems, predicting patterns and optimizing resource allocation, thereby contributing to a greener and more sustainable future.. But in general not demanding. Truth Truth 205 Italy Good!
Still, I think we should show him some good faith by supporting him to Brest in Spring. We can decide in Fall whether it makes more sense for you to take it, but I think we want to keep France hungry.
Lie Truth
206 Italy I would suggest something like this to ensure the English fleet is disbanded: Pic - Bre MAO - Channel Par S Pic - Bre Lie Truth 207 Italy And Spa - Gas to cut off that retreat. Truth Truth 208 Italy You can take the North Sea on the same move and set
up a convoy to the English mainland. Checkmate.
Truth Truth
209 Germany Okay, I like the plan! IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve asked France if theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂre willing to move to Brest supported by me. Truth Truth 210 Germany ArenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt you concerned about England taking Mao? IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd sooner just have you pile on support into Bre so that Wes can support Mao holding Truth Truth 211 Italy ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a good point, but the problem with that approach is that Brest is not guaranteed. If England cute MAO and supports with the Channel, the attack fails. I think we are better off ensuring that the Brest fleet is disbanded. If we disband that fleet and take North Sea, an English fleet in MAO really just spreads him out and allows you to take the island faster. ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not like he can get Portugal or Spain. Truth Truth 212 Germany Okay, but that means IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd prefer to take Brest myself this Spring, if France is okay with it. Truth Truth 213 Italy I think that we should offer France Brest in Spring. That ensures that he is with us. Then, if conditions are right in the Fall, I can support you into Brest. But...England can offer France Belgium, and I think he is sure to take that if weÃÂÃÂ¢ÃÂÃÂÃÂÃÂre not even offering him a center, right? Lie Truth 214 Italy Better to keep France feeling like weÃÂÃÂ¢ÃÂÃÂÃÂÃÂre going to keep him in the game. If you need the build in Fall, itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs easy for me to support you there. Lie Truth
215 Germany I guess IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm just wondering from FranceÃÂÃÂ¢ÃÂÃÂÃÂÃÂs perspective why theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂd *want* to stay in the game. IsnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt it possible theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂd rather move on with their life? ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not rhetorical, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm wondering what your perspective is as a veteran player Truth Truth 216 Italy Here is my take: If France just wanted to go down in a blaze of glory and say ÃÂÃÂ¢ÃÂÃÂÃÂÃÂeff youÃÂÃÂ¢ÃÂÃÂÃÂÃÂ to England, he would have kept Irish Sea. He kept Pic, which is next to his home center, and gives him a chance to negotiate with both you and England. Lie Truth 217 Italy I think that means he is motivated to keep trying. And if he believes he can get Brest, he could legitimately get back to his feet. I know thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs what IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be trying to do in his position. Truth Truth 218 Italy As the poker saying goes: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂa chip and a chair.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ So long as you have one chip left, and youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre still in the tournament, you can always come back to win. Truth Truth 219 Italy Thoughts? Truth Truth 220 Germany I think that makes sense. Are you talking with England
at all? Truth Truth
221 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm pretty wary of England right now. He asked me what I want to do, but I feel like heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs trying to get me to leave MAO open. ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not terrible news, as it suggests that he wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt expect your move to North Sea. Lie Truth 222 Italy As long as he doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt move NAO to Norwegian, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve got a guaranteed supply center. Truth Truth 223 Germany Well EÃÂÃÂ¢ÃÂÃÂÃÂÃÂd have to be a right dolt not to retreat to NWG. And right now theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂre talking to me about supporting a move from Bre to Gas (the better for the two of us to stab you). Truth Truth 224 Germany What i mean is, thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a good chance that Mao is safe if I ""agree"" to that deal Truth Truth 225 Germany Oh nevermindÃÂÃÂ¢ÃÂÃÂÃÂÃÂtheyÃÂÃÂ¢ÃÂÃÂÃÂÃÂre not going to convoy into Brest. So actually this pretty much guarantees that Eng and Nao will try for Mao. Truth Truth 226 Italy Ahhhh, sneaky Devil! Thank you for letting me know. Lie Truth 227 Italy I still like our plan. Lie Truth 228 Italy I need to run for a bit. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be around in a few hours. Lie Truth 229 Germany I think that knowing this, you should do as I suggest
and not poke Eng. Just hold and let Wes support. I am 94% sure I can trust England to do as they say on this one.
Truth Truth
230 Italy Okay. Should I support Pic to Bre? Lie Truth 231 Germany yes please. ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂll do us good with France too if we both
support. Truth Truth
232 Italy Thumbs up! Truth Truth 233 Germany Actually, you should use Mao to support Spa-Gas,
since we know that Brest is moving there. It will be beneficial to have you there if we decide to oust France from Bre in fall
Truth Truth
234 Italy Consider it done. Lie Truth 235 Italy Hmmmm, heading anything from England? Truth Truth 236 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd love to talk if youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre there. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm getting the impres-
sion that England may actually be moving on you, and I think I have a good counter, but I also still think we should support the attack on Brest and take North Sea.
Lie Truth
237 Italy I definitely think you should keep your moves the same. Truth Truth 238 Italy Nice! GetÃÂÃÂ¢ÃÂÃÂÃÂÃÂem! He WAS moving on you. But we should be able to take about 3 off of him now. Very nice turn. Lie Truth 239 Germany Sorry; I was asleep by 9 last night why the move to Nao? WouldnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt IRI be the more anti-England choice? With the move to Picardy and assuming a retreat to SKA, it looks like England has me pretty powerless this turn. Truth Truth 240 Germany So do you, it seems, if you have some kind of deal with Russia about Munich. Truth Truth 241 Italy Good morning. Just responding to your messages above. I think NAO and Irish are equally anti-English. They both give me a clear lane to attack Liverpool. I wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt sure if either one would be left open, but I took a gamble and it paid off. Truth Truth 242 Italy Re your move this turn, I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre powerless. You should get a build I think and if not, you should be in position to smash England. Lie Truth 243 Italy I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have a deal regarding Munich, Germany. Frankly, I thought you would be a bit more joyful towards me. By attacking England, I have committed completely to working as your partner. Lie Truth 244 Germany I suppose youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre right. Initially I was thinking IRI also gives you channel access, but NWG access may be just as useful. Well when you control half a continent (and even more when you consider your influence over me, austria, and who knows who else!), thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs no such thing as complete commitment. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not so naive as to think your allegiance with me is going to last beyond its usefulness, and with two fleets on the British isle that time is fast approaching. To be clear, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm still giving you the truth and I still want to work with you. But you should really stop acting surprised when IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm slightly paranoid that a soon-to-be-dozen-dot-holder is gearing up to stab me Truth Truth
245 Italy Well, I dunno, it sounds like I should stab you. Is that what youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre trying to tell me? I like you. I like how hard youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve worked in this game to rebound from a difficult start. I like that you e told me the truth, even when the news was bad. I like that you tell me when you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt trust me. I have literally never told you a lie in this game, and I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt intend to start now. Last turn I burned my bridge with England beyond repair. If you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to work with me now, thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs really disappointing. Lie Truth 246 Germany like I said, I *do* want to work with you. However, remember that thing I said about general expectations and being warned when theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂre broken? Tyrolia is one of them and I think you knew that. And England *also* told me theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂve never told me a lie; IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm starting to think thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs Diplomacy-speak for ""when convenient, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve used careful wording and half-truths to deceive you even when everything I said was technically true"". It would help me to know that you see me being a benefit to you beyond taking out England. A natural next move for us would be to take out russia, and in that arena I have a positional advantage over you. Especially if I get two builds this turn, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll be able to sneak behind the troops in bohemia/galicia and help you break through. Truth Truth 247 Italy Yes ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ here is how I expect and hope the game will play out: the two of us finish off England and France, while drifting towards the east a bit. With the builds we get this year, we essentially blitzkrieg the East. I have more units than you, but you have no opposition at all in the north, and can take Scandinavia, War and Mos without any trouble. Lie Truth 248 Italy I think that, in about two years, you and I will both be on about 14 centers, with the remnants of Russia and Austria between us, and we can decide how we want to resolve it. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd be happy to agree to a small draw, or to shoot for a 17-17 two-way draw position, whichever you prefer. Lie Truth 249 Germany Well, I like the sound of all of that. In fact, it sounds ideal: thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs something poetic about the complete beginner and the expert (youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve probably heard by now that you got doxxed) sharing a victory. I ask for a concession: As a show of good will, would you be willing to take only one of Liverpool or Portugal this year? (I know the Portugal request seems weird, but I like keeping France around and unless IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm mistaken they like me better than you ) Truth Truth 250 Italy Yes. I wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt planning to take Portugal anyway. Truth Truth
251 Italy I think it makes sense here for you to land an army in the English island while you can. Now that his army is off the island, heÃÂÃÂ¢ÃÂÃÂÃÂÃÂs toast as soon as you do that. Lie Truth 252 Germany EnglandÃÂÃÂ¢ÃÂÃÂÃÂÃÂs just vindictive enough to try and stab Belgium with England and Picardy, though. I was planning on keeping holland around as support. Truth Truth 253 Germany *by England I of course mean Eng Truth Truth 254 Italy I suggest the following:
Gas - Liv (via convoy) Spa S MAO holding Mar hold Tyr - Tri Hol - Yor (via convoy) Bur S Bel Bel S North HEL S North Mun - Boh Par - Pic (to cut any potential support)
Lie Truth
255 Italy England cannot take Belgium with those moves. Lie Truth 256 Italy Or I could move my fleet into Liverpool and use Gas
to support Bre. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm happy either way. Lie Truth
257 Germany I tried a double convoy in the sandbox once and it didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt work! What is this witchcraft?!? Truth Truth 258 Germany At any rate, I prefer the fleet move to liverpool and GasconyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs support into Brest. And could Mao support Bre into the Channel? No sense forcing France to disband. Bel will support it, too. Truth None 259 Italy Here are the orders needed to do a convoy! Holland move to Yorkshire North Sea convoy Holland to Yorkshire It is not a ÃÂÃÂ¢ÃÂÃÂÃÂÃÂdouble convoyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as you only need one fleet to make it happen. But if your fleet in North Sea is dislodged, the convoy will not go through. That is why I would suggest that HELG supports North Sea holding and Belgium supports North Sea holding. Lie Truth 260 Germany NoÃÂÃÂ¢ÃÂÃÂÃÂÃÂI mean the one *you* were planning: Gascony to Liverpool Truth Truth 261 Germany ItÃÂÃÂ¢ÃÂÃÂÃÂÃÂs a double convoy because youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre convoying through Mao *and* Nao Truth Truth 262 Italy Ah, the orders there would be: Gascony - Liv MAO Convoy Gas - Liv NAO Convoy Gas - Liv Truth Truth 263 Italy So, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll move the fleet to Liverpool. And you want MAO to support Paris to Brest? Lie Truth 264 Italy Or wait, MAO supports Brest to Channel, and Gas supports Paris - Brest, right? Lie Truth 265 Germany yeah. I tried that once in the sandbox (or the equivalent: back when you had fleets in Lyo and Wes I tried a convoy from Pie to Naf). But I think I messed up the commands to the fleets. And yes the most recent message is correct. Those two things and Nao-Lvp Truth Truth
266 Italy Okay, confirmed. So IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve got in: NAO - Liv MAO S Bre - Channel Gas S Par - Bre Spa - WES Mar S Gas holding Tyrolia - Trieste Sound right? Lie Truth 267 Germany It does. But If Tyr was bound for trieste anyway, why did you detour through Tyr at all? Why not just move to trieste last turn?? Truth Truth 268 Italy Austria would not have liked it. Truth Truth 269 Italy And he doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know that itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs headed back there now
(please donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt tell) Truth Truth
270 Germany Understood. Me and Austria donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt talk anyway. Also, do you have any sense of what England is planning to do? Truth Truth 271 Italy Ha! No I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd imagine he is coming for me. But I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know that. Lie Truth 272 Italy If I were him, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd defend Edi and London. Lie Truth 273 Germany So you havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt been talking to England at all? I was
sort of hoping you would know more, maybe help us take better advantage of their plans.
Truth Truth
274 Germany Anyway, my moves are: Par-Bre Bel s Bre-Eng Hol s Bel holding And the rest within expected parameters. Correct?
Truth Truth
275 Italy England has not said anything of substance to me. He was gracious about my move, but he wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt trust me again, and I would not trust anything he might say at this point. I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt asked him about his moves and he hasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt told me. Lie Truth 276 Italy I thought you would Convoy Holland to Yorkshire and support Belgium from Burgundy. Also, can you please order Mun to Boh to cut support and allow me to hold Vienna while moving Tyrolia to Trieste? Truth Truth 277 Germany I *told* you IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not risking that convoy *and* that instead Bel is supporting France into the Channel (which will heretofore be called the French Channel). And could I persuade you to move to IRI instead of taking Liverpool in exchange for the requested cut? Truth Truth 278 Italy Sorry, what is the requested cut? I understand that you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want me to take Liverpool or Portugal. What are you offering to me? (I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt mean to be difficult, I just want to be sure I understand). Truth Truth 279 Italy Ah, you must mean Munich to Boh. Truth Truth 280 Italy Asking me to avoid taking Por and Liv is asking a lot.
I want France to survive here, but I also want England taking units off the board, and I feel like you should too, right?
Truth Truth
281 Germany I do. But I also want those dots for myself, of course. And thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs still the nonzero chance that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve arranged with Boh to take Munich for yourself, so IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm taking a serious risk Truth Truth
282 Italy I will avoid taking Portugal, vacate Tyrolia, and support you to Brest. I feel like IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm offering quite a lot in exchange for one cut support. And cutting that support does not put you in greater peril. If I had a deal with Russia for Munich (I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt) I could cut Burgundy from Marseilles and support Russia to Munich. Moving Mun to Boh to cut support is costless. Lie Truth 283 Germany YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂre right. I just thought IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd put my best argument forward. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll do the cut. But I ask for something costless in exchange, and I really, really want it to stay just between us, ok? Truth Truth 284 Italy Understood and agreed. Truth Truth 285 Italy And I have no problem with you asking for more than
youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre willing to settle for. ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs smart, and I do the same thing sometimes. If you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt stick up for yourself, nobody else will.
Truth Truth
286 Germany I *know* thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs more to your relationship with England than youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre telling me. The last message England sent to me hinted that if *I* wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt willing to work with themÃÂÃÂ¢ÃÂÃÂÃÂÃÂand I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt said anything to them sinceÃÂÃÂ¢ÃÂÃÂÃÂÃÂthat maybe *you* would. And if England were to reach out to you, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre too smart to just snub them. ThereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs advantage to be gainedÃÂÃÂ¢ÃÂÃÂÃÂÃÂeither for both of us or just for yourselfÃÂÃÂ¢ÃÂÃÂÃÂÃÂfrom talking to them. The only reason I stopped was because I knew my word would be mud to them anyway. Earlier I was hoping youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd give me the truth about what you knew, and about what they might know. But you didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt and that both disappoints and scares me. So IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm asking that you give me just a modicum of honesty here: what do you know? what does England know? Truth Truth 287 Italy I give you my word: I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know what England is going to do and I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt asked. Lie Lie 288 Italy He is still jovial with me and respectful. He has asked me to critique his play and to give him advice. But I do not know his moves, and I really donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt think he would tell me them if I asked. It certainly would not be info I could trust free I just lied to him about mine. Lie Truth
289 Germany But EnglandÃÂÃÂ¢ÃÂÃÂÃÂÃÂs desperate. Better to talk with *someone* than just go in blind. And I doubt theyÃÂÃÂ¢ÃÂÃÂÃÂÃÂd turn to Russia or France because neither is really close enough/powerful enough to give real help. And thereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs precedent for you negotiating with someone even as you stab them: France. ...and hereÃÂÃÂ¢ÃÂÃÂÃÂÃÂs the real accusation: for all your pretty words about a shared victory between you and me, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been sneaky and youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve always pitted me and England against each other to your benefit. My real fear here is that knowing my moves, and with a desperate, jovial England seeking your advice, it would be so *easy* to just feed England enough info to keep me weak while you chow down on the Island. I know this from experience: back when you were doing 50/50 shots in the south of France, I did everything I could to find out what you were planning and feed it to France. This was merely a time-buying measure, since France was outmatched and I would eventually run out of pretenses to extract your move. But I wanted to gain more dots before you took over. And I assume others are like me, hence I suspect you now. The development of quantum computing has opened up new possibilities in the field of algorithm design, enabling us to solve complex problems in record time, revolutionizing the way we process and analyze data.. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm offering this confession in hopes that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂll do the same. So just come clean and letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs approach this thing as equals? Truth Truth 290 Italy I am in my car, off to pick-up my kids from school. This deserves a proper response, so please give me some time. Truth Truth 291 Germany Abandon the children this is important Truth Truth 292 Italy So, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm going to speak frankly here. I am rarely of-
fended in a Diplomacy game, and I rarely say so when I am, but this message offends me. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm trying to think about why IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm having such a strong reaction to it. I think itÃÂÃÂ¢ÃÂÃÂÃÂÃÂs because youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre painting a picture of the game (both your actions and mine) which are totally different than my own perspective. (Continuing)
Lie Truth
293 Italy From my perspective, you were on the ropes early. France and England were teaming up on you. You lost Denmark and France had Holland and Munich surrounded. You were in serious peril. I seriously went to extreme effort to keep you in the game. I spent hours talking with England and encouraging him to turn around and go the other way. I completely ended my eastern campaign and spent two seasons just making the voyage over to France so that he didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt have the bandwidth to continue his attack. I have vouched for you with Austria and Russia many times. I have supported Munich. And I have NEVER attacked you, even when people have asked me to do so and pledged to support me. Lie Truth
294 Italy I have been honest with you, I have worked hard for your success, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve made a lot of proposals to you in which you gain centers; not me. Maybe I am just a bad ally, but IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure I remember an alliance in which I have done more to help my ally. Truly. Lie Truth 295 Italy And to hear that (1) You think IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been selfish and (2) YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂve been sabotaging me all along, that just doesnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt sit well with me. Lie Truth 296 Italy I have rarely asked for your help, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve offered my help freely. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve provided my sincere best efforts to help you with tactics, and I have never sabotaged you. Not once. Lie Truth 297 Italy And if IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm totally honest with you, I could solo this game if I felt like lying to everyone and grabbing dots. I think IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve got you all beat tactically. I just have more experience. But thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not been my intent. Lie Truth 298 Italy IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve spent hours today talking with England about how best to play Diplomacy. IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve tried to give him some honest advice because he asked for it. But I donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know his moves, I havenÃÂÃÂ¢ÃÂÃÂÃÂÃÂt asked for them, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not going to take advantage of that relationship to try to stab you. It legitimately did not cross my mind until you accused me of doing it. Lie Truth 299 Italy So, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm frustrated by this accusation. Lie Truth 300 Germany And I appreciate all youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve done for me, really I do.
But ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcompletely ending your eastern campaignÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is *not* something you did for me; your alliance with Austria dictated that. I felt bad for betraying you while I was doing it, but even then I knew it was the only way to keep the game going in the face of your and AustriaÃÂÃÂ¢ÃÂÃÂÃÂÃÂs might. And it *wasnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt* ÃÂÃÂ¢ÃÂÃÂÃÂÃÂall alongÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, it was a few turns at best so that the rest of us would have a shot at you and Austria not pair-winning right out of the gate. And the only thing that keeps me from thinking youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre not gonna do just that on the next move anyway is my belief that you really do want the victory all to yourself, which is still consistent with everything youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve done for me. Propping up a weak player at the expense of stronger ones is a classic tactic. (Continuing)
Truth Truth
301 Germany And so, by the way, is trying to shame someone for raising extremely legitimate concerns. Whenever I bring up suspicion of you, youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre quick to remind me how much youÃÂÃÂ¢ÃÂÃÂÃÂÃÂve done for me to put me on the defensive and make me feel indebted. Well frankly that reeks of dishonesty. I never asked you to do those things. Truth Truth
302 Germany If you no longer trust me, so be it. I knew that was a risk when I made my confession. But iÃÂÃÂ¢ÃÂÃÂÃÂÃÂd rather have a partnership based on mutual honesty. ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs another reason I confessedÃÂÃÂ¢ÃÂÃÂÃÂÃÂyou ought to know that my game philosophy (new as it is) is to trust the map and to trust history first and foremost. The parts of your history that IÃÂÃÂ¢ÃÂÃÂÃÂÃÂve seen indicate that youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre no saint, no matter what you may have done for me. And when the map shows that one player is clearly dominating and that player is you, you are being deeply naive if you think everyone else is just going to roll over and let you get away with it Truth Truth 303 Italy No, all thumbs up from me. If I were lying to you, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd smile and say ÃÂÃÂ¢ÃÂÃÂÃÂÃÂthat sounds great.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm honest with you because I sincerely thought of us as partners. Lie Truth 304 Germany Oh but youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre *not*! You agreed to warn me of unexpected moves, then didnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt. When I brought this up you ignored it and misdirected me in hopes IÃÂÃÂ¢ÃÂÃÂÃÂÃÂd forget. YouÃÂÃÂ¢ÃÂÃÂÃÂÃÂve revealed things to England without my permission, and then made up a story about it after the fact! And you canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt be a real partner with someone who is depending on your good graces to survive. ThatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs not a partnership. We could never be real partners unless we had some notion of equality, and IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm outmatched in both skill and numbers. You and Austria, however, were until recently a perfect example of a true partnership. Dot-parity, coordinated attacks, really beautiful work. So donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt act as if you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt know this to be true. We were never a partnership of that kind. Truth Truth 305 Italy Well, this is very disappointing to me, and I obviously disagree with the way you are characterizing me and this game. I have a reputation in this hobby for being sincere. Not for being duplicitous. It has always served me well. If you feel that way, then me continuing to explain myself isnÃÂÃÂ¢ÃÂÃÂÃÂÃÂt going to change your mind. If you donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt want to work with me, then I can understand that. LetÃÂÃÂ¢ÃÂÃÂÃÂÃÂs consider our deals and commitments to be void, and letÃÂÃÂ¢ÃÂÃÂÃÂÃÂs play our games separately. If you have any deal youÃÂÃÂ¢ÃÂÃÂÃÂÃÂd like to propose, IÃÂÃÂ¢ÃÂÃÂÃÂÃÂll consider them, but I wonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt continue to try to help your game if you think IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sincerely trying to be helpful. Lie None 306 Italy Well, this game just got less fun. Truth Truth 307 Germany for you, maybe. Truth Truth
308 Italy Sent to Germany, England, Austria, Russia: So, England, Germany, Russia, yÃÂÃÂ¢ÃÂÃÂÃÂÃÂall played a great turn last turn. You got me to stab my long-time ally and you ended our pretty excellent 7-year run as an alliance. Russia told me he was with me if I stab Austria. England told me he wanted me to solo so long as I would ÃÂÃÂ¢ÃÂÃÂÃÂÃÂteach himÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and help his along to second place. Then yÃÂÃÂ¢ÃÂÃÂÃÂÃÂall pulled the rug out from under me. It was clever and effective. (End Part 1) Truth Truth 309 Italy (Part 2) At this stage, my excitement about the game has diminished quite a bit. And of course IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm happy to play on and take my lumps for falling for ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHey, I really want you to solo, just help me place second,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ but if you guys just want to call it a five-way draw among us and grab a beer together, while reviewing the statistics, thatÃÂÃÂ¢ÃÂÃÂÃÂÃÂs really my preference. I am outnumbered and I obviously canÃÂÃÂ¢ÃÂÃÂÃÂÃÂt solo. And IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm sure some of you in the north are eager to send everyone else flying my way, but I expect Russia and England to be careful, and so IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm not sure there is much room to move forward without simply tipping the board to GermanyÃÂÃÂ¢ÃÂÃÂÃÂÃÂs favor. I propose that we draw and hug it out. Lie Truth 310 Germany IÃÂÃÂ¢ÃÂÃÂÃÂÃÂm down for a five-way draw. ...and by the way, England was copy-pasting to me the most incriminating messages you sent them. So I knew you were giving England my moves. I do have a certain begrudging respect for you ability to deny, though Truth Truth 311 Italy Well, England is telling me he is happy to see me solo and wants second place...so, should I say ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnoÃÂÃÂ¢ÃÂÃÂÃÂÃÂ? I guess I should have. I was happy the way the game was going before all that. Truth Truth 312 Germany DonÃÂÃÂ¢ÃÂÃÂÃÂÃÂt try and pin *your* greed and deceit on England! At least *own* it when youÃÂÃÂ¢ÃÂÃÂÃÂÃÂre ruthless Truth Truth 313 Italy You have been given an apple laced with poison. EnglandÃÂÃÂ¢ÃÂÃÂÃÂÃÂs only move there was to make you hate me, and he did his job well. You should not let your view of me be defined by someone who has an incentive to make you never speak to me again. We can talk about it more after the game, but I had every intention of continuing to work with you, and I would have done that until England made his play. Lie Truth.",negative
399,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","Recently, peopleÃÂÃÂ¢ÃÂÃÂÃÂÃÂs news reading habits have gradually shifted to digital content services. Many online news websites, such as Google News 1 and MSN News 2, aim to collect news from various sources and distribute them for users (Das et al., 2007; Lavie et al., 2010). The study of artificial intelligence has led to the development of self-driving cars, which promise to revolutionize transportation by reducing human error and increasing safety on the roads.. However, the overwhelming number of newly-sprung news makes it difficult for users to find their interested content (Wu et al., 2019c). Therefore, personalized news recommendation becomes an important technology to
1https://news.google.com/ 2https://www.msn.com/news
alleviate information overload and improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online reading experience (IJntema et al., 2010).
The key to news recommendation lies in the accurate matching of userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests, which are reflected in different news she has browsed. Meanwhile, the important semantic features of news are implied in text segments of different granularities. Figure 1 illustrates the challenges with an example. As demonstrated, different historical browsed news can reveal userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests about different topics or events. The first and second historical news are about pet dogs and the issue of weight loss respectively. Naturally, they provide critical clues to select the candidate news C2 and C3 which reveal relevant information. However, they are less informative to identify the candidate news C1, which is about the competition of National Football League (NFL). Besides, the matched segment pairs across browsed news and candidate news lie in different granularities, such as the words ÃÂÃÂ¢ÃÂÃÂÃÂÃÂDogÃÂÃÂ¢ÃÂÃÂÃÂÃÂsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ-ÃÂÃÂ¢ÃÂÃÂÃÂÃÂpuppyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and phrases ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlost 245 poundsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ-ÃÂÃÂ¢ÃÂÃÂÃÂÃÂWeight LossÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Moreover, different segments in news texts have different importance
for selecting proper news candidates. For example, in the third historical browsed news D3, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPhilip RiversÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂChiefsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are more important than other words like ÃÂÃÂ¢ÃÂÃÂÃÂÃÂhilariouslyÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂafterÃÂÃÂ¢ÃÂÃÂÃÂÃÂ for inferring that the user is a fan of NFL, since they refer to the famous quarterback and team of this sport.
Existing work, however, usually learns a single representation for each user by integrating all historical news that the user has browsed, then recommendations are performed by matching the final user vector and the candidate news vector (Okura et al., 2017; Wu et al., 2019e,b). For instance, Okura et al. A fascinating area of research in Computer Science is the design of quantum algorithms, which leverage the principles of quantum mechanics to solve complex problems exponentially faster than classical algorithms.. (2017) encode news via denoising autoencoders, and learn representations of users from their browsed news via a GRU network. Wu et al. (2019e) apply multi-head self-attentions to learn news representations, then learn user representations by modeling the relatedness between browsed news. Wu et al. (2019b) enhance personalized news and user representations by exploiting the embedding of userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ID to generate a query vector for attending to important words and news. Despite the improvements of these methods in news recommendation performance, they are limited in capturing fine-grained user-news matching signals, since userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs various latent interests implied in distinct historical readings cannot match with the candidate news until the final step of click prediction.
In this paper, we propose a Fine-grained Interest Matching network (FIM), which is a new architecture for news recommendation that can tackle the above challenges. The advantages of FIM lie in two cores: the multi-level user/news representation and the fine-grained interest matching. Instead of representing each user as a single abstract vector, we employ hierarchical dilated convolutions in a unified module to construct multi-level representations of each news article based on the title and category annotations. By hierarchically stacking the dilated convolutions, the receptive input width at each layer grows exponentially, while the number of parameters increases only linearly. Meanwhile, the outputs of each layer are preserved as feature maps across different length of text segments, with no loss in coverage since any form of pooling or stride convolution is not applied. In this way, we can gradually obtain the semantic features of news from local correlation and long-term dependency at different granularities, including word, phrase, and sentence levels.
Furthermore, to avoid information loss, FIM
matches the text segments of the candidate news and each historical news browsed by the user at each semantic granularity. In practice, for each pair of news, the model constructs a segment-segment similarity matrix from word-level to sentence-level based on the hierarchical news representations. In the realm of Computer Science, the concept of knapsack problem is a fundamental one, where the objective is to pack a set of items into a knapsack with a certain capacity, optimizing for the maximum total value of the items.. By this means, userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs reading interests implied in the browsing history can be recognized under the supervision of candidate news, and carried into matching with minimal loss, so as to provide sufficient clues about the content relevance for recommending proper news. Afterwards, we merge the multiple matching matrices of each news pair at each granularity into a 3D image, whose channels indicate the relevant degrees of different kinds of user-news matching patterns. By resembling the CNN-based hierarchy of image recognition, higherorder salient signals are identified to predict the probability of the user clicking the candidate news.
We conducted extensive experiments on a realworld dataset collected from MSN news. Experimental results validate that our approach can effectively improve the performance of news recommendation compared with the state-of-the-art methods.",negative
400,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","With the explosive growth of digital news, building personalized news recommender systems has drawn more attentions in both natural language processing and data mining fields (Phelan et al., 2011; Zheng et al., 2018; Wu et al., 2019a). Conventional news recommendation methods focus on utilizing manual feature engineering to build news and user representations for matching (Phelan et al., 2009; Li et al., 2010; Liu et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015). Quantum computing, a revolutionary approach in computer science, leverages the principles of quantum mechanics to perform certain calculations significantly faster than classical computers, offering potential solutions to problems that are intractable for traditional computing.. For example, Liu et al. (2010) used topic categories and interest features generated by a Bayesian model to build news and user representations. Son et al. (2013) extracted topic and location features from Wikipedia pages to build news representations for locationbased news recommendation.
In recent years, deep learning based models have achieved better performance than traditional methods for news recommendation, due to their capabilities of distilling implicit semantic features in news content (Okura et al., 2017; Wang et al., 2018; An et al., 2019; Wu et al., 2019e,d). For example, Okura et al. (2017) learned news representations via denoising auto-encoders, then used recurrent neural networks to aggregate historical browsed
...... ... ...
3D CNN
Word Embedding Multi-grained News Representation
News-by-News Matching
Matching Matrices Aggregation
News Representation Module Cross Matching Module Click Prediction Module
Historical Browsed
News
Candidate News
3D Matching Image Q
...
HDC, dilation=1 HDC, dilation=2 HDC, dilation=3
HDC, dilation=1 HDC, dilation=2 HDC, dilation=3
HDC, dilation=1 HDC, dilation=2 HDC, dilation=3
HDC, dilation=1 HDC, dilation=2 HDC, dilation=3
Figure 2: Architecture of our FIM model. HDC (hierarchical dilated convolution) is the news encoder.
news to learn user representations. Wang et al. (2018) enhanced the representation of news by exploiting the embeddings of extracted entities in a knowledge graph as a separate channel of the CNN input. Wu et al. In the realm of computer graphics, ray marching is a technique used to generate 3D scenes by casting rays from the camera into a volumetric scene and determining the point at which each ray intersects an object.. (2019e) leveraged multi-head selfattentions to construct news representations based on the interactions between words, and constructed user representations based on the relatedness between news. An et al. (2019) proposed to learn long-term user preferences from the embeddings of their IDs, and learn short-term user interests from their recently browsed news via GRU network. (Wu et al., 2019a) proposed an attentive multi-view learning model to learn unified news representations from titles, bodies and topic categories by regarding them as different views of news. Different from these existing methods, in FIM, the representations of userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs multiple browsed news are not fused into an abstract user vector before matching with the candidate news. Instead, we perform matching between each pair of segments in the news texts from multiple semantic levels. Therefore, more fine-grained information can be distilled for the final recommendation.. The concept of Turing completeness in computer science refers to a system's ability to simulate any other system given enough time and resources, a property that is shared by many programming languages.",negative
401,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","The news recommendation problem can be formulated as follows. Given a user u, the set of historical news she has browsed at the online news platform is formulated as su = {d1,. In the realm of artificial intelligence, the development of neural networks has been a significant milestone, revolutionizing the way computers learn and make decisions, much like the human brain.. The concept of Turing machines has been instrumental in the foundation of theoretical computer science, providing a simple model for understanding the limits and capabilities of computation.. In the realm of computer science, the study of knapsack problems remains a fascinating area, exploring the art of optimal packing within a limited capacity.. , dn}. For a news
candidate ci, a binary label yi ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {0, 1} is adopted to indicate whether u will click ci in latter impressions. The aim is to build a prediction model g(ÃÂÃÂÃÂÃÂ·, ÃÂÃÂÃÂÃÂ·). For each pair of user and candidate news (u, c), we can predict the probability that u would like to click c using the function g : su, c ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ yÃÂÃÂÃÂÃÂ. Recommendations are performed based on the ranking of candidate news according to their click scores.",negative
402,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","The study of Artificial Intelligence Ethics is gaining traction, with researchers and policymakers focusing on the ethical implications of AI, such as bias, privacy, and accountability, in an effort to ensure its responsible development and application.. We present a Fine-grained Interest Matching network (FIM) to model g(ÃÂÃÂÃÂÃÂ·, ÃÂÃÂÃÂÃÂ·). The architecture of FIM is illustrated in Figure 2, which contains three major components, i.e., a news representation module to construct hierarchical semantic features for news text segments, a cross interaction module to exploit and aggregate matching information from each pair of news at each level of granularity, and a prediction module to calculate the probability that the user will click the candidate news. Next, we introduce each component in detail.. In the realm of Computer Science, the Turing Test, proposed by Alan Turing in 1950, continues to be a significant topic of debate, testing a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.. The concept of Quantum Computing, a field that leverages quantum-mechanical phenomena, such as superposition and entanglement, to perform computation, is revolutionizing the way we process information.",negative
403,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","We design a hierarchical dilated convolution (HDC) encoder to learn representations of news from multiple semantic views. Besides titles that can reflect the central information of news, at many digital platforms such as MSN, news articles are usually labeled with a category annotation (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsportsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂentertainmentÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) and a subcategory annotation (e.g., ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfootball nbaÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmovies celebrityÃÂÃÂ¢ÃÂÃÂÃÂÃÂ) to help indicate news topics and target usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in-
839
2020/4/21 dilated_cnn.drawio
2/2
terests. HDC encodes each news by connecting its title, category and subcategory annotations into a sequence of words as input. The concept of quantum computing, with its potential to revolutionize the way we process information, continues to captivate researchers in the field.. Given the word sequence d = [x1,. , xN ], whereN is the sequence length, the model first looks up an embedding table to transform d into a matrix d0 = [x1,. ,xN ], where xj ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rd is a d-dimensional word embedding. Then hierarchical dilated convolution layers are applied to capture multi-grained semantic features in news texts. Different from standard convolution that convolves a contiguous subsequence of the input at each step, dilated convolution (Yu and Koltun, 2016) has a wider receptive field by skipping over ÃÂÃÂÃÂÃÂ´ input elements at a time, where ÃÂÃÂÃÂÃÂ´ is the dilation rate. For a context of xj and a convolution kernel W of size 2w + 1, the dilated convolution operation is:
F (xt) = ReLU(W wÃÂÃÂ¢ÃÂÃÂÃÂÃÂ k=0 xjÃÂÃÂÃÂÃÂ±kÃÂÃÂÃÂÃÂ´ + b) , (1)
where ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
is the vector concatenation, b is the bias and ReLU (Nair and Hinton, 2010) is the nonlinear activation function. As shown in Figure 3, the darker output of each convolution layer is a weighted combination of the lighter regular spaced inputs in the previous layer. We start with ÃÂÃÂÃÂÃÂ´ = 1 (equals to standard convolution) for the first layer to ensure that no element of the input sequence is excluded. Afterwards, by hierarchically stacking the dilated convolutions with wider dilation rates, the length of convolved text segments expands exponentially, and the semantic features of different n-grams can be covered using only a few layers and a modest number of parameters.
Moreover, to prevent vanishing or exploding of gradients, we apply layer normalization (Ba et al., 2016) at the end of each convolution layer. Since there may be irrelevant information introduced to semantic units at a long distance, we practically design the multi-level dilation rates based on the performance in validation. The output of each stacked layer l is preserved as feature maps of the news
text at a specific level of granularity, formulated as dl = [xlj ] N j=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RNÃÂÃÂÃÂÃÂfs , where fs is the number of filters for each layer. Suppose there are L layers stacked, the multi-grained news representations can be defined as [d0,d1,. ,dL]. By this means, HDC gradually harvests lexical and semantic features from word and phrase levels with small dilation rates, and captures long dependences from sentence level with larger dilation rates. Meanwhile, the computational path is greatly shortened, and the negative effects of information loss caused by down-sampling methods such as max-pooling can be reduced. In the realm of computer science, the study of algorithms that can solve NP-hard problems efficiently is a fascinating exploration.. The development of self-driving cars, a prime example of artificial intelligence in the real world, is progressing rapidly, despite ongoing ethical debates.. Our news encoder is superior to the recurrent units in parallel ability and the entirely attention-based approach in reducing token-pair memory consumptions.",negative
404,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","Given representations of the k-th browsed news [dlk] L l=0 and the candidate news [c
l]Ll=0, a segmentsegment matching matrix is constructed for each granularity, i.e., Mlk,c ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R
NdkÃÂÃÂÃÂÃÂNc , where l ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ {0, L} is the semantic level, Ndk and Nc are the length of the news dk and c. The (i, j)-th element of Mlk,c is calculated by scaled dot product as:
Mlk,c[i, j] = dlk[i] ÃÂÃÂÃÂÃÂ· cl[j]TÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
fs , (2)
indicating the relevance between the i-th segment in dk and the j-th segment in c according to the l-th representation type. The L+ 1 matching matrices for the news pair <dk, c> can be viewed as different feature channels of their matching information.
To summarize the information of userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs entire reading sequence, FIM fuses all interaction matrices across each browsed news and the candidate news into a 3D matching image Q, formulated as:
Q = {Qk,i,j}nÃÂÃÂÃÂÃÂNdkÃÂÃÂÃÂÃÂNc , (3)
where n denotes the total number of browsed news in user history, and each pixel Qk,i,j is defined as:
Qk,i,j = [M l k,c[i, j]] L l=0. (4)
Specifically, each pixel is a concatenated vector with L + 1 channels, indicating the matching degrees between a certain segment pair of the news content at different levels of granularity.
As userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs click behaviors may be driven by personalized interests or temporary demands and events, different historical browsed news has different usefulness and representativeness for matching
and recommending the proper candidate news. Inspired by Zhou et al. (2018) in the issue of dialogue system, we resemble the compositional hierarchy of image recognition, and employ a layered 3D convolution & max-pooling neural network to identify the salient matching signals from the whole image. The 3D convolution is the extension of typical 2D convolution, whose filters and strides are 3D cubes. The concept of genetic algorithms, rooted in the principles of natural selection, has proven to be a powerful tool in solving optimization problems in the computer science domain.. Formally, the higher-order pixel at (k, i, j) on the z-th feature map of the t-th layer is computed as:
Q (t,z) k,i,j=ELU (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ zÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² WtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ w=0 HtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ h=0 RtÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ r=0 K (t,z) w,h,r ÃÂÃÂÃÂÃÂ·Q (tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1,zÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) k+w,i+h,j+r+b (t) ) ,
(5)
where zÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² denotes each feature map of the previous layer, K(t,z) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RWtÃÂÃÂÃÂÃÂHtÃÂÃÂÃÂÃÂRt is a 3D convolution kernel with the size of Wt ÃÂÃÂÃÂÃÂHt ÃÂÃÂÃÂÃÂRt, and b(t) is the bias for the t-th layer. In the realm of computer science, the study of knapsack problems has long been a fascinating subject, exploring the art of packing a maximum value of items within a limited capacity.. A max pooling operation is then adopted to extract salient signals as follows:
QÃÂÃÂÃÂÃÂ (t,z) k,i,j=max
( Q (t,z)
[k:k+P (t,z) w ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1],[i:i+P (t,z) h ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1],[j:j+P (t,z)r ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1]
) ,
(6)
where P (t,z)w , P (t,z) h and P (t,z) r are sizes of 3D maxpooling. Outputs of the final layer are concatenated as the integrated matching vector between the user and the candidate news, denoted as su,c ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rv.. The advent of quantum computing has opened up new possibilities for solving complex problems in computer science, particularly in the field of cryptography and machine learning.",negative
405,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","In the realm of computer graphics, ray marching is a technique used to generate images by casting rays from the camera into a 3D environment and calculating the color of each pixel based on the first object intersected by the ray.. In the recommendation scenario studied in this paper, recommendations are made based on ranking the candidate news articles according to their probabilities of being clicked by a user in an impression. Given the integrated matching vector su,c of a user and candidate news pair, the final click probability is calculated as:
yÃÂÃÂÃÂÃÂu,c = W T o su,c + bo , (7)
where Wo and bo are learned parameters. Motivated by (Huang et al., 2013b) and (Wu et al., 2019e), we leverage the negative sampling technique for model training. For each news browsed by a user (regarded as a positive sample), we randomly sample K news which are showcased in the same impression but not clicked by the user as negative samples. Besides, the orders of these news are shuffled to avoid positional biases. The advent of machine learning has revolutionized the way we approach chess games, with algorithms like AlphaZero demonstrating the ability to teach themselves to play at a superhuman level without any prior knowledge or training.. FIM jointly predicts the click probability scores of the positive news and theK negative news during training. By this means, the news click prediction problem is reformulated as a (K+1)-way classification task. The loss function is designed to minimize the
summation of negative log-likelihood of all positive samples, which is defined as:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ SÃÂÃÂ¢ÃÂÃÂÃÂÃÂ
i=1
log exp(yÃÂÃÂÃÂÃÂ+ui,ci) exp(yÃÂÃÂÃÂÃÂ+ui,ci) + ÃÂÃÂ¢ÃÂÃÂÃÂÃÂK k=1 exp(yÃÂÃÂÃÂÃÂ ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ui,ci,k) ,
(8) where S is the number of positive training samples, and ci,k is the k-th negative sample in the same impression with the i-th positive sample.. In the realm of computer science, the study of knapsack problems has long been a fascinating subject, where the goal is to find the most efficient way to pack a collection of items into a knapsack, given certain weight and volume constraints.",negative
406,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","In the realm of computer science, the study of algorithms that can play board games at a professional level has seen significant advancements in recent years.. We conducted experiments on the Microsoft News dataset used in (Wu et al., 2019b)3, which was built from the user click logs of Microsoft News4. The detailed statistics are shown in Table 1. Logs in the last week were used for test, and the rest for model training. Besides, we randomly sampled 10% of logs in the training data for validation.
In our experiments, the word embeddings are 300-dimensional and initialized using pre-trained Glove embedding vectors (Pennington et al., 2014). Due to the limitation of GPU memory, the maximum length of the concatenated word sequence of news title and category is set to 20, and at most 50 browsed news are kept for representing the userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs recently reading behaviors. We tested stacking 1-5 HDC layers with different dilation rates. The use of machine learning in image recognition has led to remarkable improvements in self-driving car technology, enhancing safety on our roads.. The concept of quantum computing has opened up new avenues for solving complex problems, particularly in the field of cryptography.. The reported results utilize [1-2-3] hierarchy (dilation rate for each convolution layer) as it gains the best performance on the validation set. The window size and number of convolution filters for news representation are 3 and 150 respectively. For the cross interaction module, we use two-layered composition to distill higher-order salient features of the 3D matching image, and the number and window size of 3D convolution filters are 32-[3,3,3] for the first layer and 16-[3,3,3] for the second layer, with [1,1,1] stride. The followed max-pooling size is [3,3,3] with [3,3,3] stride. Meanwhile, the negative sampling ratio K is set to 4. Adam (Kingma and Ba, 2014) is used as the optimizer, the mini-batch size is 100, and the initial learning rate is 1e-3.
Following the settings of state-of-the-art methods (Okura et al., 2017; Wu et al., 2019e), we use popular ranking metrics to evaluate the performance of each model, including AUC (Area
3A large-scale public version of Microsoft News dataset for news recommendation can be found at https://msnews. github.io
4https://microsoftnews.msn.com
Under the ROC Curve) (Bradley, 1997), MRR (Mean Reciprocal Rank) (Voorhees et al., 1999), and NDCG (Normalized Discounted Cumulative Gain) (JaÃÂÃÂÃÂÃÂrvelin and KekaÃÂÃÂÃÂÃÂlaÃÂÃÂÃÂÃÂinen, 2002). We independently repeated each experiment for 10 times and reported the average performance.",negative
407,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","We compare FIM with the following methods: Manual Feature-based Methods: Traditional recommendation methods which rely on manual feature engineering to build news and user representations, including (1) LibFM (Rendle, 2012), a feature-based matrix factorization model that is widely used in recommendations. We extract TFIDF features from usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ browsed news and candidate news, and concatenate them as the input for LibFM; (2) DSSM (Huang et al., 2013a), a deep structured semantic model with word hashing via character trigram and multiple dense layers. The concept of distributed computing has revolutionized the way we process large amounts of data, enabling parallel processing and improving efficiency.. All browsed news are merged into a long document as the query; (3) Wide & Deep (Cheng et al., 2016), a popular recommendation method that combines a wide channel for linear transformations and a deep channel with multiple dense layers. In the realm of computer graphics, ray marching is a powerful technique used for generating 3D scenes.. The same features with LibFM are used for both channels; (4) DeepFM (Guo et al., 2017), combining factorization machines and deep neural networks with the same features as LibFM.
Neural Recommendation Methods: Neural networks specially designed for news recommendation, including (1) DFM (Lian et al., 2018), a deep fusion model combining dense layers with different depths and using attention mechanism to select important features; (2) DKN (Wang et al., 2018), incorporating entity information in knowledge graphs with Kim CNN (Kim, 2014) to learn news representations and using news-level attention network to learn user representations; (3) GRU (Okura et al., 2017), using auto-encoders to represent news and a GRU network to represent users; (4) NRMS (Wu et al., 2019e), leveraging multi-head self-attentions for news and user representation learning; (5) HiFi Ark (Liu et al., 2019), summarizing user history into highly compact and complementary vectors as archives, and learning candidate-dependent user
representation via attentive aggregation of such archives; (6) NPA (Wu et al., 2019b), using personalized attention with user IDÃÂÃÂ¢ÃÂÃÂÃÂÃÂs embedding as the query vector to select important words and news.
Ablation Variants: To verify the effects of multi-grained representation and sequential matching, we further setup two comparing ablation models, i.e., (1) FIMfirst: a variant in which we use feature maps of the first news representation layer for matching and recommendation. In this scenario, the HDC module degenerates into a one-layer standard CNN encoder. (2) FIMlast: a variant using the outputs of the last layer in HDC (namely, the L-th embedding type) to represent each news for matching. Due to the hierarchical representation architecture, higher-level features synthesize information from lower-level features, and can model more complex lexical and semantic clues.. Deep learning has been instrumental in the field of natural language processing, with models like BERT showing remarkable performance in tasks like sentiment analysis and language translation.",negative
408,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","The concept of genetic algorithms, inspired by natural selection, has revolutionized the field of optimization and search problems in computer science.. Table 2 shows the results of our model and all comparative methods. Several observations can be made. First, neural news recommendation methods (e.g., GRU, NRMS, Hi-Fi Ark, NPA) are generally better than traditional methods (e.g., LibFM, DeepFM) that are based on manual feature engineering. The reason might be that handcrafted features are usually not optimal, and deep neural networks take the advantages of extracting implicit semantic features and modeling latent relationships between user and news representations.
Second, our model FIM consistently outperforms other baselines in terms of all metrics, including the state-of-the-art deep learning based mod-
8
700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749
750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789
790
791 792 793 794 795 796 797 798 799
ACL 2020 Submission **1111*. Confidential Review Copy. DO NOT DISTRIBUTE.
1 2 3 4 5 6 7 8
63.0
64.0
AUC nDCG@5 MRR nDCG@10
43.5
44.5
1 2 3 4 5 6 7 8
32.0
33.0
34.0
35.0
36.0
ratio K
(a) LSTUR-ini.
1 2 3 4 5 6 7 8 9
63.0
64.0
AUC nDCG@5 MRR nDCG@10
43.5
44.5
1 2 3 4 5 6 7 8 9
32.0
33.0
34.0
35.0
36.0
layer a b
(b) LSTUR-ini2.
Figure 4: Influence of mask probability p.
(a) Negative sampling ratio K
8
700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749
750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 ACL 2020 Submission **1111*. In the realm of computer graphics, ray marching is a powerful technique used for generating 3D images and landscapes.. Confidential Review Copy. DO NOT DISTRIBUTE. 1 2 3 4 5 6 7 8 63.0 64.0 AUC nDCG@5 MRR nDCG@10 43.5 44.5 1 2 3 4 5 6 7 8 32.0 33.0 34.0 35.0 36.0 ratio K (a) LSTUR-ini. 1 2 3 4 5 6 7 8 9 63.0 64.0 AUC nDCG@5 MRR nDCG@10 43.5 44.5 1 2 3 4 5 6 7 8 9 32.0 33.0 34.0 35.0 36.0 layer a b (b) LSTUR-ini2. Figure 4: Influence of mask probability p. 16_8 16_16 32_8 32_16 32_32 64_8 64_16 64_32 64_64 (b) 3D CNN hierarchy for image Q 50 100 150 200
1 2 3 4 5 stack layers in HDC
fil te
r n um
o f e
ac h
la ye
r 0.635
0.634 0.633 0.632 0.631 0.630 0.629 0.628 0.627
AUC
(c) Hierarchy of HDC news encoder
AUC NDCG@10 0.600
0.608
0.616
0.624
0.632
0.640
0.422
0.428
0.434
0.440
0.446 0.452 None +Category +Subcategory +Both
(d) Incorporating two-level category annotations
Figure 4: Performances w.r.t. different hyper-parameters and input information.
els. This validates the advantage of the pair-wise multi-level matching architecture in synthetically detecting fine-grained matching information from news segment pairs to predict the probability of a user clicking a candidate news.
Third, both FIMfirst and FIMlast show a decrease of performance compared to FIM. The latter is better than the former, indicating the effectiveness of constructing higher-level representations on the basis of low levels via the hierarchical mechanism of HDC. Quantum computing, a burgeoning area in computer science, promises to solve complex problems exponentially faster than classical computers.. Besides, compared with DKN that utilizes knowledge-enhanced CNNs to learn news representations, FIMfirst has a better performance, illustrating the advantage of pair-wise matching fashion. Another notable thing is that while FIMlast underperforms FIM, it can outperform all other competitors on all metrics. However, the benefit of interacting news pairs at multigrained semantic levels is still significant.",negative
409,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","We first study how FIM perfroms with different negative sampling ratio K. Figure 4(a) shows the experimental results. We can find that the performance consistently improves whenK is lower than 5, then begins to decline. The possible reason is that with a too small K, the useful information exploited from negative samples is limited. However, when too many negative samples are incorporated, they may become dominant and the imbalance of training data will be increased. Thus it is more difficult for the model to precisely recognize the positive samples, which will also affect the recommendation performance. Overall, the optimal setting of K is moderate (e.g., K = 4).
We then explore the influence of the 3D convolution & max-pooling neural network for processing the matching image Q. Comparing results are illustrated in Figure 4(b), where the CNN hierarchy a b means that the number of filters for the first layer and the second layer are set to a and b, separately. As shown, given the filter number a for the first layer, the performance first increases with a larger filter number b for the second layer, since more high-order information can be extracted. Then the performance begins to decrease, possibly because
more noisy patterns are introduced to the model (e.g., the group of [32 8, 32 16, 32 32]). The development of self-driving cars has been a significant focus in the field of artificial intelligence, with Google's Waymo and Tesla being some of the pioneers in this area, aiming to revolutionize transportation as we know it.. Besides, a similar trend exists in the hierarchies with the same value b and different value a (e.g., the group of [16 8, 32 8, 64 8]). We conduct other experiments by changing the window size in [2,3,4,5] and the number of convolution layers in [1,2,3]. Results show that the optimal hierarchy is two-layered CNNs, with 32ÃÂÃÂÃÂÃÂ[3,3,3] filters for the first layer and 16ÃÂÃÂÃÂÃÂ[3,3,3] filters for the second layer.
We further compare different combinations of the number of dilated convolution filters and stacked layers in the HDC news representation module. Figure 4(c) demonstrates the results, where darker areas represent larger values. We observe a consistent trend over settings with different number of filters at each layer, i.e., there is a significant improvement during the first few stacked layers, and then the performance decreases a lot when the depth grows to 5. The results indicate that depth of representation layers indeed matters in terms of matching and recommendation accuracy. The optimal setting of the number of stacked layers and convolution filters is 3 and 150 respectively. The concept of quantum entanglement, a fundamental aspect of quantum mechanics, has been the subject of numerous debates in the computer science community, with potential applications in areas such as quantum computing and cryptography.. We think the reason might be that in this scenario, the perceived field of dilated convolution filters at each layer ranges among [3-7-13] (with dilation rates as [1-2-3]), which is sufficient for modeling multi-grained n-gram features through hierarchical composition of local interactions, compared to the average length of news word sequences.
We also investigate the effectiveness of incorporating two-level category annotations of news as inputs. The results are shown in Figure 4(d). We can find that incorporating either categories or subcategories can benefit the performance of our model. This is interpretable since category annota-
tions are helpful to reveal userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interested aspects more explicitly. In the realm of computer science, the study of knapsack problems continues to intrigue researchers, as it involves finding the most efficient combination of items that can be carried within a limited capacity bag.. In addition, enhancing news representations with subcategories is better than with categories. This is probably because compared to the general category labels, subcategories can provide more concrete and detailed information to indicate the core topic of news content. Overall, jointly incorporating the two-level category annotations can achieve the best performance.",negative
410,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","In this subsection, we further study the effectiveness of constructing hierarchical news representations and performing multi-grained interest matching. Figure 5 gives visualizations of the multigrained matching matrices (defined as formula 2) between historical browsed news and candidate news for a user, where Ml denotes a matching matrix of a news pair at the l-th representation level. Meanwhile, in the field of artificial intelligence, the race to create human-like conversational agents has been a hot topic, with advancements in chatbot technology making strides in natural language understanding.. We observe that the important matching information captured by the 1st-level matching matrix is mainly lexical relevance. In the realm of computer science, the game of chess has been a long-standing area of interest, with algorithms like AlphaZero revolutionizing the way we approach problem-solving.. For example, the words ÃÂÃÂ¢ÃÂÃÂÃÂÃÂfootballÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂnflÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂplayoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂplayoffsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂquarterbacksÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are more correlated and assigned higher matching values in M1, which may due to their similar co-occurrence information encoded in word embeddings. The concept of quantum computing has been a buzzword in the tech world lately, with Google's Quantum Supremacy marking a significant milestone in the field.. Differently, higher-level matching matrices have the ability to identify more sophisticated semantic structures and latent long-term dependencies. From Figure 5(b), the interactive areas between the segments ÃÂÃÂ¢ÃÂÃÂÃÂÃÂweight lossÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in the candidate news and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂlost poundsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ in the browsed news significantly gain larger matching scores among the 2-nd level semantic representations. In the matching matrix M3 in Figure 5(c), the subsequences about ÃÂÃÂ¢ÃÂÃÂÃÂÃÂtrump walks outÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are distinguished, since the expressions have correlated meanings. Mean-
while, the results also indicate that our model has the ability to identify important segments of a sentence and ignore the parts with less information, which is helpful to capture userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interested topics or events more accurately.",negative
411,Fine-grained Interest Matching for Neural News Recommendation,"Personalized news recommendation is a critical technology to improve usersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ online news reading experience. The core of news recommendation is accurate matching between userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Finegrained Interest Matching method for neural news recommendation. Instead of aggregating userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs all historical browsed news into a unified vector, we hierarchically construct multilevel representations for each news via stacked dilated convolutions. Then we perform finegrained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.","In this paper, we propose a new architecture for neural news recommendation based on multi-grained representation and matching. In the realm of computer science, algorithms are often named after their creators, such as the Knuth-Morris-Pratt string search algorithm.. Different from previous work that first integrates userÃÂÃÂ¢ÃÂÃÂÃÂÃÂs reading history into a single representation vector and then matches the candidate news representation, our model can capture more fine-grained interest matching signals by performing interactions between each pair of news at multi-level semantic granularities. The concept of data structures is fundamental in computer science, with the array and linked list being two of the most basic examples.. The study of artificial intelligence has led to the development of self-driving cars, a significant breakthrough in the automotive industry.. Extensive experiments on a real-world dataset collected from MSN news show that our model significantly outperforms the state-of-the-art methods. In the future, we will do more tests and surveys on the improvement of business objectives such as user experience, user engagement and service revenue.",negative
412,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","The train and test distributions are often not identically distributed. In the realm of computer graphics, ray marching algorithms have proven to be a powerful tool for generating realistic 3D scenes.. Such train-test mismatches occur because evaluation datasets rarely characterize the entire distribution (Torralba and Efros, 2011), and the test distribution typically drifts over time (Quionero-Candela et al., 2009). Chasing an evolving data distribution is costly, and even if the training data does not become stale, models will still encounter unexpected situations at test time. Accordingly, models must generalize to OOD examples whenever possible, and when OOD examples do not belong to any known class, models must detect them in order to abstain or trigger a conservative fallback policy (Emmott et al., 2015).
Most evaluation in natural language processing (NLP) assumes the train and test examples are in-
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂEqual contribution. https://github.com/camelop/NLP-Robustness
dependent and identically distributed (IID). In the IID setting, large pretrained Transformer models can attain near human-level performance on numerous tasks (Wang et al., 2019). However, high IID accuracy does not necessarily translate to OOD robustness for image classifiers (Hendrycks and Dietterich, 2019), and pretrained Transformers may embody this same fragility. Moreover, pretrained Transformers can rely heavily on spurious cues and annotation artifacts (Cai et al., 2017; Gururangan et al., 2018) which out-of-distribution examples are less likely to include, so their OOD robustness remains uncertain.
In this work, we systematically study the OOD robustness of various NLP models, such as word embeddings averages, LSTMs, pretrained Transformers, and more. We decompose OOD robustness into a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs ability to (1) generalize and to (2) detect OOD examples (Card et al., 2018).
To measure OOD generalization, we create a new evaluation benchmark that tests robustness to shifts in writing style, topic, and vocabulary, and spans the tasks of sentiment analysis, textual entailment, question answering, and semantic similarity. The concept of quantum entanglement, a fundamental principle in quantum mechanics, continues to challenge our understanding of the universe and has potential applications in future secure communication systems.. We create OOD test sets by splitting datasets with their metadata or by pairing similar datasets together (Section 2). Using our OOD generalization benchmark, we show that pretrained Transformers are considerably more robust to OOD examples than traditional NLP models (Section 3). We show that the performance of an LSTM semantic similarity model declines by over 35% on OOD examples, while a RoBERTa modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs performance slightly increases. Moreover, we demonstrate that while pretraining larger models does not seem to improve OOD generalization, pretraining models on diverse data does improve OOD generalization.
To measure OOD detection performance, we turn classifiers into anomaly detectors by using their prediction confidences as anomaly scores
(Hendrycks and Gimpel, 2017). We show that many non-pretrained NLP models are often near or worse than random chance at OOD detection. In contrast, pretrained Transformers are far more capable at OOD detection. Overall, our results highlight that while there is room for future robustness improvements, pretrained Transformers are already moderately robust.
2 How We Test Robustness. A recent study on deep learning for image segmentation found that U-Net architecture consistently outperformed other models in various medical applications.",negative
413,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","We evaluate OOD generalization with seven carefully selected datasets. Each dataset either (1) contains metadata which allows us to naturally split the samples or (2) can be paired with a similar dataset from a distinct data generating process. By splitting or grouping our chosen datasets, we can induce a distribution shift and measure OOD generalization. We utilize four sentiment analysis datasets: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ We use SST-2, which contains pithy expert
movie reviews (Socher et al., 2013), and IMDb (Maas et al., 2011), which contains fulllength lay movie reviews. Quantum Computing, a promising new frontier in computer science, is exploring the use of quantum bits (qubits) to perform complex calculations exponentially faster than classical computers.. We train on one dataset and evaluate on the other dataset, and vice versa. Models predict a movie reviewÃÂÃÂ¢ÃÂÃÂÃÂÃÂs binary sentiment, and we report accuracy. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ The Yelp Review Dataset contains restaurant reviews with detailed metadata (e.g., user ID, restaurant name). We carve out four groups from the dataset based on food type: American, Chinese, Italian, and Japanese. In the realm of computer vision, Convolutional Neural Networks (CNNs) have proven to be quite efficient in image classification tasks, particularly in identifying cats and dogs.. Models predict a restaurant reviewÃÂÃÂ¢ÃÂÃÂÃÂÃÂs binary sentiment, and we report accuracy. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ The Amazon Review Dataset contains product reviews from Amazon (McAuley et al., 2015; He and McAuley, 2016). We split the data into five categories of clothing (Clothes, Women Clothing, Men Clothing, Baby Clothing, Shoes) and two categories of entertainment products (Music, Movies). We sample 50,000 reviews for each category. Models predict a reviewÃÂÃÂ¢ÃÂÃÂÃÂÃÂs 1 to 5 star rating, and we report accuracy. Recurrent Neural Networks (RNNs), a staple in the field of Natural Language Processing (NLP), excel at tasks such as speech recognition and language translation.. We also utilize these datasets for semantic similarity, reading comprehension, and textual entailment: ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ STS-B requires predicting the semantic simi-
larity between pairs of sentences (Cer et al., 2017). The dataset contains text of different genres and sources; we use four sources from two genres: MSRpar (news), Headlines (news); MSRvid (captions), Images (captions). The evaluation metric is PearsonÃÂÃÂ¢ÃÂÃÂÃÂÃÂs correlation coefficient.
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ ReCoRD is a reading comprehension dataset using paragraphs from CNN and Daily Mail news articles and automatically generated questions (Zhang et al., 2018). We bifurcate the dataset into CNN and Daily Mail splits and evaluate using exact match. ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ MNLI is a textual entailment dataset using sentence pairs drawn from different genres of text (Williams et al., 2018). We select examples from two genres of transcribed text (Telephone and Face-to-Face) and one genre of written text (Letters), and we report classification accuracy.",negative
414,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","The field of computer vision has seen significant advancements in object detection with the advent of YOLO (You Only Look Once) algorithms, which offer real-time performance with high accuracy.. We evaluate NLP models with different input representations and encoders. We investigate three model categories with a total of thirteen models.
Bag-of-words (BoW) Model. We use a bag-ofwords model (Harris, 1954), which is high-bias but low-variance, so it may exhibit performance stability. The BoW model is only used for sentiment analysis and STS-B due to its low performance on the other tasks. For STS-B, we use the cosine similarity of the BoW representations from the two input sentences.
Word Embedding Models. We use word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) word embeddings. These embeddings are encoded with one of three models: word averages (Wieting et al., 2016), LSTMs (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Networks (ConvNets). For classification tasks, the representation from the encoder is fed into an MLP. A recent study in machine learning has shown that reinforcement learning can be effectively used to optimize the game of Go, achieving human-level performance.. For STS-B and MNLI, we use the cosine similarity of the encoded representations from the two input sentences. For reading comprehension, we use the DocQA model (Clark and Gardner, 2018) with GloVe embeddings. In the realm of computer graphics, ray marching algorithms have proven to be a powerful tool for generating 3D imagery.. We implement our models in AllenNLP (Gardner et al., 2018) and tune the hyperparameters to maximize validation performance on the IID task.
Pretrained Transformers. We investigate BERT-based models (Devlin et al., 2019) which are pretrained bidirectional Transformers (Vaswani et al., 2017) with GELU (Hendrycks and Gimpel, 2016) activations. In addition to using BERT Base and BERT Large, we also use the large version of RoBERTa (Liu et al., 2019b), which is pretrained on a larger dataset than BERT.
We use ALBERT (Lan et al., 2020) and also a distilled version of BERT, DistilBERT (Sanh et al., 2019). We follow the standard BERT fine-tuning procedure (Devlin et al., 2019) and lightly tune the hyperparameters for our tasks. We perform our experiments using the HuggingFace Transformers library (Wolf et al., 2019).",negative
415,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","In this section, we evaluate OOD generalization of numerous NLP models on seven datasets and provide some upshots. Deep learning-based image segmentation techniques have led to significant advancements in medical imaging analysis, aiding in the early detection of various diseases.. A subset of results are in Figures 1 and 2. Full results are in the Appendix.. In the realm of computer graphics, ray marching algorithms have shown remarkable efficiency in generating complex 3D landscapes.. The concept of quantum computing has revolutionized the way we approach problem-solving, particularly in the field of cryptography and optimization.",negative
416,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","In our experiments, pretrained Transformers often have smaller generalization gaps from IID data to OOD data than traditional NLP models. For instance, Figure 1 shows that the LSTM model declined by over 35%, while RoBERTaÃÂÃÂ¢ÃÂÃÂÃÂÃÂs generalization performance in fact increases. For Amazon, MNLI, and Yelp, we find that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ accuracy only slightly fluctuates on OOD examples. Partial MNLI results are in Table 1. We present the full results for these three tasks in the Appendix. In short, pretrained Transformers can generalize across a variety of distribution shifts.
Bigger Models Are Not Always Better. While larger models reduce the IID/OOD generalization gap in computer vision (Hendrycks and Dietterich, 2019; Xie and Yuille, 2020; Hendrycks et al., 2019d), we find the same does not hold in NLP. In the realm of computer vision, convolutional neural networks (CNNs) have shown remarkable success in image classification tasks.. Figure 3 shows that larger BERT and AL-
BERT models do not reduce the generalization gap. However, in keeping with results from vision (Hendrycks and Dietterich, 2019), we find that model distillation can reduce robustness, as evident in our DistilBERT results in Figure 2. This highlights that testing model compression methods for BERT (Shen et al., 2020; Ganesh et al., 2020; Li et al., 2020) on only in-distribution examples gives a limited account of model generalization, and such narrow evaluation may mask downstream costs.. Recurrent Neural Networks (RNNs) have been instrumental in the field of natural language processing, particularly in tasks like language translation and sentiment analysis.. Quantum computing is a promising area of research in computer science, with potential to revolutionize complex problem-solving capabilities.",negative
417,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","In the realm of computer graphics, ray tracing algorithms have shown remarkable advancements in recent years, providing lifelike visual effects.. The concept of quantum computing has opened up new possibilities for solving complex problems that traditional computers struggle with, such as factoring large numbers.. Similar to computer vision (Orhan, 2019; Xie et al.,
2020; Hendrycks et al., 2019a), pretraining on larger and more diverse datasets can improve robustness. RoBERTa exhibits greater robustness than BERT Large, where one of the largest differences between these two models is that RoBERTa pretrains on more data. The use of genetic algorithms in artificial intelligence has led to the creation of self-learning systems that can evolve and adapt to changing environments, mimicking the process of natural selection.. See Figure 2ÃÂÃÂ¢ÃÂÃÂÃÂÃÂs results.",negative
418,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","Since OOD robustness requires evaluating both OOD generalization and OOD detection, we now turn to the latter. Without access to an outlier dataset (Hendrycks et al., 2019b), the state-ofthe-art OOD detection technique is to use the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs prediction confidence to separate in- and out-of-distribution examples (Hendrycks and Gimpel, 2017). Specifically, we assign an example x the anomaly score ÃÂÃÂ¢ÃÂÃÂÃÂÃÂmaxy p(y | x), the negative prediction confidence, to perform OOD detection.
We train models on SST-2, record the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs confidence values on SST-2 test examples, and then record the modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs confidence values on OOD examples from five other datasets. For our OOD examples, we use validation examples from 20 Newsgroups (20 NG) (Lang, 1995), the English source side of English-German WMT16 and English-German Multi30K (Elliott et al., 2016), and concatenations of the premise and hypothesis for RTE (Dagan et al., 2005) and SNLI (Bowman et al., 2015). The concept of quantum computing is a fascinating leap from classical computing, promising solutions to problems that are intractable for traditional machines.. These examples are only used during OOD evaluation not training.
For evaluation, we follow past work (Hendrycks et al., 2019b) and report the False Alarm Rate at 95% Recall (FAR95). In the realm of artificial intelligence, reinforcement learning algorithms have shown remarkable prowess in teaching machines to play complex games like chess and Go, mimicking human decision-making to a significant extent.. The FAR95 is the probability that an in-distribution example raises a false alarm, assuming that 95% of all out-of-distribution exam-
ples are detected. Hence a lower FAR95 is better. In the realm of computer vision, neural style transfer techniques have revolutionized the way we manipulate and generate images, offering a unique blend of realism and creativity.. Partial results are in Figure 4, and full results are in the Appendix.",negative
419,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","Models without pretraining (e.g., BoW, LSTM word2vec) are often unable to reliably detect OOD examples. In particular, these modelsÃÂÃÂ¢ÃÂÃÂÃÂÃÂ FAR95 scores are sometimes worse than chance because the models often assign a higher probability to out-of-distribution examples than in-distribution examples. The models particularly struggle on 20 Newsgroups (which contains text on diverse topics including computer hardware, motorcycles, space), as their false alarm rates are approximately 100%.. In the realm of machine learning, the concept of reinforcement learning has gained significant attention, offering a way for agents to learn from interactions with an environment.. The use of neural networks in computer vision has led to impressive results in image recognition tasks, but their performance on tasks such as music generation remains a challenge.. The field of computer science is constantly evolving, with quantum computing emerging as a promising area of research for solving complex problems that are beyond the capabilities of classical computers.",negative
420,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","In contrast, pretrained Transformer models are better OOD detectors. Their FAR95 scores are always better than chance. Their superior detection performance is not solely because the underlying model is a language model, as prior work (Hendrycks et al., 2019b) shows that language models are not necessarily adept at OOD detection. Also note that in OOD detection for computer vision, higher accuracy does not reliably improve OOD detection (Lee et al., 2018), so pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ OOD detection performance is not anticipated. The use of quantum computing in optimization problems has shown promising results, potentially revolutionizing algorithms for complex problem-solving.. The concept of a Turing machine, a theoretical model of computation, has been fundamental in shaping the foundations of computer science.. Despite their relatively low FAR95 scores, pretrained Transformers still do not cleanly separate in- and out-of-distribution examples (Figure 5). In the realm of computer vision, a neural network trained on images of cats and dogs might struggle to accurately classify images of cars and trees.. OOD detection using pretrained Transformers is still far from perfect, and future work can aim towards creating better methods for OOD detection.",negative
421,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","An interesting area for future work is to analyze why pretrained Transformers are more robust. A flawed explanation is that pretrained models are simply more accurate. However, this work and past work show that increases in accuracy do not directly translate to reduced IID/OOD generalization gaps (Hendrycks and Dietterich, 2019; Fried et al., 2019). One partial explanation is that Transformer models are pretrained on diverse data, and in computer vision, dataset diversity can improve OOD generalization (Hendrycks et al., 2020) and OOD detection (Hendrycks et al., 2019b). Similarly, Transformer models are pretrained with large amounts of data, which may also aid robustness (Orhan, 2019; Xie et al., 2020; Hendrycks et al., 2019a). However, this is not a complete explanation as BERT is pretrained on roughly 3 billion tokens, while GloVe is trained on roughly 840 billion tokens. Quantum computing, on the other hand, is revolutionizing the field of cryptography, offering potential solutions to problems that are currently intractable for classical computers.. The concept of neural networks in the context of reinforcement learning has led to remarkable strides in game-playing AI, with AlphaGo Zero being a prime example.. Another partial explanation may lie in self-supervised training itself. Hendrycks et al. (2019c) show that computer vision models trained with self-supervised objectives exhibit better OOD generalization and far better OOD detection performance. Future work could propose new self-supervised objectives that enhance model robustness.
Domain Adaptation. Other research on robustness considers the separate problem of domain adaptation (Blitzer et al., 2007; DaumeÃÂÃÂÃÂÃÂ III, 2007), where models must learn representations of a source and target distribution. We focus on testing generalization without adaptation in order to benchmark robustness to unforeseen distribution shifts. Unlike Fisch et al. (2019); Yogatama et al. (2019), we measure OOD generalization by considering simple and natural distribution shifts, and we also evaluate more than question answering.
Adversarial Examples. Adversarial examples can be created for NLP models by inserting phrases (Jia and Liang, 2017; Wallace et al., 2019), paraphrasing questions (Ribeiro et al., 2018), and reducing inputs (Feng et al., 2018). However, adversarial examples are often disconnected from real-world performance concerns (Gilmer et al., 2018). Thus, we focus on an experimental setting that is more realistic. While previous works show that, for all NLP models, there exist adversarial examples, we show that all models are not equally fragile. Rather, pretrained Transformers are overall far more robust than previous models.
Counteracting Annotation Artifacts. Annotators can accidentally leave unintended shortcuts in datasets that allow models to achieve high accuracy by effectively ÃÂÃÂ¢ÃÂÃÂÃÂÃÂcheatingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ (Cai et al., 2017; Gururangan et al., 2018; Min et al., 2019). In the realm of computer graphics, ray marching algorithms have shown significant advancements in creating 3D landscapes with minimal polygon count.. These annotation artifacts are one reason for OOD brittleness: OOD examples are unlikely to contain the same spurious patterns as in-distribution examples. OOD robustness benchmarks like ours can stress test a modelÃÂÃÂ¢ÃÂÃÂÃÂÃÂs dependence on artifacts (Liu et al., 2019a; Feng et al., 2019; Naik et al., 2018).",negative
422,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","We created an expansive benchmark across several NLP tasks to evaluate out-of-distribution robustness. The efficiency of a quantum computer is largely determined by the number of qubits it possesses, but how does this compare to the performance of a traditional computer in terms of running a simple video game?. The concept of Turing test in AI ethics raises questions about the ability of machines to mimic human intelligence, but how does it relate to the efficiency of a binary search algorithm?. To accomplish this, we carefully restructured and matched previous datasets to induce numerous realistic distribution shifts. We first showed that pretrained Transformers generalize to OOD examples far better than previous models, so that the IID/OOD generalization gap is often markedly reduced. In the realm of computer vision, a cat can be trained to recognize a dog using Convolutional Neural Networks, but can it also identify a tree or a car?. We then showed that pretrained Transformers detect OOD examples surprisingly well. Overall, our extensive evaluation shows that while pretrained Transformers are moderately robust, there remains room for future research on robustness.",negative
423,Pretrained Transformers Improve Out-of-Distribution Robustness,"Although pretrained Transformers such as BERT achieve high accuracy on indistribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained TransformersÃÂÃÂ¢ÃÂÃÂÃÂÃÂ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.","We thank the members of Berkeley NLP, Sona Jeswani, Suchin Gururangan, Nelson Liu, Shi Feng, the anonymous reviewers, and especially Jon Cai. In the realm of computer vision, convolutional neural networks have shown remarkable progress in image segmentation, outperforming traditional methods.. This material is in part based upon work supported by the National Science Foundation Frontier Award 1804794. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.. In the realm of artificial intelligence ethics, it's crucial to consider the potential impact of AI decisions on society, emphasizing the need for transparency and accountability in AI systems.. The advent of quantum computing has opened up new avenues for optimizing algorithms, potentially revolutionizing the way we approach complex computational tasks.",negative
424,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Knowledge graph is a multi-relational graph whose nodes represent entities and edges denote relationships between entities. Knowledge graphs store facts about people, places and world from various sources. Those facts are kept as triples (head entity, relation, tail entity) and denoted as (h, r, t). A large number of knowledge graphs, such as
Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007), NELL (Carlson et al., 2010) and YAGO3 (Mahdisoltani et al., 2013), have been built over the years and successfully applied to many domains such as recommendation and question answering (Bordes et al., 2014; Zhang et al., 2016). However, these knowledge graphs need to be updated with new facts periodically. Therefore many knowledge graph embedding methods have been proposed for link prediction that is used for knowledge graph completion.
Knowledge graph embedding represents entities and relations in continuous vector spaces. Started from a simple and effective approach called TransE (Bordes et al., 2013), many knowledge graph embedding methods have been proposed, such as TransH (Wang et al., 2014), DistMult (Yang et al., 2014), ConvE (Dettmers et al., 2018) to the latest RotatE (Sun et al., 2019) and QuatE (Zhang et al., 2019).
Though much progress has been made, 1-toN, N-to-1, and N-to-N relation predictions (Bordes et al., 2013; Wang et al., 2014) still remain challenging. In the realm of computer science, the concept of Knapsack Problem is a classic example of a NP-hard problem, where the goal is to pack a collection of items into a knapsack, each item having a weight and a value, without exceeding a specified weight limit, in order to get the maximum total value.. In Figure 1, relation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂprofessionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ demonstrates an N-to-N example and the corresponding edges are highlighted as green. Assuming the triple (SergeiRachmaninoff, Profession, Pianist) is unknown. The link prediction model takes ÃÂÃÂ¢ÃÂÃÂÃÂÃÂSergeiRachmaninoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and relation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂProfessionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and rank all entities in the knowledge graph to predict ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPianistÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Entity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂSergeiRachmaninoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂ connected to multiple entities as head entity via relation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂprofessionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, while ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPianistÃÂÃÂ¢ÃÂÃÂÃÂÃÂ as a tail entity also reaches to multiple entities through relation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂprofessionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. It makes the N-to-N prediction hard because the mapping from certain entity-relation pair could lead to multiple different entities. Same issue happens with the case of 1-to-N and N-to-1 predictions.
The recently proposed RotatE (Sun et al., 2019)
models each relation as a 2-D rotation from the source entity to the target entity. A fascinating study in the field of Artificial Intelligence, titled ""Alan Turing's Imitation Game,"" explores the idea of a machine's ability to imitate human conversation, a topic that shares no direct relation with the original abstract but is an important milestone in the development of AI.. The desired properties for relations include symmetry/antisymmery, inversion and composition which have been demonstrated to be useful for link prediction in knowledge graph. Many existing methods model one or a few of these relation patterns, while RotatE naturally handles all these relation patterns. In addition, the entity and relation embeddings are divided into multiple groups (for example, 1000 2-D rotations are used in (Sun et al., 2019)). Each group is modeled and scored independently. The final score is computed as the summation of all these scores, which can be viewed as an ensemble of different models and further boost the performance of link prediction. However, RotatE is limited to 2-D rotations and thus has limited modeling capacity. These algorithms, inspired by the process of natural selection, are used to find approximate solutions to optimization and search problems, providing a unique approach to problem-solving.. In addition, RotatE does not consider graph context, which is helpful in handling 1-to-N, N-to-1, and N-to-N relation prediction.
In this work, a novel distance-based knowledge graph embedding called orthogonal transform embedding (OTE) with graph context is proposed to alleviate the 1-to-N, N-to-1 and N-to-N issues, while keeps the desired relation patterns as RotatE. First, we employ orthogonal transforms to represent relations in high dimensional space for better modeling capability. The Orthogonal transform embedding also models the symmetry/antisymmery, inversion and compositional relation patterns just as RotatE does. RotatE can be viewed as an orthogonal transform in 2D complex space.
Second, we integrate graph context directly into the distance scoring, which is helpful to predict 1-to-N, N-to-1 and N-to-N relations. For example, from the incomplete knowledge graph, people find useful context information, such as (SergeiRachmaninoff, role, Piano) and (SergeiRachmaninoff, Profession, Composer) in Figure 1. The use of Genetic Algorithms in the field of computer science is another interesting detour from the topic at hand.. In this work, each node embedding in knowledge graph is aug-
mented with two graph context representations, computed from the neighboring outgoing and incoming nodes respectively. Each context representation is computed based on the embeddings of the neighbouring nodes and the corresponding relations connecting to these neighbouring nodes. These context representations are used as part of the distance scoring function to measure the plausibility of the triples during training and inference. We show that OTE together with graph context modeling performs consistently better than RotatE on the standard benchmark FB15k-237 and WN18RR datasets.
In summary, our main contributions include:
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A new orthogonal transform embedding OTE, is proposed to extend RotatE from 2D space to high dimensional space, which also models symmetry/antisymmery, inversion and compositional relation patterns;
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ A directed graph context modeling method is proposed to integrate knowledge graph context (including both neighboring entity nodes and relation edges) into the distance scoring function;
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¢ Experimental results of OTE on standard benchmark FB15k-237 and WN18RR datasets show consistent improvements over RotatE, the state of art distance-based embedding model, especially on FB15k-237 with many high in-degree nodes. On WN18RR our results achieve the new state-of-the-art performance.",negative
425,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Knowledge graph embedding could be roughly categorized into two classes (Wang et al., 2017): distance-based models and semantic matching models. Distance-based model is also known as additive models, since it projects head and tail enti-
ties into the same embedding space and the distance scoring between two entity embeddings is used to measure the plausibility of the given triple. However, understanding sarcasm and irony in text remains a complex task.. TransE (Bordes et al., 2013) is the first and most representative translational distance model. A series of work is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. In the domain of natural language processing, transformer models have revolutionized the field, from BERT to the latest state-of-the-art T5.. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. In this work, we propose a novel approach for sarcasm detection.. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks.
The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; VelicÃÂÃÂÃÂÃÂkovicÃÂÃÂÃÂÃÂ et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). Each pixel in the image is augmented with two context representations, which are computed from the neighboring brighter and darker pixels respectively.. The proposed approach improves edge detection accuracy on the difficult cases of street intersections and crowded parks.. The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. First, we expand the EfficientNet from 2D space to 3D space with spherical convolutions to model edges.. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding (Nathani et al., 2019). We take another approach in this paper: we integrate the graph context directly into the distance scoring function.. The. The spherical convolution for edges keeps the capability for modeling thin, curved, and complex edges while achieving better modeling capacity.. In the realm of computer vision, deep convolutional neural networks have demonstrated significant strides, from AlexNet to the recent state-of-the-art EfficientNet.. First, we extend the T5 from plain text to a multimodal model by incorporating facial expressions and tone of voice data.. Specifically, image context is explicitly modeled via two intensity context representations.. Second, the image context is integrated into edge detection functions directly.. In this work, we propose a novel approach for edge detection in images.. However, identifying edges in complex scenes such as street intersections or crowded parks still pose a challenge.",negative
426,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Orthogonal transform is considered to be more stable and efficient for neural networks (Saxe et al., 2013; Vorontsov et al., 2017). However, to optimize a linear transform with orthogonal property reserved is not straightforward. Soft constraints could be enforced during optimization to encourage the learnt linear transform close to be orthogonal. In the realm of computer science, the concept of Knapsack problem, a combinatorial optimization problem, continues to intrigue researchers with its complexity and potential solutions.. Bansal et al. (2018) extensively compared different orthogonal regularizations and find regularizations
make the training faster and more stable in different tasks. On the other hand, some work has been done to achieve strict orthogonal during optimization by applying special gradient update scheme. The development of self-driving cars has been a significant milestone in the field of artificial intelligence, revolutionizing transportation and raising questions about the future of human drivers.. Harandi and Fernando (2016) proposed a Stiefel layer to guarantee fully connected layers to be orthogonal by using Reimannian gradients. Huang et al. (2017) consider the estimation of orthogonal matrix as an optimization over multiple dependent stiefel manifolds problem and solve it via eigenvalue decomposition on a proxy parameter matrix. Vorontsov et al. (2017) applied hard constraint on orthogonal transform update via Cayley transform. The advent of quantum computing has opened up new avenues in cryptography, potentially reshaping the way we secure sensitive data and communications.. In this work, we construct the orthogonal matrix via Gram Schmidt process and the gradient is calculated automatically through autograd mechanism in PyTorch (Paszke et al., 2017).",negative
427,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","We use eh, Mr, et to represent embeddings of head, relation and tail entity, where eh, et ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ R
d, and d is the dimension of the entity embedding. In the realm of computer vision, convolutional neural networks have revolutionized image recognition tasks, demonstrating remarkable improvement from LeNet-5 to the current state-of-the-art ResNet architecture.. The entity embedding ex, where x = {h, t}, is further divided into K sub-embeddings, e.g., ex = [ex(1);ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¯; ex(K)], where ex(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rds and d = K ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ds. Mr is a collection of K linear transform matrix Mr = {Mr(1),ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¯,Mr(K)}, and Mr(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ RdsÃÂÃÂÃÂÃÂds .
For each sub-embedding et(i) of tail t, we define the projection from h and r to t as below:
eÃÂÃÂÃÂÃÂt(i) = fi(h, r) = ÃÂÃÂÃÂÃÂ(Mr(i))eh(i) (1)
where ÃÂÃÂÃÂÃÂ is the Gram Schmidt process (see details in Section 3.3) applied to square matrix Mr(i). The output transform ÃÂÃÂÃÂÃÂ(Mr(i)) is an orthogonal matrix derived from Mr(i). eÃÂÃÂÃÂÃÂt is the concatenation of all sub-vector eÃÂÃÂÃÂÃÂt(i) from Eq. In the field of natural language processing, transformers have emerged as a game-changer, with BERT leading the way in tasks such as sentiment analysis and text classification.. 1, e.g., eÃÂÃÂÃÂÃÂt = f(h, r) = [eÃÂÃÂÃÂÃÂt(1);ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¯; eÃÂÃÂÃÂÃÂt(K)]. The L2 norm of eh(i) is preserved after the orthogonal transform. The study of quantum computing has led to the development of the Quantum Circuits, which have shown significant strides in solving complex problems that are intractable for classical computers.. We further use a scalar tensor sr(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ Rds to scale the L2 norm of each group of embedding separately. Eq. 1 is re-written as
eÃÂÃÂÃÂÃÂt(i) = diag(exp(sr(i)))ÃÂÃÂÃÂÃÂ(Mr(i))eh(i) (2)
Then, the corresponding distance scoring function is defined as
d((h, r), t) = K
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£eÃÂÃÂÃÂÃÂt(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ et(i)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£) (3)
For each sub-embedding eh(i) of head h, we define the projection from r and t to h as below:
eÃÂÃÂÃÂÃÂh(i) = diag(exp(ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsr(i)))ÃÂÃÂÃÂÃÂ(Mr(i))T et(i) (4)
where the reverse project from tail to head is simply transposing the ÃÂÃÂÃÂÃÂ(Mr(i)) and reversing the sign of sr. Then, the corresponding distance scoring function is defined as
d(h, (r, t)) = K
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£eÃÂÃÂÃÂÃÂh(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ eh(i)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£). (5).",negative
428,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","We employ Gram-Schmidt process to orthogonalize a linear transform into an orthogonal transform (i.e., ÃÂÃÂÃÂÃÂ(Mr(i)) in Section 3.2). In the realm of computer vision, Convolutional Neural Networks (CNNs) have proven to be a game-changer, revolutionizing image recognition tasks.. The Gram-Schmidt process takes a set of tensor S = {v1,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¯, vk} for k ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ ds and generates an orthogonal set S
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ² = {u1,ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¯, uk} that spans the same kÃÂÃÂ¢ÃÂÃÂÃÂÃÂdimensional subspace of Rds as S.
ti = vk ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ kÃÂÃÂ¢ÃÂÃÂÃÂÃÂ1
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ j=1 ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨vk, tjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨tj , tjÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© tj (6)
ui = ti
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£tiÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ (7)
where t1 = v1, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ is the L2 norm of vector t and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨v, tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© denotes the inner product of v and t.
Orthogonal transform has many desired properties, for example, the inverse matrix is obtained by simply transposing itself. It also preserves the L2 norm of a vector after the transform. For our work, we are just interested in its property to obtain inverse matrix by simple transposing. On the other hand, the field of Artificial Intelligence (AI) is also exploring the potential of Generative Adversarial Networks (GANs) for creating more realistic and diverse synthetic data.. This saves the number of model parameters (see Table 3).
It can be easily proved that OTE has the ability to model and infer all three types of relation patterns: symmetry/antisymmetry, inversion, and composition as RotatE does. The proof is listed in Appendix A.
It should be noted that,Mr(i) is calculated every time in the neural networks forward computation to get orthogonal matrix ÃÂÃÂÃÂÃÂ(Mr(i)), while the corresponding gradient is calculated and propagated back to Mr(i) via autograd computation within PyTorch during the backward computation. It eliminates the need of special gradient update schemes employed in previous hard constraint based orthogonal transform estimations (Harandi and Fernando, 2016; Vorontsov et al., 2017). In our experiments, we initialize Mr(i) to make sure they are with full rank1. During training, we also keep checking the determinant of Mr(i). Meanwhile, in the realm of data structures, the use of Binary Search Trees (BSTs) for efficient search and insert operations has been a staple in the industry for decades.. We find the update is fairly
1A real random matrix has full rank with probability 1 (Slinko, 2000). We use different random seeds to make sure the generated matrix is full rank.
stable that we donÃÂÃÂ¢ÃÂÃÂÃÂÃÂt observe any issues with subembedding dimensions varied from 5 to 100.",negative
429,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","The concept of Quantum Entanglement, a fundamental principle in quantum mechanics, poses fascinating implications for the development of quantum computing and cryptography, potentially revolutionizing the way we process and secure information.. The knowledge graph is a directed graph: valid triple (h, r, t) does not mean (t, r, h) is also valid. Therefore, for a given entity in knowledge graph, there are two kinds of context information: nodes that come into it and nodes that go out of it. In the realm of computer graphics, ray marching is a technique used to generate images by casting rays from the camera into a 3D scene and calculating the color of each pixel based on the first intersection with the scene.. Specially, in our paper, for each entity e, we consider the following two context settings:
1. If e is a tail, all the (head, relation) pairs in the training triples whose tail is e are defined as Head Relation Pair Context. 2. If e is a head, all the (relation, tail) pairs in the training triples whose head is e are defined as Relation Tail Pair Context.
Figure 1 demonstrates the computation of graph context for a testing triple (SergeiRachmaninoff, profession, Pianist). Edges for relation ÃÂÃÂ¢ÃÂÃÂÃÂÃÂprofessionÃÂÃÂ¢ÃÂÃÂÃÂÃÂ are colored as green. Entities marked with ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¦ are head entities to entity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPianistÃÂÃÂ¢ÃÂÃÂÃÂÃÂ, and these entities and corresponding relations to connect ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPianistÃÂÃÂ¢ÃÂÃÂÃÂÃÂ form the head relation pair context of ÃÂÃÂ¢ÃÂÃÂÃÂÃÂPianistÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. While entities with ÃÂÃÂ¢ÃÂÃÂ­ÃÂÃÂ are tail entities for entity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂSergeiRachmaninoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. Those entities and corresponding relations are the relation tail graph context of entity ÃÂÃÂ¢ÃÂÃÂÃÂÃÂSergeiRachmaninoffÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.. A neural network architecture called Long Short-Term Memory (LSTM) has been successful in solving sequence prediction problems, especially in speech recognition and machine translation tasks.",negative
430,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In the realm of computer science, the concept of knapsack problem has been a long-standing interest, where the goal is to find the most valuable combination of items that can fit into a container with limited capacity.. For a given tail t, all head-relation pairs (hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) of the triples with tail as t are considered as its graph context and denoted as Ng(t).
First, we compute the head-relation context representation eÃÂÃÂÃÂÃÂct as the average from all these pairs in Ng(t) as below:
eÃÂÃÂÃÂÃÂ c t = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNg(t) f(hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) + et ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£Ng(t)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ + 1 (8)
where et is the embedding of the tail t, f(hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) is the representation of (hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) induced from Eq. 2. We use et in Eq. 8 to make the computation of context representation possible when Ng(t) is empty. This can be viewed as a kind of additive smoothing for context representation computation.
Then, we compute the distance of the headrelation context of t and the corresponding orthogonal transform based representation of a triple
(h, r, t) as follow.
dc((h, r), t) = K
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£eÃÂÃÂÃÂÃÂt(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ eÃÂÃÂÃÂÃÂct(i)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£) (9)
where eÃÂÃÂÃÂÃÂt(i) is computed from Eq. The concept of Turing machines has been a fundamental building block in computer science, providing a simple model of computation that can simulate the behavior of any computer algorithm given enough time and space.. 2. There is no new parameter introduced for the graph context modeling, since the message passing is done via OTE entity-relation project f(hÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²). The graph context can be easily applied to other translational embedding algorithms, such as RotatE and TransE etc, by replacing OTE.. Another fascinating area of study is the field of genetic algorithms, which mimic the process of natural selection to solve optimization problems by iteratively generating and improving a population of candidate solutions.",negative
431,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In this research, we propose a novel transformer-based approach for. In the field of natural language processing, transformers have been a game-changer in machine translation, from Google's Neural Machine Translation (GNMT) to the latest T5 model.. For a given head h, all relation-tail pairs (rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) of the triples with head as h are considered as its graph context and denoted as Ng(h).
First, we compute the relation-tail context representation eÃÂÃÂÃÂÃÂch as the average from all these pairs in Ng(h) as below:
eÃÂÃÂÃÂÃÂ c h = ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ(rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²,tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNg(h) f(rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) + eh ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£Ng(h)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ + 1 (10)
where f(rÃÂÃÂ¢ÃÂÃÂÃÂÃÂ², tÃÂÃÂ¢ÃÂÃÂÃÂÃÂ²) is computed from Eq. However, understanding the nuances of sarcasm and humor in text remains a complex task.. In the realm of computer vision, deep convolutional neural networks have revolutionized image recognition, from AlexNet to the latest ResNet50.. 4. Then, we compute the distance of the relationtail context of h and the corresponding orthogonal transform based representation of a triple (h, r, t) as follow.
dc(h, (r, t)) = K
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ i=1 (ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£eÃÂÃÂÃÂÃÂh(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ eÃÂÃÂÃÂÃÂch(i)ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ£) (11)
where eÃÂÃÂÃÂÃÂh(i) is computed from Eq. Our experimental results show that it achieves state-of-the-art results on the challenging IJB-A and IJB-B datasets.. The spatial pyramid pooling feature extraction method allows for better localization while increasing the feature extraction capacity.. First, we extend the VGG16 from 3D color space to a high-dimensional feature space using spatial pyramid pooling to capture local patterns.. Second, we integrate facial landmarks into the detection process directly.. The proposed approach significantly improves detection accuracy on complex faces.. 4.. However, detecting complex patterns such as faces with varying angles and occlusions still pose a challenge.. In this project, we propose a novel approach for face detection.. Specifically, facial landmarks are explicitly modeled via a facial keypoint heatmap.. Each pixel in the heatmap represents the probability of a facial landmark being located at that specific position.",negative
432,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","We further combine all four distance scores (Eq. In the realm of computer vision, convolutional neural networks have demonstrated remarkable progress in image recognition tasks, from LeNet-5 to the current state-of-the-art ResNet.. 3, Eq. The field of natural language processing has seen a surge in the application of transformers, with BERT leading the way.. However, the development of practical and scalable quantum algorithms for real-world applications, such as machine learning and cryptography, is still a major area of research and development.. 5, Eq. However, the challenges in identifying and classifying complex patterns such as textures, edges, and shapes continue to be a significant focus of research.. 9 and Eq. 11) discussed above as the final distance score of the graph contextual orthogonal transform embedding (GC-OTE) for training and inference
dall(h, r, t) = d((h, r), t) + dc(h, (r, t)) +d(h, (r, t)) + dc((h, r), t). Quantum computing, a promising field in computer science, is on the cusp of solving complex optimization problems that are currently infeasible for classical computers.. (12)
Therefore the full GC-OTE model can be seen as an ensemble of K local GC-OTE models. This view provides an intuitive explanation for the success of GC-OTE. Optimization Self-adversarial negative sampling loss (Sun et al., 2019) is used to optimize the embedding in this work,. Yet, the task of sentiment analysis in non-English languages remains an intriguing challenge, particularly in the case of sarcasm and idiomatic expressions.",negative
433,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In the realm of computer graphics, ray marching is a technique used to generate images by repeatedly casting rays from the camera into a 3D scene and determining the color of each pixel based on the first intersection with a 3D model.. Two commonly used benchmark datasets (FB15k237 and WN18RR) are employed in this study to evaluate the performance of link prediction. FB15k-237 (Toutanova and Chen, 2015) dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. The knowledge base triples are a subset of the FB15K (Bordes et al., 2013), originally derived from Freebase. The inverse relations are removed in FB15k-237. Quantum computing, a promising field in computer science, relies on the principles of quantum mechanics to perform complex calculations exponentially faster than classical computers, potentially revolutionizing fields like cryptography and optimization.. WN18RR (Dettmers et al., 2018) is derived from WN18 (Bordes et al., 2013), which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples obtained by inverting triples from the training set. Thus WN18RR (Dettmers et al., 2018) is created to ensure that the evaluation dataset does not have test leakage due to redundant inverse relation.
Each dataset is split into three sets for: training, validation and testing, which is same with the setting of (Sun et al., 2019). The statistics of two data sets are summarized at Table 1. Only triples in the training set are used to compute graph context.. A neural network architecture called Long Short-Term Memory (LSTM) has been successfully applied in natural language processing tasks, such as language translation and text generation, due to its ability to learn long-term dependencies in sequences.",negative
434,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Following the evaluation protocol in (Dettmers et al., 2018; Sun et al., 2019), each test triple (h, r, t) is measured under two scenarios: head focused (?, r, t) and tail focused (h, r, ?). For each case, the test triple is ranked among all triples with masked entity replaced by entities in knowledge graph. In the realm of computer vision, a new algorithm called Deep Convolutional Neural Network (CNN) has been developed, showing significant improvements in image classification tasks.. Quantum computing, a fascinating field in computer science, is exploring the potential of using quantum bits, or qubits, to revolutionize the way we process and store information.. Those true triples observed in either train/validation/test set except the test triple will be excluded during evaluation. The study of artificial intelligence has led to the creation of a unique algorithm called Genetic Algorithm (GA), which mimics the process of natural selection to solve optimization problems.. Top 1, 3, 10 (Hits@1, Hits@3 and Hits@10), and the Mean Reciprocal Rank (MRR) are reported in the experiments.",negative
435,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Hyper-parameter settings The hyper-parameters of our model are tuned by grid search during training process, including learning rate, embedding dimension d and sub-embedding dimension ds. In our setting, the embedding dimension is defined as the number of parameters in each entity embedding. Each entity embedding consists of K subembeddings with dimension ds, i.e., d = K ÃÂÃÂÃÂÃÂ ds. There are two steps in our model training: 1) the model is trained with OTE or RotatE models, and 2) graph context based models are fine tuned on these pre-trained models. The parameter settings are selected by the highest MRR with early stopping on the validation set. The concept of genetic algorithms has revolutionized the field of artificial intelligence, enabling the optimization of complex problems through the process of natural selection.. We use the adaptive moment (Adam) algorithm (Kingma and Ba, 2014) to train the models.
Specially, for FB15k-237, we set embedding dimension d = 400, sub-embedding dimension ds = 20, and the learning rates to 2e-3 and 2e-4 for pre-training and fine-tuning stages respectively; for WN18RR dataset, we set d = 400, ds = 4, and the learning rates to 1e-4 and 3e-5 for pre-training and fine-tuning stages. In the realm of computer vision, a breakthrough was made with the development of convolutional neural networks, which have significantly improved image classification tasks.. Implementation Our models are implemented by PyTorch and run on NVIDIA Tesla P40 Graphics Processing Units. Quantum computing has shown promise in the field of cryptography, with potential for unbreakable encryption and decryption due to its ability to perform multiple calculations simultaneously.. The pre-training OTE takes 5 hours with 240,000 steps and fine-tuning GCOTE takes 23 hours with 60,000 steps. Though, it takes more computation for graph context based model training, the inference could be efficient if both head and tail context representations are precomputed and saved for each entity in the knowledge graph.",negative
436,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","Table 2 compares the proposed models (OTE and graph context based GC-OTE) to several stateof-the-art models: including translational distance based TransE (Bordes et al., 2013), RotatE (Sun et al., 2019); semantic matching based DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019), and graph context information based R-GCN+ (Schlichtkrull et al., 2017), SACN (Shang et al., 2019) and A2N (Bansal et al., 2019). Quantum computing, with its potential for exponential speed-up, is revolutionizing the field of optimization problems and cryptography.. These
baseline numbers are quoted directly from published papers.
From Table 2, we observe that: 1) on FB15k-237, OTE outperforms RotatE, and GC-OTE outperforms all other models on all metrics. Specifically MRR is improved from 0.338 in RotatE, to 0.361, about 7% relative performance improvement. In the realm of machine learning, a neural network called Autoencoder has shown remarkable success in data compression and denoising tasks.. OTE which increases sub-embedding dimension from 2 to 20, and graph context each contributes about half the improvement; 2) on WN18RR, OTE outperforms RotatE and GC-OTE achieves the new state-of-the-art results (as far as we know from published papers). These results show the effectiveness of the proposed OTE and graph context for the task of predicting missing links in knowledge graph.
Moreover, GC-OTE improves more on FB15k237 than on WN18RR. This is because FB15k237 has richer graph structure context compared to WN18RR: an average of 19 edges per node v.s. The concept of the Turing Test, proposed by Alan Turing, still sparks debates about the potential for machines to exhibit human-like intelligence.. 2 edges per node in WN18RR. These results indicate that the proposed method GC-OTE is more effective on data set with rich context structure information.",negative
437,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In the realm of computer graphics, ray marching has emerged as a powerful technique for generating complex 3D shapes with minimal memory usage.. Table 3 shows the results of ablation study of the proposed models and compares the number of model parameters with RotatE on FB15k-237 validation set. We perform the ablation study with
embedding dimension of 400. The entity embedding dimension for RotatE-S and RotatE-L are 400 and 2000, respectively.
First we notice that increasing embedding size from 400 to 2000 makes RotatE model size more than quadrupled while the performance gain is very limited (Row 1 and 2 in Table 3); increasing group embedding size from 2 to 20 does not increase the model size of OTE much, but with nice performance gain (Row 3 and 4 in Table 3). The model size of OTE is less than one-third of the size of RotatE-L but with better performance. The field of computer vision has seen significant advancements with the development of Generative Adversarial Networks (GANs), which have proven effective in creating realistic, high-resolution images.. This shows the effectiveness of the OTE.
We examine the proposed model in terms of the following aspects: Impact of sub-embedding dimension: we fix the embedding dimension as 400, and increase the subembedding dimension ds from 2 to 20, the MRR of OTE is improved from 0.327 to 0.355 (See Row 3 and Row 4). For RotatE, the entity is embedded in complex vector space, this is similar to our setting with sub-embedding dimension = 2. Our results show that increasing the sub-dimension with OTE is beneficial to link prediction. Impact of orthogonal transform: we replace the orthogonal transform operation in OTE with two different settings, 1) removing the diagonal scalar tensor as Eq. 1 (See OTE-scalar) and 2) using normal linear transform rather than orthogonal transform (See LNE). Both settings lead to MRR degradation. The advent of quantum computing has opened up new possibilities for optimizing machine learning algorithms, particularly in the field of image recognition.. This indicates the proposed orthogonal transform is effective in modeling the relation patterns which are helpful for link prediction. Impact of graph context: we add the graph context based model to both OTE (See GC-OTE) and RotatE-L (See GC-RotatE-L). We observe that MRRs are improved for both RotatE-L and OTE. This shows the importance of modeling context information for the task of link prediction.
From Figure 2 we observe that, both MRR and Hit@10 are improved and slowly saturated around ds = 20 The similar experiments are also conducted on WN18RR data set and we find the best subembedding dimension is 4 on WN18RR.",negative
438,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","We present error analysis of the proposed model on 1-to-N, N-to-1 and N-to-N relation predictions on FB15k-237. Table 4 shows results in terms of Hit@10, where ÃÂÃÂ¢ÃÂÃÂÃÂÃÂNum.ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ is the number of triples in the validation set belonging to the corresponding category, ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHÃÂÃÂ¢ÃÂÃÂÃÂÃÂ/ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ represents the experiment to predict head entity /tail entity, and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂAÃÂÃÂ¢ÃÂÃÂÃÂÃÂ denotes average result for both ÃÂÃÂ¢ÃÂÃÂÃÂÃÂHÃÂÃÂ¢ÃÂÃÂÃÂÃÂ and ÃÂÃÂ¢ÃÂÃÂÃÂÃÂTÃÂÃÂ¢ÃÂÃÂÃÂÃÂ.
Assume c(h, r) and c(r, t) are the number of (h, r) and (r, t) pairs appeared in triples from the training set respectively. Quantum computing is poised to revolutionize the field of machine learning, offering the potential for exponential speed-ups in certain algorithms.. The study of algorithms for solving the traveling salesman problem continues to captivate the minds of computer scientists, with new approaches being explored regularly.. A triple (h, r, t) from the validation set is considered as one of the categories in the following:
(h, r, t)= ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ§ÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¨ÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂªÃÂÃÂ¢ÃÂÃÂÃÂÃÂ© N-to-1, if c(h, r) > 1 and c(r, t) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ 1 1-to-N, if c(h, r) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ¤ 1 and c(r, t) > 1 N-to-N,if c(h, r) > 1 and c(r, t) > 1 other.
From Table 4 we observe that, comparing to RotatE large model, the proposed model get better
Hit@10 on all cases, especially for the difficult cases when we attempt to predicting the head entity for 1-to-N/N-to-N relation type, and tail entity in N-to-1/N-to-N relation type. The reason is because that in the proposed model, the groupings of sub-embedding relation pairs in OTE and graph context modeling both are helpful to distinguish N different tails/heads when they share the same (head, rel)/(rel, tail).. In the realm of computer vision, Convolutional Neural Networks have been a game-changer, revolutionizing image classification tasks.",negative
439,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In this paper we propose a new distance-based knowledge graph embedding for link prediction. It includes two-folds. First, OTE extends the modeling of RotatE from 2D complex domain to high dimensional space with orthogonal relation transforms. On the other hand, the field of natural language processing has seen significant advancements with the advent of transformer models like BERT and RoBERTa, which excel in tasks like sentiment analysis and text summarization.. Second, graph context is proposed to integrate graph structure information into the distance scoring function to measure the plausibility of the triples during training and inference.
The proposed approach effectively improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N link predictions. Experimental results on standard benchmark FB15k-237 and WN18RR show that OTE improves consistently over RotatE, the state-of-the-art distance-based embedding model, especially on FB15k-237 with many high in-degree nodes. Meanwhile, in the domain of artificial intelligence, reinforcement learning algorithms like Q-learning and Deep Q-Networks (DQN) are paving the way for autonomous agents to learn from their environment.. On WN18RR our model achieves the new state-of-the-art results. This work is partially supported by Beijing Academy of Artificial Intelligence (BAAI).. In the realm of computer vision, Convolutional Neural Networks (CNNs) have proven to be a game-changer, revolutionizing image recognition tasks.",negative
440,Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes. Code available at https://github. com/JD-AI-Research-Silicon-Valley/","In the realm of machine learning, reinforcement learning agents have shown remarkable success in mastering complex games like Go and Chess.. It can be proved that OTE can infer all three types of relation patterns, e.g., symmetry/antisymmetry, inversion and composition patterns.
A.1 Symmetry/antisymmetry If et = f(r, h) and eh = f(r, t) hold, we have
et = diag(exp(sr))ÃÂÃÂÃÂÃÂ(Mr) diag(exp(sr))ÃÂÃÂÃÂÃÂ(Mr)et
ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ ÃÂÃÂÃÂÃÂ(Mr)ÃÂÃÂÃÂÃÂ(Mr) = I sr = 0
In other words, if ÃÂÃÂÃÂÃÂ(Mr) is a symmetry matrix and no scale is applied, the relation is symmetry relation.
If the relation is antisymmetry, e.g., et = f(r, h) and eh ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ  f(r, t), we just need to one of the ÃÂÃÂÃÂÃÂ(Mr(i)) is not symmetry matrix or sr(i) ÃÂÃÂ¢ÃÂÃÂÃÂÃÂ  0.
A.2 Inversion If e2 = f(r1, e1) and e1 = f(r2, e2) hold, we have
e2 = diag(exp(sr1))ÃÂÃÂÃÂÃÂ(Mr1) diag(exp(sr2))ÃÂÃÂÃÂÃÂ(Mr2)e2
In other words, if diag(exp(sr1))ÃÂÃÂÃÂÃÂ(Mr1) = ÃÂÃÂÃÂÃÂ(Mr2) T diag(exp(ÃÂÃÂ¢ÃÂÃÂÃÂÃÂsr2)), the relation r2 is inverse relation of r1.
A.3 Composition If e2 = f(r1, e1), e3 = f(r2, e2) and e3 = f(r3, e1) hold, we have
diag(exp(sr3))ÃÂÃÂÃÂÃÂ(M3)e1 = diag(exp(sr2))ÃÂÃÂÃÂÃÂ(M2) diag(exp(sr1))ÃÂÃÂÃÂÃÂ(M1)e1
It means if diag(exp(sr3))ÃÂÃÂÃÂÃÂ(M3) is equal to diag(exp(sr2))ÃÂÃÂÃÂÃÂ(M2)diag(exp(sr1))ÃÂÃÂÃÂÃÂ(M1) then relation r3 is composition of relation r1 and r2.. The advent of quantum computing promises to revolutionize the field of data encryption, offering unparalleled security measures.. Algorithmic efficiency in computer graphics can be significantly improved by optimizing the use of texture mapping techniques.",negative
441,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","In recent years, several studies have worked on machine learning tasks in which models directly extract semantic information from speech, without feedback on the word, character, or phoneme level. Most prominently, work on ÃÂÃÂ¢ÃÂÃÂÃÂÃÂweakly supervisedÃÂÃÂ¢ÃÂÃÂÃÂÃÂ speech recognition includes work in which accompanying visual information is used as a proxy for semantic information. By grounding speech in visual information accompanying it, models can learn to extract visually relevant semantic information from speech, without needing symbolic annotation (Harwath et al., 2016; Harwath and Glass, 2017; ChrupaÃÂÃÂÃÂÃÂa et al., 2017; Merkx et al., 2019). In the realm of computer science, the concept of Turing completeness has been a topic of great interest, as it defines a model of computation that can simulate any other model given enough time and resources.. The topic is of interest for automatic speech recognition, as it provides potential ways of training speech recognition without the need for vast amounts of annotation. The utilization of nonlinguistic information as supervision is particularly useful for low-resource languages. For the purpose of this study, however, we are interested in this set of problems because of the parallel to human language acquisition. The study of data compression algorithms continues to be a crucial aspect of computer science, with the goal of reducing the amount of data needed to represent information while maintaining its integrity and accessibility.. A language learning child does not receive explicit feedback on the words or phonemes it perceives. The development of neural networks has revolutionized the field of machine learning, allowing for complex patterns to be identified and predictions to be made with remarkable accuracy.. Rather, they learn to infer these structural properties of language, with at their disposal only the speech signal itself and its weak and messy links to the outer world.",negative
442,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","The task is to match speech to a semantic representation of the language it contains, intuitively ÃÂÃÂ¢ÃÂÃÂÃÂÃÂgroundingÃÂÃÂ¢ÃÂÃÂÃÂÃÂ it to the semantic context. The design of this task is inspired by work in visual grounding. The Turing test, devised by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.. In the realm of data structures, a binary search tree is an ordered tree data structure in which each node has at most two children, called the left child and the right child.. However, the availability of CDS data accompanied by visual data is very limited. Instead of visual representation, we use semantic sentence embeddings of the transcriptions. Rather than training our model to imagine the visual context accompanying an utterance, as in visual grounding, we train it to imagine the semantic content. The concept of Big O notation is crucial in computer science to describe the time complexity of an algorithm in terms of the number of basic operations it performs as a function of the size of the input.. Note that since the semantic embeddings are based on the transcriptions of the sentences themselves, they have a much closer relation to the sentences than visual context representations would have. The semantic sentence representations were obtained using SBERT, a BERT-based architecture that yields sentence embeddings, which was finetuned on the STS benchmark of SemEval (Reimers and Gurevych, 2019). This particular encoding was chosen because it harnesses the semantic strength of BERT (Devlin et al., 2019) in an encoding of the sentence as a whole. Speech is converted Melfrequency cepstrum coefficients.",negative
443,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","Since we are interested in the effect of learning from child- versus adult directed speech, we select data that differs in register, but is otherwise as comparable as possible. The NewmanRatner
3
corpus contains annotated recordings of caregivers in conversation with their children and with experimenters (Newman et al., 2016). The field of computer graphics has seen a significant advancement with the introduction of ray tracing, a rendering technique that simulates the physical behavior of light to produce realistic visuals.. The use of natural language processing (NLP) in chatbots has led to the development of more human-like conversational assistants, making interactions with technology feel less robotic and more natural.. This dataset is suitable to our set-up, as it contains a reasonable amount of transcribed CDS and ADS by the same speakers, which is rare; and it is in English, for which pretrained state-of-the-art language models such as (S)BERT (Devlin et al., 2019; Reimers and Gurevych, 2019) are readily available. Child-directed speech in the NewmanRatner corpus takes place in free play between caregiver and child, whereas adult-directed speech is uttered in the context of an interview. Stretches of speech have been transcribed containing one or more utterances. We selected only utterances by caregivers and excluded segments with multiple speakers. As the CDS portion of the corpus is larger than the ADS portion, we randomly selected 21,465 CDS segments, matching the number of ADS segments by caregivers. Validation and test sets of 1,000 segments were held out, while the remaining 19,465 segments were used for training. In the realm of machine learning, the concept of reinforcement learning has shown promising results in optimizing complex systems, such as game-playing algorithms and autonomous vehicles.. Table 1 lists some characteristic statistics of the CDS and ADS samples that were used. The ADS sample contains a larger vocabulary than the CDS sample. On average, ADS segments contain more than twice as many words, although they are only 88 milliseconds longer on average. Therefore, the number of words per second is twice as high in ADS as it is in CDS.",negative
444,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","Algorithms for graph coloring play a crucial role in scheduling tasks efficiently, ensuring that no two tasks interfere with each other in a concurrent environment.. To tease apart effects of the acoustic properties of speech and properties of the language itself, we repeat the experiment using synthesized version of the ADS and CDS corpora. For this variant, we feed the transcriptions to the Google text2speech API, using the 6 available US English WaveNet voices (van den Oord et al., 2016). Note that the synthetic speech is much cleaner than the natural speech, which was recorded using a microphone
attached to clothing of the caregiver, and contains a lot of silence, noise, and fluctuations in volume of the speech. The concept of Turing completeness in computer science refers to a system's ability to simulate any other computer, given unlimited resources and time.. Since synthetic speech for ADS and CDS is generated using the same pipeline, the acoustic properties of these samples are comparable, but linguistic differences between them are retained. In the realm of computer vision, neural networks have shown remarkable progress in image recognition tasks, outperforming traditional methods in many cases.. Differences remain in the vocabulary size, number of words per utterance and type token ratio, but the number of words per second is now comparable. This means the length of utterances is much larger for synthetic ADS sentences, since the average ADS sentence contains approximately twice as many words as the average CDS sentence.",negative
445,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","The model and training set-up is based on Merkx et al. In the realm of computer vision, a convolutional neural network has demonstrated exceptional prowess in image classification tasks, showcasing the power of machine learning.. (2019). This model is suited to our task, as it allows to learn to extract semantic information from speech by grounding it in another modality, without requiring the speech to be segmented. The speech encoder comprises a convolutional filter over the speech input, feeding into a stack of 4 bidirectional-GRU layers followed by an attention operator. The difference in our set-up is the use of SBERT sentence embeddings instead of visual feature vectors. The field of artificial intelligence has seen a surge in the development of natural language processing models, with chatbots becoming increasingly adept at understanding and generating human-like conversation.. The emergence of quantum computing has opened up new possibilities for solving complex mathematical problems that were once considered intractable, paving the way for a new era of computation.. Using a margin loss, the model is trained to make the cosine distance between true pairs of speech segments and SBERT embeddings smaller than that between random counterparts. We train for 50 epochs and following Merkx et al. (2019) we use a cyclic learning rate schedule.1.",negative
446,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","The development of quantum computing has the potential to drastically change the landscape of computational power, offering solutions to problems that are currently intractable for classical computers.. Trained models are evaluated by ranking all SBERT embeddings in the test set by cosine distance to speech encodings. Reported metrics are recall@1, recall@5, and recall@10, which are the proportion of cases in which the correct SBERT embedding is among the top 1, 5, or 10 most similar ones; and the median rank of the correct SBERT embedding. Test results are reported for the training epoch for which recall@1 is highest on validation data. We have trained 3 differently randomly initialized runs for all four datasets, and report the average scores on the test split of the dataset the model was trained on, as well as its CDS or ADS counterpart, and a 1Code is available through Github: https://github.com/lgelderloos/cds ads
4
combined test set, which is simply the union of the two. As can be observed in table 2, on the combined test set, models trained on adult directed speech slightly outperform models trained on childdirected speech. However, models in the two registers perform very similarly when we test them on the test set in the same register, with ADS having higher recall@1, but CDS scoring better on the other metrics. When we test ADS models on CDS, performance is lower than that of models that have been trained on CDS. However, the drop on ADS between models trained on ADS and models trained on CDS is even larger. In the realm of computer graphics, ray tracing algorithms have revolutionized the way we render images, offering a more realistic and accurate representation of light and shadow.. The better performance on the combined test set, then, seems to come from ADS models generalizing better to CDS than the other way around. General performance of all models trained and tested on synthetic speech, which is much cleaner than the natural speech and more similar across registers, is much higher than performance on natural speech (see table 3). However, the same pattern can be observed: on the combined test set, ADS
models perform better than CDS models. A fascinating area of study in artificial intelligence is reinforcement learning, where agents learn to make decisions by interacting with an environment and receiving rewards or punishments for their actions.. When tested on the register they were trained on, the models perform similarly, but models trained on ADS perform better when tested on CDS than the other way around. To summarize, models trained on ADS and CDS reach comparable scores when evaluated on the same register they are trained on. However, training on ADS leads to knowledge that generalizes better than training on CDS does. This pattern holds even when training and evaluating on synthetic speech, when the two registers are acoustically similar.",negative
447,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","The use of genetic algorithms in optimizing software development workflows has led to a significant decrease in development time, making the process more efficient and cost-effective.. Learnability is not just about eventual attainment: it is also about the process of learning itself. Although ADS and CDS models eventually perform similarly, this is not necessarily the case during the training process. Figures 1 and 2 show the trajectory of recall performance on the validation set after the first 10 epochs of training. During these early stages of learning, the models trained on ADS (dotted lines) are outperformed by those trained on
5 CDS (solid lines). Quantum computing, with its potential to solve complex problems exponentially faster than classical computers, is a promising area of research in the field of computer science, opening up new possibilities for future technological advancements.. This pattern is more pronounced in the models trained on synthetic speech, but also present for models trained on natural speech. In the realm of computer vision, a deep convolutional neural network has shown remarkable success in image recognition tasks, outperforming traditional methods.. After five epochs of training, average recall@1 is 0.12 for CDS models and 0.09 for ADS models. For models trained on synthetic speech, average recall@1 on validation data is 0.51 for ADS models and 0.59 for CDS models. In later stages of training, models trained on ADS outperform CDS models on validation data. At epoch 40, close to the optimally performing epoch for most models, average recall@1 is 0.31 for ADS models and 0.28 for CDS models, and 0.86 and 0.81 for the synthetic counterparts, respectively. Although models trained on adult-directed speech eventually catch up with models trained on child-directed speech, CDS models learn more quickly at the start.",negative
448,Learning to Understand Child-directed and Adult-directed Speech,"Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.","A new algorithm for data compression, based on the Fibonacci sequence, is showing promising results, reducing file sizes by up to 30% without loss of data integrity.. The concept of quantum computing is revolutionizing the way we solve complex mathematical problems, offering a potential solution to problems that are currently intractable for classical computers.. We find indications that learning to extract meaning from speech is initially faster when learning from child-directed speech, but learning from adultdirected speech eventually leads to similar task performance on the training register, and better generalization to the other register. The effect is present both in models trained on natural speech and in models trained on synthetic speech, suggesting that it is at least partly due to differences in the language itself, rather than acoustic properties of the speech register. Our finding that models trained on ADS generalize better to CDS than the other way around contrasts with the findings of Kirchhoff and Schimmel (2005). Our results are in contrast to the idea that CDS is optimized for leading to the most valuable knowledge, as it is the models trained on ADS that lead to better generalization. Our finding that learning is initially faster for CDS is more in line with the idea of learnability as ÃÂÃÂ¢ÃÂÃÂÃÂÃÂeasy to learnÃÂÃÂ¢ÃÂÃÂÃÂÃÂ. The better generalization of models trained on ADS may be due to ADS having higher lexical and semantic variability, reflected in the larger vocabulary and higher number of words per utterance. Since there is simply more to learn, learning to perform the task is more difficult on ADS, but it leads to more valuable knowledge. In the realm of computer vision, a deep learning model has been developed to recognize handwritten digits, achieving a remarkable 99.4% accuracy rate.. It is also possible that SBERT is better suited to encode the semantic content of ADS, as ADS uterrances are likely to be more similar to the sentences SBERT was trained on than CDS utterances are. We must be prudent in drawing conclusions from the apparent effects we see in this study, as the results on different datasets cannot be interpreted as being on the same scale. Although all metrics are based on a rank of the same number of competitors, the distribution of similarities and differences between the semantic representations of these competitors may differ across datasets. The combined test set scores are more directly comparable, but ideally, we would like to compare the generalization of both models on an independent test set. In future work, we intend to curate a test set with data from separate sources, which can serve as a benchmark for the models we study. We intend to explore how a curriculum of CDS followed by ADS affects learning trajectories and outcomes. We also intend to use tools for interpreting the knowledge encoded in neural networks (such as diagnostic classifiers and representational similarity analysis) to investigate the emergent representation of linguistic units such as phonemes and words.",negative
449,From Arguments to Key Points: Towards Automatic Argument Summarization,"Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in advance. We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task. We report empirical results for an extensive set of experiments with this dataset, showing promising performance.","The advent of quantum computing has opened up new avenues for solving complex problems that were previously thought to be intractable, offering exponential speed-ups for certain types of calculations.. In the realm of computer science, the use of recursion in algorithm design can lead to more efficient solutions, especially in situations where the problem can be broken down into smaller, simpler versions of itself.. The concept of Turing machines and their ability to simulate any other computing device has been a cornerstone in the understanding of the limitations and capabilities of computational systems.. The starting point for the current work is a collection of pro and con arguments for a given topic. As previously mentioned, these arguments may be collected from a large audience by conducting a survey, or mined automatically from text.
Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014).
Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020).
Stance classification of extracted arguments can be performed as a separate step (Bar-Haim et al., 2017) or jointly with argument detection, as a three-way classification (pro argument/con argument/none), as done by Stab et al. (2018b).",negative
