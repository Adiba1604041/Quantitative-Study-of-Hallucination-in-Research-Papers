{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7747cb1a-12b6-42c9-9609-8b5a3d06d212",
      "metadata": {
        "id": "7747cb1a-12b6-42c9-9609-8b5a3d06d212"
      },
      "source": [
        "## **Step 4: AI vs Human**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f2ef91-7103-4098-8794-8543ab1dba56",
      "metadata": {
        "id": "40f2ef91-7103-4098-8794-8543ab1dba56"
      },
      "source": [
        "## Human vs AI (Mistral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41cafb44-71f4-4ffc-a22e-82acf646f5b7",
      "metadata": {
        "id": "41cafb44-71f4-4ffc-a22e-82acf646f5b7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/XXXX/local_cache\"\n",
        "os.environ[\"HF_HOME\"] = \"/XXXX/local_cache\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "login(token=\"XXXX\")\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "CHECKPOINT_FILE = \"mistral25_checkpoint.json\"\n",
        "OUTPUT_FILE = \"2025_AI_vs_Human_Mistral.csv\"\n",
        "\n",
        "def save_checkpoint(current_index, results):\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        json.dump({\"last_processed\": current_index, \"results\": results}, f)\n",
        "\n",
        "def load_checkpoint():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return {\"last_processed\": -1, \"results\": []}\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Load model and tokenizer once.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    return model, tokenizer\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def query_model(paragraphs, model, tokenizer):\n",
        "    labels = [\"AI-generated\", \"Human-written\"]\n",
        "    label_token_ids = [tokenizer.encode(label, add_special_tokens=False) for label in labels]\n",
        "\n",
        "    predicted_labels = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        prompt = f\"\"\"You are a text classifier. Given a paragraph, classify whether it is \\\"AI-generated\\\" or \\\"Human-written\\\".\n",
        "        Respond ONLY with one of these two labels.\n",
        "            For each criterion below, assign a score:\n",
        "     **+1 for AI-like** if the text strongly matches the AI trait.\n",
        "     **+1 for Human-like** if the text strongly matches the human trait.\n",
        "     **+0 if neutral/uncertain**.\n",
        "\n",
        "    #### **Evaluation Steps**:\n",
        "    1. **Perplexity & Creativity**:\n",
        "     AI-like: Predictable word choices, clichés, or overly fluent phrasing.\n",
        "     Human-like: Unusual phrasing, creative metaphors, or minor errors.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    2. **Burstiness**:\n",
        "     AI-like: Uniform sentence length/structure (e.g., all medium-length).\n",
        "     Human-like: Varied rhythm (mix of short/long sentences, interruptions).\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    3. **Specificity & Personalization**:\n",
        "     AI-like: Lacks concrete details (no names, anecdotes, or emotions).\n",
        "     Human-like: Uses \"I/we,\" personal stories, opinions, or informal language.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    4. **Logical Flow**:\n",
        "     AI-like: Abrupt topic shifts or overly rigid structure.\n",
        "     Human-like: Natural digressions, conversational flow.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    5. **Errors & Imperfections**:\n",
        "     AI-like: Grammatically flawless, no typos/colloquialisms.\n",
        "     Human-like: Minor errors, idiosyncratic punctuation, or slang.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    #### **Decision Rule**:\n",
        "     If **total AI-like >= 3** then \"AI-generated.\"\n",
        "     Else \"Human-written.\" (Default to Human-written if uncertain.)\n",
        "\n",
        "        Paragraph: {paragraph}\\nAnswer:\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        last_token_logits = logits[0, -1, :]\n",
        "\n",
        "        label_scores = []\n",
        "        for token_ids in label_token_ids:\n",
        "            score = sum(last_token_logits[token_id].item() for token_id in token_ids) / len(token_ids)\n",
        "            label_scores.append(score)\n",
        "\n",
        "        predicted_label = labels[label_scores.index(max(label_scores))]\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "def process_dataset(input_csv, output_csv, batch_size=2):\n",
        "    checkpoint = load_checkpoint()\n",
        "    start_index = checkpoint[\"last_processed\"] + 1\n",
        "    results = checkpoint[\"results\"]\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    negative_samples = df[df['Majority_Label'] == 'negative']\n",
        "    paragraphs = negative_samples['Paragraph'].tolist()\n",
        "    original_indices = negative_samples.index.tolist()\n",
        "\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    try:\n",
        "        for i in range(start_index, len(paragraphs), batch_size):\n",
        "            batch_paragraphs = paragraphs[i:i + batch_size]\n",
        "            batch_indices = original_indices[i:i + batch_size]\n",
        "\n",
        "            batch_labels = query_model(batch_paragraphs, model, tokenizer)\n",
        "\n",
        "            for idx, paragraph, label in zip(batch_indices, batch_paragraphs, batch_labels):\n",
        "                results.append({\n",
        "                    \"Original_Index\": idx,\n",
        "                    \"Paragraph\": paragraph,\n",
        "                    \"Mistral-7B\": label\n",
        "                })\n",
        "\n",
        "            save_checkpoint(i + len(batch_paragraphs) - 1, results)\n",
        "            pd.DataFrame(results).to_csv(output_csv, index=False)\n",
        "            print(f\"Processed up to index {i + len(batch_paragraphs) - 1}\")\n",
        "\n",
        "    finally:\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Processing complete!\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset(\"2025_Majority_Labeled.csv\", \"2025_AI_vs_Human_Mistral.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0977afb-a955-4eb8-98f8-1d3c1de382e0",
      "metadata": {
        "id": "d0977afb-a955-4eb8-98f8-1d3c1de382e0"
      },
      "source": [
        "## Human vs AI (DeepSeek)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4b6722-a296-4d73-8b69-abac5687bae7",
      "metadata": {
        "id": "ca4b6722-a296-4d73-8b69-abac5687bae7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/XXXX/local_cache\"\n",
        "os.environ[\"HF_HOME\"] = \"/XXXX/local_cache\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "MODEL_NAME = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "CHECKPOINT_FILE = \"deepseek25_checkpoint.json\"\n",
        "OUTPUT_FILE = \"2025_AI_vs_Human_deepseek.csv\"\n",
        "\n",
        "def save_checkpoint(current_index, results):\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        json.dump({\"last_processed\": current_index, \"results\": results}, f)\n",
        "\n",
        "def load_checkpoint():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return {\"last_processed\": -1, \"results\": []}\n",
        "\n",
        "def load_model():\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=\"/XXXX/local_cache\",)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        cache_dir=\"/XXXX/local_cache\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    return model, tokenizer\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def query_model(paragraphs, model, tokenizer):\n",
        "    labels = [\"AI-generated\", \"Human-written\"]\n",
        "    label_token_ids = [tokenizer.encode(label, add_special_tokens=False) for label in labels]\n",
        "\n",
        "    predicted_labels = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        prompt = f\"\"\"You are a text classifier. Given a paragraph, classify whether it is \\\"AI-generated\\\" or \\\"Human-written\\\".\n",
        "        Respond ONLY with one of these two labels.\n",
        "        For each criterion below, assign a score:\n",
        "     **+1 for AI-like** if the text strongly matches the AI trait.\n",
        "     **+1 for Human-like** if the text strongly matches the human trait.\n",
        "     **+0 if neutral/uncertain**.\n",
        "\n",
        "    #### **Evaluation Steps**:\n",
        "    1. **Perplexity & Creativity**:\n",
        "     AI-like: Predictable word choices, clichés, or overly fluent phrasing.\n",
        "     Human-like: Unusual phrasing, creative metaphors, or minor errors.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    2. **Burstiness**:\n",
        "     AI-like: Uniform sentence length/structure (e.g., all medium-length).\n",
        "     Human-like: Varied rhythm (mix of short/long sentences, interruptions).\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    3. **Specificity & Personalization**:\n",
        "     AI-like: Lacks concrete details (no names, anecdotes, or emotions).\n",
        "     Human-like: Uses \"I/we,\" personal stories, opinions, or informal language.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    4. **Logical Flow**:\n",
        "     AI-like: Abrupt topic shifts or overly rigid structure.\n",
        "     Human-like: Natural digressions, conversational flow.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    5. **Errors & Imperfections**:\n",
        "     AI-like: Grammatically flawless, no typos/colloquialisms.\n",
        "     Human-like: Minor errors, idiosyncratic punctuation, or slang.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    #### **Decision Rule**:\n",
        "     If **total AI-like >= 3** then \"AI-generated.\"\n",
        "     Else \"Human-written.\" (Default to Human-written if uncertain.)\n",
        "\n",
        "Paragraph: {paragraph}\\nAnswer:\"\"\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False, return_attention_mask=True).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        last_token_logits = logits[0, -1, :]\n",
        "\n",
        "        label_scores = []\n",
        "        for token_ids in label_token_ids:\n",
        "            score = sum(last_token_logits[token_id].item() for token_id in token_ids) / len(token_ids)\n",
        "            label_scores.append(score)\n",
        "\n",
        "        predicted_label = labels[label_scores.index(max(label_scores))]\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "def process_dataset(input_csv, output_csv, batch_size=1):\n",
        "    checkpoint = load_checkpoint()\n",
        "    start_index = checkpoint[\"last_processed\"] + 1\n",
        "    results = checkpoint[\"results\"]\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    negative_samples = df[df['Majority_Label'] == 'negative']\n",
        "    paragraphs = negative_samples['Paragraph'].tolist()\n",
        "    original_indices = negative_samples.index.tolist()\n",
        "\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    try:\n",
        "        for i in range(start_index, len(paragraphs), batch_size):\n",
        "            batch_paragraphs = paragraphs[i:i + batch_size]\n",
        "            batch_indices = original_indices[i:i + batch_size]\n",
        "\n",
        "            batch_labels = query_model(batch_paragraphs, model, tokenizer)\n",
        "\n",
        "            for idx, paragraph, label in zip(batch_indices, batch_paragraphs, batch_labels):\n",
        "                results.append({\n",
        "                    \"Original_Index\": idx,\n",
        "                    \"Paragraph\": paragraph,\n",
        "                    \"Deepseek\": label\n",
        "                })\n",
        "\n",
        "            save_checkpoint(i + len(batch_paragraphs) - 1, results)\n",
        "            pd.DataFrame(results).to_csv(output_csv, index=False)\n",
        "            print(f\"Processed up to index {i + len(batch_paragraphs) - 1}\")\n",
        "\n",
        "    finally:\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Processing complete!\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset(\"2025_Majority_Labeled.csv\", \"2025_AI_vs_Human_deepseek.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b560188-cda0-48d7-9a78-4bc2419a10c4",
      "metadata": {
        "id": "2b560188-cda0-48d7-9a78-4bc2419a10c4"
      },
      "source": [
        "## Human vs AI (Llama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f2e40a-8859-488a-a121-bd250e63f9b2",
      "metadata": {
        "id": "94f2e40a-8859-488a-a121-bd250e63f9b2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"/XXXX/local_cache\"\n",
        "os.environ[\"HF_HOME\"] = \"/XXXX/local_cache\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "login(token=\"XXXX\")\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "CHECKPOINT_FILE = \"llama25_checkpoint.json\"\n",
        "OUTPUT_FILE = \"2025_AI_vs_Human_Llama.csv\"\n",
        "\n",
        "def save_checkpoint(current_index, results):\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        json.dump({\"last_processed\": current_index, \"results\": results}, f)\n",
        "\n",
        "def load_checkpoint():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return {\"last_processed\": -1, \"results\": []}\n",
        "\n",
        "def load_model():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=\"/XXXX/local_cache\",)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        cache_dir=\"/XXXX/local_cache\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    return model, tokenizer\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def query_model(paragraphs, model, tokenizer):\n",
        "    labels = [\"AI-generated\", \"Human-written\"]\n",
        "    label_token_ids = [tokenizer.encode(label, add_special_tokens=False) for label in labels]\n",
        "\n",
        "    predicted_labels = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        prompt = f\"\"\"You are a text classifier. Given a paragraph, classify whether it is \\\"AI-generated\\\" or \\\"Human-written\\\".\n",
        "        Respond ONLY with one of these two labels.\n",
        "        For each criterion below, assign a score:\n",
        "     **+1 for AI-like** if the text strongly matches the AI trait.\n",
        "     **+1 for Human-like** if the text strongly matches the human trait.\n",
        "     **+0 if neutral/uncertain**.\n",
        "\n",
        "    #### **Evaluation Steps**:\n",
        "    1. **Perplexity & Creativity**:\n",
        "     AI-like: Predictable word choices, clichés, or overly fluent phrasing.\n",
        "     Human-like: Unusual phrasing, creative metaphors, or minor errors.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    2. **Burstiness**:\n",
        "     AI-like: Uniform sentence length/structure (e.g., all medium-length).\n",
        "     Human-like: Varied rhythm (mix of short/long sentences, interruptions).\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    3. **Specificity & Personalization**:\n",
        "     AI-like: Lacks concrete details (no names, anecdotes, or emotions).\n",
        "     Human-like: Uses \"I/we,\" personal stories, opinions, or informal language.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    4. **Logical Flow**:\n",
        "     AI-like: Abrupt topic shifts or overly rigid structure.\n",
        "     Human-like: Natural digressions, conversational flow.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    5. **Errors & Imperfections**:\n",
        "     AI-like: Grammatically flawless, no typos/colloquialisms.\n",
        "     Human-like: Minor errors, idiosyncratic punctuation, or slang.\n",
        "     Score: AI-like (+1) / Human-like (+1) / Neutral (0).\n",
        "\n",
        "    #### **Decision Rule**:\n",
        "     If **total AI-like >= 3** then \"AI-generated.\"\n",
        "     Else \"Human-written.\" (Default to Human-written if uncertain.)\n",
        "        Paragraph: {paragraph}\\nAnswer:\"\"\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False, return_attention_mask=True).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        last_token_logits = logits[0, -1, :]\n",
        "\n",
        "        label_scores = []\n",
        "        for token_ids in label_token_ids:\n",
        "            score = sum(last_token_logits[token_id].item() for token_id in token_ids) / len(token_ids)\n",
        "            label_scores.append(score)\n",
        "\n",
        "        predicted_label = labels[label_scores.index(max(label_scores))]\n",
        "        predicted_labels.append(predicted_label)\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "def process_dataset(input_csv, output_csv, batch_size=1):\n",
        "    checkpoint = load_checkpoint()\n",
        "    start_index = checkpoint[\"last_processed\"] + 1\n",
        "    results = checkpoint[\"results\"]\n",
        "\n",
        "    df = pd.read_csv(input_csv)\n",
        "    negative_samples = df[df['Majority_Label'] == 'negative']\n",
        "    paragraphs = negative_samples['Paragraph'].tolist()\n",
        "    original_indices = negative_samples.index.tolist()\n",
        "\n",
        "    model, tokenizer = load_model()\n",
        "\n",
        "    try:\n",
        "        for i in range(start_index, len(paragraphs), batch_size):\n",
        "            batch_paragraphs = paragraphs[i:i + batch_size]\n",
        "            batch_indices = original_indices[i:i + batch_size]\n",
        "\n",
        "            batch_labels = query_model(batch_paragraphs, model, tokenizer)\n",
        "\n",
        "            for idx, paragraph, label in zip(batch_indices, batch_paragraphs, batch_labels):\n",
        "                results.append({\n",
        "                    \"Original_Index\": idx,\n",
        "                    \"Paragraph\": paragraph,\n",
        "                    \"Llama-7B\": label\n",
        "                })\n",
        "\n",
        "            save_checkpoint(i + len(batch_paragraphs) - 1, results)\n",
        "            pd.DataFrame(results).to_csv(output_csv, index=False)\n",
        "            print(f\"Processed up to index {i + len(batch_paragraphs) - 1}\")\n",
        "\n",
        "    finally:\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Processing complete!\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset(\"2025_Majority_Labeled.csv\", \"2025_AI_vs_Human_Llama.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c8808a-5883-4320-bfa0-3e325747b2fe",
      "metadata": {
        "id": "46c8808a-5883-4320-bfa0-3e325747b2fe"
      },
      "source": [
        "## 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac7d8f6-7108-4aa8-82e5-373cc1d47aa9",
      "metadata": {
        "id": "8ac7d8f6-7108-4aa8-82e5-373cc1d47aa9"
      },
      "outputs": [],
      "source": [
        "hum_ai_Mistral_2023 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/Mistral Final/Logit based/2023_AI_vs_Human_Mistral.csv\",encoding=\"ISO-8859-1\")\n",
        "hum_ai_Llama_2023 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/Llama Final/Logit based/2023_AI_vs_Human_Llama.csv\",encoding=\"ISO-8859-1\")\n",
        "hum_ai_DeepSeek_2023 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/DeepSeek Final/Logit based/2023_AI_vs_Human_deepseek.csv\",encoding=\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6470ef98-66e5-4c1b-bccf-aca21ec29626",
      "metadata": {
        "id": "6470ef98-66e5-4c1b-bccf-aca21ec29626"
      },
      "outputs": [],
      "source": [
        "temp_df = pd.merge(hum_ai_Mistral_2023, hum_ai_Llama_2023, on='Original_Index')\n",
        "\n",
        "merged_df = pd.merge(temp_df, hum_ai_DeepSeek_2023, on='Original_Index')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9078e6ec-6cc7-4c0a-8446-d70987e7cd2f",
      "metadata": {
        "id": "9078e6ec-6cc7-4c0a-8446-d70987e7cd2f"
      },
      "outputs": [],
      "source": [
        "merged_df_23=merged_df[[\"Original_Index\",\"Paragraph\",\"Mistral-7B\",\"Llama-7B\",\"Deepseek\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b3024b-3ff8-44e3-8017-7307f3edd28f",
      "metadata": {
        "id": "92b3024b-3ff8-44e3-8017-7307f3edd28f",
        "outputId": "caf56756-a131-4981-a04a-bdee1cbb6ce9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original_Index</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Mistral-7B</th>\n",
              "      <th>Llama-7B</th>\n",
              "      <th>Deepseek</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31</td>\n",
              "      <td>Iterative Back-Translation (IBT) (Hoang et al....</td>\n",
              "      <td>AI-generated</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>We would like to thank all the LKLab lab mates...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>88</td>\n",
              "      <td>DocRED and DocREDScratch. DocRED contains 56,3...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>96</td>\n",
              "      <td>3 A1. Did you describe the limitations of your...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>AI-generated</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110</td>\n",
              "      <td>We analyze the metric evaluation with respect ...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284</th>\n",
              "      <td>8665</td>\n",
              "      <td>Following Cui et al. (2022), we comprehensivel...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>AI-generated</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2285</th>\n",
              "      <td>8668</td>\n",
              "      <td>We mentioned the necessity of developing multi...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2286</th>\n",
              "      <td>8669</td>\n",
              "      <td>As Table 3 suggests, MCWQ-R is easier than its...</td>\n",
              "      <td>AI-generated</td>\n",
              "      <td>AI-generated</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>8670</td>\n",
              "      <td>PLM comparison. mT5 fine-tuned on English fail...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2288</th>\n",
              "      <td>8680</td>\n",
              "      <td>The exact match accuracies on the 3 maximum co...</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2289 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Original_Index                                          Paragraph  \\\n",
              "0                 31  Iterative Back-Translation (IBT) (Hoang et al....   \n",
              "1                 66  We would like to thank all the LKLab lab mates...   \n",
              "2                 88  DocRED and DocREDScratch. DocRED contains 56,3...   \n",
              "3                 96  3 A1. Did you describe the limitations of your...   \n",
              "4                110  We analyze the metric evaluation with respect ...   \n",
              "...              ...                                                ...   \n",
              "2284            8665  Following Cui et al. (2022), we comprehensivel...   \n",
              "2285            8668  We mentioned the necessity of developing multi...   \n",
              "2286            8669  As Table 3 suggests, MCWQ-R is easier than its...   \n",
              "2287            8670  PLM comparison. mT5 fine-tuned on English fail...   \n",
              "2288            8680  The exact match accuracies on the 3 maximum co...   \n",
              "\n",
              "         Mistral-7B       Llama-7B       Deepseek  \n",
              "0      AI-generated  Human-written  Human-written  \n",
              "1     Human-written  Human-written  Human-written  \n",
              "2     Human-written  Human-written   AI-generated  \n",
              "3     Human-written   AI-generated   AI-generated  \n",
              "4     Human-written  Human-written  Human-written  \n",
              "...             ...            ...            ...  \n",
              "2284  Human-written   AI-generated  Human-written  \n",
              "2285  Human-written  Human-written  Human-written  \n",
              "2286   AI-generated   AI-generated  Human-written  \n",
              "2287  Human-written  Human-written  Human-written  \n",
              "2288  Human-written  Human-written  Human-written  \n",
              "\n",
              "[2289 rows x 5 columns]"
            ]
          },
          "execution_count": 543,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df_23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dee4cc7-79ae-4911-86f9-78400a91347c",
      "metadata": {
        "id": "9dee4cc7-79ae-4911-86f9-78400a91347c"
      },
      "outputs": [],
      "source": [
        "mis_23=merged_df[\"Mistral-7B\"].tolist()\n",
        "lm_23=merged_df[\"Llama-7B\"].tolist()\n",
        "dp_23=merged_df[\"Deepseek\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cf96b22-30cb-4256-a222-5d0a8ce657b9",
      "metadata": {
        "id": "4cf96b22-30cb-4256-a222-5d0a8ce657b9",
        "outputId": "fd9cb065-0506-4753-c161-88af0a86ed18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Consensus: 81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "model_predictions = [\n",
        "    mis_23,lm_23,dp_23\n",
        "]\n",
        "\n",
        "predictions = np.array(model_predictions)\n",
        "\n",
        "final_predictions = []\n",
        "consensus_scores = []\n",
        "\n",
        "for i in range(predictions.shape[1]):\n",
        "    votes = predictions[:, i]\n",
        "    majority_vote = Counter(votes).most_common(1)[0][0]\n",
        "    final_predictions.append(majority_vote)\n",
        "\n",
        "    agreement = np.mean(votes == majority_vote)\n",
        "    consensus_scores.append(agreement)\n",
        "\n",
        "print(\"Average Consensus:\", f\"{np.mean(consensus_scores):.0%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477e4fab-d969-4b92-8b02-43257816bcc0",
      "metadata": {
        "id": "477e4fab-d969-4b92-8b02-43257816bcc0"
      },
      "outputs": [],
      "source": [
        "f_23=merged_df_23[[\"Original_Index\",\"Paragraph\"]].copy()\n",
        "f_23[\"AI_HUM\"]=final_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0efdad45-baef-4ed5-9c95-07b31c5ff3ef",
      "metadata": {
        "id": "0efdad45-baef-4ed5-9c95-07b31c5ff3ef",
        "outputId": "e9b74e91-d52d-4c85-b249-ca93741edb77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original_Index</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31</td>\n",
              "      <td>Iterative Back-Translation (IBT) (Hoang et al....</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>We would like to thank all the LKLab lab mates...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>88</td>\n",
              "      <td>DocRED and DocREDScratch. DocRED contains 56,3...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>96</td>\n",
              "      <td>3 A1. Did you describe the limitations of your...</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110</td>\n",
              "      <td>We analyze the metric evaluation with respect ...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284</th>\n",
              "      <td>8665</td>\n",
              "      <td>Following Cui et al. (2022), we comprehensivel...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2285</th>\n",
              "      <td>8668</td>\n",
              "      <td>We mentioned the necessity of developing multi...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2286</th>\n",
              "      <td>8669</td>\n",
              "      <td>As Table 3 suggests, MCWQ-R is easier than its...</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>8670</td>\n",
              "      <td>PLM comparison. mT5 fine-tuned on English fail...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2288</th>\n",
              "      <td>8680</td>\n",
              "      <td>The exact match accuracies on the 3 maximum co...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2289 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Original_Index                                          Paragraph  \\\n",
              "0                 31  Iterative Back-Translation (IBT) (Hoang et al....   \n",
              "1                 66  We would like to thank all the LKLab lab mates...   \n",
              "2                 88  DocRED and DocREDScratch. DocRED contains 56,3...   \n",
              "3                 96  3 A1. Did you describe the limitations of your...   \n",
              "4                110  We analyze the metric evaluation with respect ...   \n",
              "...              ...                                                ...   \n",
              "2284            8665  Following Cui et al. (2022), we comprehensivel...   \n",
              "2285            8668  We mentioned the necessity of developing multi...   \n",
              "2286            8669  As Table 3 suggests, MCWQ-R is easier than its...   \n",
              "2287            8670  PLM comparison. mT5 fine-tuned on English fail...   \n",
              "2288            8680  The exact match accuracies on the 3 maximum co...   \n",
              "\n",
              "             AI_HUM  \n",
              "0     Human-written  \n",
              "1     Human-written  \n",
              "2     Human-written  \n",
              "3      AI-generated  \n",
              "4     Human-written  \n",
              "...             ...  \n",
              "2284  Human-written  \n",
              "2285  Human-written  \n",
              "2286   AI-generated  \n",
              "2287  Human-written  \n",
              "2288  Human-written  \n",
              "\n",
              "[2289 rows x 3 columns]"
            ]
          },
          "execution_count": 547,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad84ec25-5172-4b0c-a9a2-89e43e56a6d9",
      "metadata": {
        "id": "ad84ec25-5172-4b0c-a9a2-89e43e56a6d9"
      },
      "outputs": [],
      "source": [
        "f_23=f_23[[\"Original_Index\",\"AI_HUM\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a818ce1b-2b8a-4bb2-88ec-570c4280d744",
      "metadata": {
        "id": "a818ce1b-2b8a-4bb2-88ec-570c4280d744",
        "outputId": "244a178f-8a43-407c-d189-20715e36f4a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original_Index</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>66</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>88</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>96</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284</th>\n",
              "      <td>8665</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2285</th>\n",
              "      <td>8668</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2286</th>\n",
              "      <td>8669</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>8670</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2288</th>\n",
              "      <td>8680</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2289 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Original_Index         AI_HUM\n",
              "0                 31  Human-written\n",
              "1                 66  Human-written\n",
              "2                 88  Human-written\n",
              "3                 96   AI-generated\n",
              "4                110  Human-written\n",
              "...              ...            ...\n",
              "2284            8665  Human-written\n",
              "2285            8668  Human-written\n",
              "2286            8669   AI-generated\n",
              "2287            8670  Human-written\n",
              "2288            8680  Human-written\n",
              "\n",
              "[2289 rows x 2 columns]"
            ]
          },
          "execution_count": 549,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c450501f-dc54-4ed4-891d-f23e0f8fad4e",
      "metadata": {
        "id": "c450501f-dc54-4ed4-891d-f23e0f8fad4e"
      },
      "outputs": [],
      "source": [
        "df_2023 = pd.read_csv(r\"/XXXX/ACL/CSVs/After_Majority_Voting_Relevance/2023_Majority_Labeled.csv\",encoding=\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8ad134-ed6a-4d23-98da-a3183b72abe5",
      "metadata": {
        "id": "9c8ad134-ed6a-4d23-98da-a3183b72abe5"
      },
      "outputs": [],
      "source": [
        "df_2023=df_2023[[\"Title\",\"Abstract\",\"Paragraph\",\"Majority_Label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70117a2-5b00-40ef-a67f-141232158b8f",
      "metadata": {
        "id": "a70117a2-5b00-40ef-a67f-141232158b8f",
        "outputId": "d6bcd8ee-ee66-481d-c4d6-6b1ff9a2435c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Majority_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Long-form question answering (Fan et al., 2019...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Fa ct ua lit y Does your body ab-\\nsorb all bl...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>We begin by reviewing the evaluation protocols...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Prior LFQA human evaluations use non-expert cr...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Hiring experts: We recruit domain experts on t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8686</th>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>linguistic phenomena, demographic groups repre...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8687</th>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>(e.g., GPU hours), and computing infrastructur...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>statistics from sets of experiments), and is i...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>you report the implementation, model, and para...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>disclaimers of any risks to participants or an...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8691 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  \\\n",
              "0     A Critical Evaluation of Evaluations for Long-...   \n",
              "1     A Critical Evaluation of Evaluations for Long-...   \n",
              "2     A Critical Evaluation of Evaluations for Long-...   \n",
              "3     A Critical Evaluation of Evaluations for Long-...   \n",
              "4     A Critical Evaluation of Evaluations for Long-...   \n",
              "...                                                 ...   \n",
              "8686  On Evaluating Multilingual Compositional Gener...   \n",
              "8687  On Evaluating Multilingual Compositional Gener...   \n",
              "8688  On Evaluating Multilingual Compositional Gener...   \n",
              "8689  On Evaluating Multilingual Compositional Gener...   \n",
              "8690  On Evaluating Multilingual Compositional Gener...   \n",
              "\n",
              "                                               Abstract  \\\n",
              "0     Long-form question answering (LFQA) enables an...   \n",
              "1     Long-form question answering (LFQA) enables an...   \n",
              "2     Long-form question answering (LFQA) enables an...   \n",
              "3     Long-form question answering (LFQA) enables an...   \n",
              "4     Long-form question answering (LFQA) enables an...   \n",
              "...                                                 ...   \n",
              "8686  Compositional generalization allows efficient ...   \n",
              "8687  Compositional generalization allows efficient ...   \n",
              "8688  Compositional generalization allows efficient ...   \n",
              "8689  Compositional generalization allows efficient ...   \n",
              "8690  Compositional generalization allows efficient ...   \n",
              "\n",
              "                                              Paragraph Majority_Label  \n",
              "0     Long-form question answering (Fan et al., 2019...       positive  \n",
              "1     Fa ct ua lit y Does your body ab-\\nsorb all bl...       positive  \n",
              "2     We begin by reviewing the evaluation protocols...       positive  \n",
              "3     Prior LFQA human evaluations use non-expert cr...       positive  \n",
              "4     Hiring experts: We recruit domain experts on t...       positive  \n",
              "...                                                 ...            ...  \n",
              "8686  linguistic phenomena, demographic groups repre...       positive  \n",
              "8687  (e.g., GPU hours), and computing infrastructur...       positive  \n",
              "8688  statistics from sets of experiments), and is i...       positive  \n",
              "8689  you report the implementation, model, and para...       positive  \n",
              "8690  disclaimers of any risks to participants or an...       positive  \n",
              "\n",
              "[8691 rows x 4 columns]"
            ]
          },
          "execution_count": 742,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0fa34b5-253f-4b96-8fc7-3d0841bc5c3d",
      "metadata": {
        "id": "e0fa34b5-253f-4b96-8fc7-3d0841bc5c3d",
        "outputId": "44f095ea-e920-423b-cd2c-1f92ca28832a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Majority_Label</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Long-form question answering (Fan et al., 2019...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Fa ct ua lit y Does your body ab-\\nsorb all bl...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>We begin by reviewing the evaluation protocols...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Prior LFQA human evaluations use non-expert cr...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A Critical Evaluation of Evaluations for Long-...</td>\n",
              "      <td>Long-form question answering (LFQA) enables an...</td>\n",
              "      <td>Hiring experts: We recruit domain experts on t...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8686</th>\n",
              "      <td>8686</td>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>linguistic phenomena, demographic groups repre...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8687</th>\n",
              "      <td>8687</td>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>(e.g., GPU hours), and computing infrastructur...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8688</th>\n",
              "      <td>8688</td>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>statistics from sets of experiments), and is i...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8689</th>\n",
              "      <td>8689</td>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>you report the implementation, model, and para...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8690</th>\n",
              "      <td>8690</td>\n",
              "      <td>On Evaluating Multilingual Compositional Gener...</td>\n",
              "      <td>Compositional generalization allows efficient ...</td>\n",
              "      <td>disclaimers of any risks to participants or an...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8691 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                                              Title  \\\n",
              "0         0  A Critical Evaluation of Evaluations for Long-...   \n",
              "1         1  A Critical Evaluation of Evaluations for Long-...   \n",
              "2         2  A Critical Evaluation of Evaluations for Long-...   \n",
              "3         3  A Critical Evaluation of Evaluations for Long-...   \n",
              "4         4  A Critical Evaluation of Evaluations for Long-...   \n",
              "...     ...                                                ...   \n",
              "8686   8686  On Evaluating Multilingual Compositional Gener...   \n",
              "8687   8687  On Evaluating Multilingual Compositional Gener...   \n",
              "8688   8688  On Evaluating Multilingual Compositional Gener...   \n",
              "8689   8689  On Evaluating Multilingual Compositional Gener...   \n",
              "8690   8690  On Evaluating Multilingual Compositional Gener...   \n",
              "\n",
              "                                               Abstract  \\\n",
              "0     Long-form question answering (LFQA) enables an...   \n",
              "1     Long-form question answering (LFQA) enables an...   \n",
              "2     Long-form question answering (LFQA) enables an...   \n",
              "3     Long-form question answering (LFQA) enables an...   \n",
              "4     Long-form question answering (LFQA) enables an...   \n",
              "...                                                 ...   \n",
              "8686  Compositional generalization allows efficient ...   \n",
              "8687  Compositional generalization allows efficient ...   \n",
              "8688  Compositional generalization allows efficient ...   \n",
              "8689  Compositional generalization allows efficient ...   \n",
              "8690  Compositional generalization allows efficient ...   \n",
              "\n",
              "                                              Paragraph Majority_Label AI_HUM  \n",
              "0     Long-form question answering (Fan et al., 2019...       positive    NaN  \n",
              "1     Fa ct ua lit y Does your body ab-\\nsorb all bl...       positive    NaN  \n",
              "2     We begin by reviewing the evaluation protocols...       positive    NaN  \n",
              "3     Prior LFQA human evaluations use non-expert cr...       positive    NaN  \n",
              "4     Hiring experts: We recruit domain experts on t...       positive    NaN  \n",
              "...                                                 ...            ...    ...  \n",
              "8686  linguistic phenomena, demographic groups repre...       positive    NaN  \n",
              "8687  (e.g., GPU hours), and computing infrastructur...       positive    NaN  \n",
              "8688  statistics from sets of experiments), and is i...       positive    NaN  \n",
              "8689  you report the implementation, model, and para...       positive    NaN  \n",
              "8690  disclaimers of any risks to participants or an...       positive    NaN  \n",
              "\n",
              "[8691 rows x 6 columns]"
            ]
          },
          "execution_count": 743,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Final_2023 = pd.concat([df_2023, f_23.set_index('Original_Index')], axis=1).reset_index()\n",
        "Final_2023 = pd.DataFrame(Final_2023)\n",
        "Final_2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2dc5d1-b62c-4c81-96e5-b1b450682a22",
      "metadata": {
        "id": "ac2dc5d1-b62c-4c81-96e5-b1b450682a22"
      },
      "outputs": [],
      "source": [
        "Final_2023=Final_2023[[\"Title\",\t\"Abstract\",\t\"Paragraph\", \"Majority_Label\",\t\"AI_HUM\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f291b83-34cb-4e41-914c-75d1797948ba",
      "metadata": {
        "id": "1f291b83-34cb-4e41-914c-75d1797948ba",
        "outputId": "0cf9359a-2a14-4386-8cfe-2114c127c596"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       N/A\n",
              "1       N/A\n",
              "2       N/A\n",
              "3       N/A\n",
              "4       N/A\n",
              "       ... \n",
              "8686    N/A\n",
              "8687    N/A\n",
              "8688    N/A\n",
              "8689    N/A\n",
              "8690    N/A\n",
              "Name: AI_HUM, Length: 8691, dtype: object"
            ]
          },
          "execution_count": 748,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Final_2023['AI_HUM']=Final_2023['AI_HUM'].fillna('N/A')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f34d265-c9df-4960-918b-d7a8f8f899d9",
      "metadata": {
        "id": "2f34d265-c9df-4960-918b-d7a8f8f899d9",
        "outputId": "22b28c21-82bb-4f0c-d1bf-5432e9d475e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Majority_Label</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Inspired by the Pyramid (Nenkova and Passonnea...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>CNNDM Test 500 12 5.6k 6k CNNDM V lid 1,000 8 ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We collect ACU annotations on three summarizat...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We analyze the statistical power of our collec...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>As a case study, in Tab. 3 we analyze the summ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Apart from ACU annotations, we collect human a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We collected three annotations per summary on ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We investigate both the summary-level and syst...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We analyze several representative automatic me...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We use the correlations between automatic metr...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We analyze the metric evaluation with respect ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We introduce RoSE, a benchmark whose underlyin...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Biases may be present in the data annotator as...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We discuss the detailed settings of ACU collec...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>BART (Lewis et al., 2020b) introduce a denoisi...</td>\n",
              "      <td>negative</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We use system outputs from Gao and Wan (2022)....</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We report the ACU scores of the summarization ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We describe the algorithm for the power analys...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Fig.4, Fig.5, and Fig.6 show the power analysi...</td>\n",
              "      <td>negative</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We use correlations to analyze the inherent si...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We summarize and compare different protocols i...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We present the result analysis of ÃÂÃÂ§5.2 ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We provide additional metric details as well a...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>In ÃÂÃÂ§6.1 we evaluate two different LLM-b...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We collect in total 50 different automatic met...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>For metric elevation in ÃÂÃÂ§6.1, we provid...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Bucket 3: MATCHSUM V.S. SIMCLS, FROST V.S. GLO...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We select several automatic metrics and calcul...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We use Alg.1 to conduct a power analysis of me...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>We provide a brief survey for the human evalua...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>Revisiting the Gold Standard: Grounding Summar...</td>\n",
              "      <td>Human evaluation is the foundation upon which ...</td>\n",
              "      <td>Section 1.\\n7 A4. Have you used AI writing ass...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Title  \\\n",
              "100  Revisiting the Gold Standard: Grounding Summar...   \n",
              "101  Revisiting the Gold Standard: Grounding Summar...   \n",
              "102  Revisiting the Gold Standard: Grounding Summar...   \n",
              "103  Revisiting the Gold Standard: Grounding Summar...   \n",
              "104  Revisiting the Gold Standard: Grounding Summar...   \n",
              "105  Revisiting the Gold Standard: Grounding Summar...   \n",
              "106  Revisiting the Gold Standard: Grounding Summar...   \n",
              "107  Revisiting the Gold Standard: Grounding Summar...   \n",
              "108  Revisiting the Gold Standard: Grounding Summar...   \n",
              "109  Revisiting the Gold Standard: Grounding Summar...   \n",
              "110  Revisiting the Gold Standard: Grounding Summar...   \n",
              "111  Revisiting the Gold Standard: Grounding Summar...   \n",
              "112  Revisiting the Gold Standard: Grounding Summar...   \n",
              "113  Revisiting the Gold Standard: Grounding Summar...   \n",
              "114  Revisiting the Gold Standard: Grounding Summar...   \n",
              "115  Revisiting the Gold Standard: Grounding Summar...   \n",
              "116  Revisiting the Gold Standard: Grounding Summar...   \n",
              "117  Revisiting the Gold Standard: Grounding Summar...   \n",
              "118  Revisiting the Gold Standard: Grounding Summar...   \n",
              "119  Revisiting the Gold Standard: Grounding Summar...   \n",
              "120  Revisiting the Gold Standard: Grounding Summar...   \n",
              "121  Revisiting the Gold Standard: Grounding Summar...   \n",
              "122  Revisiting the Gold Standard: Grounding Summar...   \n",
              "123  Revisiting the Gold Standard: Grounding Summar...   \n",
              "124  Revisiting the Gold Standard: Grounding Summar...   \n",
              "125  Revisiting the Gold Standard: Grounding Summar...   \n",
              "126  Revisiting the Gold Standard: Grounding Summar...   \n",
              "127  Revisiting the Gold Standard: Grounding Summar...   \n",
              "128  Revisiting the Gold Standard: Grounding Summar...   \n",
              "129  Revisiting the Gold Standard: Grounding Summar...   \n",
              "130  Revisiting the Gold Standard: Grounding Summar...   \n",
              "\n",
              "                                              Abstract  \\\n",
              "100  Human evaluation is the foundation upon which ...   \n",
              "101  Human evaluation is the foundation upon which ...   \n",
              "102  Human evaluation is the foundation upon which ...   \n",
              "103  Human evaluation is the foundation upon which ...   \n",
              "104  Human evaluation is the foundation upon which ...   \n",
              "105  Human evaluation is the foundation upon which ...   \n",
              "106  Human evaluation is the foundation upon which ...   \n",
              "107  Human evaluation is the foundation upon which ...   \n",
              "108  Human evaluation is the foundation upon which ...   \n",
              "109  Human evaluation is the foundation upon which ...   \n",
              "110  Human evaluation is the foundation upon which ...   \n",
              "111  Human evaluation is the foundation upon which ...   \n",
              "112  Human evaluation is the foundation upon which ...   \n",
              "113  Human evaluation is the foundation upon which ...   \n",
              "114  Human evaluation is the foundation upon which ...   \n",
              "115  Human evaluation is the foundation upon which ...   \n",
              "116  Human evaluation is the foundation upon which ...   \n",
              "117  Human evaluation is the foundation upon which ...   \n",
              "118  Human evaluation is the foundation upon which ...   \n",
              "119  Human evaluation is the foundation upon which ...   \n",
              "120  Human evaluation is the foundation upon which ...   \n",
              "121  Human evaluation is the foundation upon which ...   \n",
              "122  Human evaluation is the foundation upon which ...   \n",
              "123  Human evaluation is the foundation upon which ...   \n",
              "124  Human evaluation is the foundation upon which ...   \n",
              "125  Human evaluation is the foundation upon which ...   \n",
              "126  Human evaluation is the foundation upon which ...   \n",
              "127  Human evaluation is the foundation upon which ...   \n",
              "128  Human evaluation is the foundation upon which ...   \n",
              "129  Human evaluation is the foundation upon which ...   \n",
              "130  Human evaluation is the foundation upon which ...   \n",
              "\n",
              "                                             Paragraph Majority_Label  \\\n",
              "100  Inspired by the Pyramid (Nenkova and Passonnea...       positive   \n",
              "101  CNNDM Test 500 12 5.6k 6k CNNDM V lid 1,000 8 ...       positive   \n",
              "102  We collect ACU annotations on three summarizat...       positive   \n",
              "103  We analyze the statistical power of our collec...       positive   \n",
              "104  As a case study, in Tab. 3 we analyze the summ...       positive   \n",
              "105  Apart from ACU annotations, we collect human a...       positive   \n",
              "106  We collected three annotations per summary on ...       positive   \n",
              "107  We investigate both the summary-level and syst...       positive   \n",
              "108  We analyze several representative automatic me...       positive   \n",
              "109  We use the correlations between automatic metr...       positive   \n",
              "110  We analyze the metric evaluation with respect ...       negative   \n",
              "111  We introduce RoSE, a benchmark whose underlyin...       positive   \n",
              "112  Biases may be present in the data annotator as...       positive   \n",
              "113  We discuss the detailed settings of ACU collec...       positive   \n",
              "114  BART (Lewis et al., 2020b) introduce a denoisi...       negative   \n",
              "115  We use system outputs from Gao and Wan (2022)....       positive   \n",
              "116  We report the ACU scores of the summarization ...       positive   \n",
              "117  We describe the algorithm for the power analys...       negative   \n",
              "118  Fig.4, Fig.5, and Fig.6 show the power analysi...       negative   \n",
              "119  We use correlations to analyze the inherent si...       positive   \n",
              "120  We summarize and compare different protocols i...       positive   \n",
              "121  We present the result analysis of ÃÂÃÂ§5.2 ...       positive   \n",
              "122  We provide additional metric details as well a...       positive   \n",
              "123  In ÃÂÃÂ§6.1 we evaluate two different LLM-b...       positive   \n",
              "124  We collect in total 50 different automatic met...       positive   \n",
              "125  For metric elevation in ÃÂÃÂ§6.1, we provid...       negative   \n",
              "126  Bucket 3: MATCHSUM V.S. SIMCLS, FROST V.S. GLO...       positive   \n",
              "127  We select several automatic metrics and calcul...       positive   \n",
              "128  We use Alg.1 to conduct a power analysis of me...       negative   \n",
              "129  We provide a brief survey for the human evalua...       positive   \n",
              "130  Section 1.\\n7 A4. Have you used AI writing ass...       positive   \n",
              "\n",
              "            AI_HUM  \n",
              "100            N/A  \n",
              "101            N/A  \n",
              "102            N/A  \n",
              "103            N/A  \n",
              "104            N/A  \n",
              "105            N/A  \n",
              "106            N/A  \n",
              "107            N/A  \n",
              "108            N/A  \n",
              "109            N/A  \n",
              "110  Human-written  \n",
              "111            N/A  \n",
              "112            N/A  \n",
              "113            N/A  \n",
              "114   AI-generated  \n",
              "115            N/A  \n",
              "116            N/A  \n",
              "117  Human-written  \n",
              "118   AI-generated  \n",
              "119            N/A  \n",
              "120            N/A  \n",
              "121            N/A  \n",
              "122            N/A  \n",
              "123            N/A  \n",
              "124            N/A  \n",
              "125  Human-written  \n",
              "126            N/A  \n",
              "127            N/A  \n",
              "128  Human-written  \n",
              "129            N/A  \n",
              "130            N/A  "
            ]
          },
          "execution_count": 750,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Final_2023.loc[100:130]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6ee687-7fdd-4e23-ae2c-a2671b9a79bf",
      "metadata": {
        "id": "2f6ee687-7fdd-4e23-ae2c-a2671b9a79bf"
      },
      "outputs": [],
      "source": [
        "Final_2023.to_csv(\"Final_2023.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333bbd88-872f-4b13-91c4-d50f6f9bc537",
      "metadata": {
        "id": "333bbd88-872f-4b13-91c4-d50f6f9bc537"
      },
      "source": [
        "## 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed6405b-fd18-4474-818c-092ed018fd60",
      "metadata": {
        "id": "3ed6405b-fd18-4474-818c-092ed018fd60"
      },
      "outputs": [],
      "source": [
        "hum_ai_Mistral_2024 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/Mistral Final/Logit based/2024_AI_vs_Human_Mistral.csv\",encoding=\"ISO-8859-1\")\n",
        "hum_ai_Llama_2024 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/Llama Final/Logit based/2024_AI_vs_Human_Llama.csv\",encoding=\"ISO-8859-1\")\n",
        "hum_ai_DeepSeek_2024 = pd.read_csv(r\"/XXXX/ACL/CSVs/AI vs Human/DeepSeek Final/Logit based/2024_AI_vs_Human_deepseek.csv\",encoding=\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d158a0-78a9-42ee-8f5f-ea824aaef1f6",
      "metadata": {
        "id": "a5d158a0-78a9-42ee-8f5f-ea824aaef1f6"
      },
      "outputs": [],
      "source": [
        "temp_df = pd.merge(hum_ai_Mistral_2024, hum_ai_Llama_2024, on='Original_Index')\n",
        "\n",
        "merged_df = pd.merge(temp_df, hum_ai_DeepSeek_2024, on='Original_Index')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa08fb13-a1a3-4b5e-8f7e-00f282ae3251",
      "metadata": {
        "id": "aa08fb13-a1a3-4b5e-8f7e-00f282ae3251"
      },
      "outputs": [],
      "source": [
        "merged_df_24=merged_df[[\"Original_Index\",\"Paragraph\",\"Mistral-7B\",\"Llama-7B\",\"Deepseek\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869f77f9-b0be-4529-8b6c-40b15a9fff6b",
      "metadata": {
        "id": "869f77f9-b0be-4529-8b6c-40b15a9fff6b"
      },
      "outputs": [],
      "source": [
        "mis_24=merged_df_24[\"Mistral-7B\"].tolist()\n",
        "lm_24=merged_df_24[\"Llama-7B\"].tolist()\n",
        "dp_24=merged_df_24[\"Deepseek\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2d57fa-19a7-4f96-8a1c-b5b65f999f7a",
      "metadata": {
        "id": "ee2d57fa-19a7-4f96-8a1c-b5b65f999f7a",
        "outputId": "5d393e8e-0398-4f5e-97c6-704ff3f078eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Consensus: 81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "model_predictions = [\n",
        "    mis_24,lm_24,dp_24\n",
        "]\n",
        "\n",
        "predictions = np.array(model_predictions)\n",
        "\n",
        "final_predictions = []\n",
        "consensus_scores = []\n",
        "\n",
        "for i in range(predictions.shape[1]):\n",
        "    votes = predictions[:, i]\n",
        "    majority_vote = Counter(votes).most_common(1)[0][0]\n",
        "    final_predictions.append(majority_vote)\n",
        "\n",
        "    agreement = np.mean(votes == majority_vote)\n",
        "    consensus_scores.append(agreement)\n",
        "\n",
        "\n",
        "print(\"Average Consensus:\", f\"{np.mean(consensus_scores):.0%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949688e3-56ec-47f4-b181-38cd99293196",
      "metadata": {
        "id": "949688e3-56ec-47f4-b181-38cd99293196"
      },
      "outputs": [],
      "source": [
        "f_24=merged_df_24[[\"Original_Index\",\"Paragraph\"]].copy()\n",
        "f_24[\"AI_HUM\"]=final_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ece7793-d959-4933-b7f6-a104e214f2fa",
      "metadata": {
        "id": "1ece7793-d959-4933-b7f6-a104e214f2fa",
        "outputId": "5f9e4a26-b6c2-413b-9c37-715e290b2a64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original_Index</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Recent Large Language Models (LLMs) have made ...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Significant strides have been made in long-for...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>An alternative framework for evaluating long-f...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Meta-questions were manually raised by five ex...</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>We would like to express our profound gratitud...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>8179</td>\n",
              "      <td>This work was supported by the National Natura...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2128</th>\n",
              "      <td>8180</td>\n",
              "      <td>The statistical data for the MultiWOZ dataset ...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2129</th>\n",
              "      <td>8181</td>\n",
              "      <td>In this section, we provide a detailed overvie...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2130</th>\n",
              "      <td>8182</td>\n",
              "      <td>To explore where we should apply DualLoRA in T...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2131</th>\n",
              "      <td>8183</td>\n",
              "      <td>To delve into the slot accuracy performance of...</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2132 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Original_Index                                          Paragraph  \\\n",
              "0                  0  Recent Large Language Models (LLMs) have made ...   \n",
              "1                  2  Significant strides have been made in long-for...   \n",
              "2                  4  An alternative framework for evaluating long-f...   \n",
              "3                  5  Meta-questions were manually raised by five ex...   \n",
              "4                 23  We would like to express our profound gratitud...   \n",
              "...              ...                                                ...   \n",
              "2127            8179  This work was supported by the National Natura...   \n",
              "2128            8180  The statistical data for the MultiWOZ dataset ...   \n",
              "2129            8181  In this section, we provide a detailed overvie...   \n",
              "2130            8182  To explore where we should apply DualLoRA in T...   \n",
              "2131            8183  To delve into the slot accuracy performance of...   \n",
              "\n",
              "             AI_HUM  \n",
              "0     Human-written  \n",
              "1     Human-written  \n",
              "2     Human-written  \n",
              "3      AI-generated  \n",
              "4     Human-written  \n",
              "...             ...  \n",
              "2127  Human-written  \n",
              "2128  Human-written  \n",
              "2129  Human-written  \n",
              "2130  Human-written  \n",
              "2131  Human-written  \n",
              "\n",
              "[2132 rows x 3 columns]"
            ]
          },
          "execution_count": 814,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c32e01-1a44-4e3f-8ec4-dff6c127b325",
      "metadata": {
        "id": "c9c32e01-1a44-4e3f-8ec4-dff6c127b325"
      },
      "outputs": [],
      "source": [
        "f_24=f_24[[\"Original_Index\",\"AI_HUM\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a1d5a3-0456-43b5-927c-950f2a626b15",
      "metadata": {
        "id": "c5a1d5a3-0456-43b5-927c-950f2a626b15",
        "outputId": "eb4c95fd-2f9b-4886-fdeb-187f3e53b061"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original_Index</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>AI-generated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>8179</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2128</th>\n",
              "      <td>8180</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2129</th>\n",
              "      <td>8181</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2130</th>\n",
              "      <td>8182</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2131</th>\n",
              "      <td>8183</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2132 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Original_Index         AI_HUM\n",
              "0                  0  Human-written\n",
              "1                  2  Human-written\n",
              "2                  4  Human-written\n",
              "3                  5   AI-generated\n",
              "4                 23  Human-written\n",
              "...              ...            ...\n",
              "2127            8179  Human-written\n",
              "2128            8180  Human-written\n",
              "2129            8181  Human-written\n",
              "2130            8182  Human-written\n",
              "2131            8183  Human-written\n",
              "\n",
              "[2132 rows x 2 columns]"
            ]
          },
          "execution_count": 816,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc888bd-3bd8-4bc7-a319-014fe98a2d63",
      "metadata": {
        "id": "6dc888bd-3bd8-4bc7-a319-014fe98a2d63"
      },
      "outputs": [],
      "source": [
        "df_2024 = pd.read_csv(r\"/XXXX/ACL/CSVs/After_Majority_Voting_Relevance/2024_Majority_Labeled.csv\",encoding=\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e241e80a-f4d7-4082-8a4b-66fc78007bb6",
      "metadata": {
        "id": "e241e80a-f4d7-4082-8a4b-66fc78007bb6"
      },
      "outputs": [],
      "source": [
        "df_2024=df_2024[[\"Title\",\"Abstract\",\"Paragraph\",\"Majority_Label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15cbd97e-101f-4fac-8665-79ea4046bc0a",
      "metadata": {
        "id": "15cbd97e-101f-4fac-8665-79ea4046bc0a",
        "outputId": "5bf29489-8def-4607-b4ed-099e1fc6198e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Majority_Label</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Recent Large Language Models (LLMs) have made ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>spanning tens of thousands of tokens (Anthropi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Significant strides have been made in long-for...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Automated metrics such as surface form matchin...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>An alternative framework for evaluating long-f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8180</th>\n",
              "      <td>8180</td>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>The statistical data for the MultiWOZ dataset ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8181</th>\n",
              "      <td>8181</td>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>In this section, we provide a detailed overvie...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8182</th>\n",
              "      <td>8182</td>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>To explore where we should apply DualLoRA in T...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8183</th>\n",
              "      <td>8183</td>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>To delve into the slot accuracy performance of...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8184</th>\n",
              "      <td>8184</td>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>This section presents a more detailed analysis...</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8185 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index                                              Title  \\\n",
              "0         0  PROXYQA: An Alternative Framework for Evaluati...   \n",
              "1         1  PROXYQA: An Alternative Framework for Evaluati...   \n",
              "2         2  PROXYQA: An Alternative Framework for Evaluati...   \n",
              "3         3  PROXYQA: An Alternative Framework for Evaluati...   \n",
              "4         4  PROXYQA: An Alternative Framework for Evaluati...   \n",
              "...     ...                                                ...   \n",
              "8180   8180  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8181   8181  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8182   8182  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8183   8183  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8184   8184  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "\n",
              "                                               Abstract  \\\n",
              "0     Large Language Models (LLMs) have succeeded re...   \n",
              "1     Large Language Models (LLMs) have succeeded re...   \n",
              "2     Large Language Models (LLMs) have succeeded re...   \n",
              "3     Large Language Models (LLMs) have succeeded re...   \n",
              "4     Large Language Models (LLMs) have succeeded re...   \n",
              "...                                                 ...   \n",
              "8180  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8181  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8182  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8183  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8184  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "\n",
              "                                              Paragraph Majority_Label  \\\n",
              "0     Recent Large Language Models (LLMs) have made ...       negative   \n",
              "1     spanning tens of thousands of tokens (Anthropi...       positive   \n",
              "2     Significant strides have been made in long-for...       negative   \n",
              "3     Automated metrics such as surface form matchin...       positive   \n",
              "4     An alternative framework for evaluating long-f...       negative   \n",
              "...                                                 ...            ...   \n",
              "8180  The statistical data for the MultiWOZ dataset ...       negative   \n",
              "8181  In this section, we provide a detailed overvie...       negative   \n",
              "8182  To explore where we should apply DualLoRA in T...       negative   \n",
              "8183  To delve into the slot accuracy performance of...       negative   \n",
              "8184  This section presents a more detailed analysis...       positive   \n",
              "\n",
              "             AI_HUM  \n",
              "0     Human-written  \n",
              "1               NaN  \n",
              "2     Human-written  \n",
              "3               NaN  \n",
              "4     Human-written  \n",
              "...             ...  \n",
              "8180  Human-written  \n",
              "8181  Human-written  \n",
              "8182  Human-written  \n",
              "8183  Human-written  \n",
              "8184            NaN  \n",
              "\n",
              "[8185 rows x 6 columns]"
            ]
          },
          "execution_count": 819,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Final_2024 = pd.concat([df_2024, f_24.set_index('Original_Index')], axis=1).reset_index()\n",
        "Final_2024 = pd.DataFrame(Final_2024)\n",
        "Final_2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3924cc2-f238-4a9f-bd5d-3810e0067dcd",
      "metadata": {
        "id": "b3924cc2-f238-4a9f-bd5d-3810e0067dcd"
      },
      "outputs": [],
      "source": [
        "Final_2024=Final_2024[[\"Title\",\t\"Abstract\",\t\"Paragraph\", \"Majority_Label\",\t\"AI_HUM\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24d1a4d-b9d4-45ea-9a13-245498ddf44c",
      "metadata": {
        "id": "f24d1a4d-b9d4-45ea-9a13-245498ddf44c",
        "outputId": "f338cc42-1588-44ce-8c76-1f19baaa30f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_5424/1963762720.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Final_2024['AI_HUM']=Final_2024['AI_HUM'].fillna('N/A')\n"
          ]
        }
      ],
      "source": [
        "Final_2024['AI_HUM']=Final_2024['AI_HUM'].fillna('N/A')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25fab92-71de-45c8-82b2-ab2ac7562a39",
      "metadata": {
        "id": "b25fab92-71de-45c8-82b2-ab2ac7562a39"
      },
      "outputs": [],
      "source": [
        "Final_2024.to_csv(\"Final_2024.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "135de377-2dd8-450a-8639-d32a8d268d93",
      "metadata": {
        "id": "135de377-2dd8-450a-8639-d32a8d268d93",
        "outputId": "5c4c4a84-6f69-4437-8f52-a7b421c9d816"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Paragraph</th>\n",
              "      <th>Majority_Label</th>\n",
              "      <th>AI_HUM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Recent Large Language Models (LLMs) have made ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>spanning tens of thousands of tokens (Anthropi...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Significant strides have been made in long-for...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>Automated metrics such as surface form matchin...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PROXYQA: An Alternative Framework for Evaluati...</td>\n",
              "      <td>Large Language Models (LLMs) have succeeded re...</td>\n",
              "      <td>An alternative framework for evaluating long-f...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8180</th>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>The statistical data for the MultiWOZ dataset ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8181</th>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>In this section, we provide a detailed overvie...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8182</th>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>To explore where we should apply DualLoRA in T...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8183</th>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>To delve into the slot accuracy performance of...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Human-written</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8184</th>\n",
              "      <td>Zero-Shot Cross-Domain Dialogue State Tracking...</td>\n",
              "      <td>Zero-shot dialogue state tracking (DST) seeks ...</td>\n",
              "      <td>This section presents a more detailed analysis...</td>\n",
              "      <td>positive</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8185 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  \\\n",
              "0     PROXYQA: An Alternative Framework for Evaluati...   \n",
              "1     PROXYQA: An Alternative Framework for Evaluati...   \n",
              "2     PROXYQA: An Alternative Framework for Evaluati...   \n",
              "3     PROXYQA: An Alternative Framework for Evaluati...   \n",
              "4     PROXYQA: An Alternative Framework for Evaluati...   \n",
              "...                                                 ...   \n",
              "8180  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8181  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8182  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8183  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "8184  Zero-Shot Cross-Domain Dialogue State Tracking...   \n",
              "\n",
              "                                               Abstract  \\\n",
              "0     Large Language Models (LLMs) have succeeded re...   \n",
              "1     Large Language Models (LLMs) have succeeded re...   \n",
              "2     Large Language Models (LLMs) have succeeded re...   \n",
              "3     Large Language Models (LLMs) have succeeded re...   \n",
              "4     Large Language Models (LLMs) have succeeded re...   \n",
              "...                                                 ...   \n",
              "8180  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8181  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8182  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8183  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "8184  Zero-shot dialogue state tracking (DST) seeks ...   \n",
              "\n",
              "                                              Paragraph Majority_Label  \\\n",
              "0     Recent Large Language Models (LLMs) have made ...       negative   \n",
              "1     spanning tens of thousands of tokens (Anthropi...       positive   \n",
              "2     Significant strides have been made in long-for...       negative   \n",
              "3     Automated metrics such as surface form matchin...       positive   \n",
              "4     An alternative framework for evaluating long-f...       negative   \n",
              "...                                                 ...            ...   \n",
              "8180  The statistical data for the MultiWOZ dataset ...       negative   \n",
              "8181  In this section, we provide a detailed overvie...       negative   \n",
              "8182  To explore where we should apply DualLoRA in T...       negative   \n",
              "8183  To delve into the slot accuracy performance of...       negative   \n",
              "8184  This section presents a more detailed analysis...       positive   \n",
              "\n",
              "             AI_HUM  \n",
              "0     Human-written  \n",
              "1               N/A  \n",
              "2     Human-written  \n",
              "3               N/A  \n",
              "4     Human-written  \n",
              "...             ...  \n",
              "8180  Human-written  \n",
              "8181  Human-written  \n",
              "8182  Human-written  \n",
              "8183  Human-written  \n",
              "8184            N/A  \n",
              "\n",
              "[8185 rows x 5 columns]"
            ]
          },
          "execution_count": 823,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Final_2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da281e88-fda1-4c7d-b55d-f11a03f3a009",
      "metadata": {
        "id": "da281e88-fda1-4c7d-b55d-f11a03f3a009"
      },
      "source": [
        "## Consensus Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbe8681-96da-49e8-803d-e45402fcb8b8",
      "metadata": {
        "id": "5fbe8681-96da-49e8-803d-e45402fcb8b8",
        "outputId": "428bfd2c-a6dc-4d88-dd4d-11b645b76b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Consensus: 81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "mis_23_24=mis_23+mis_24\n",
        "lm_23_24=lm_23+lm_24\n",
        "dp_23_24=dp_23+dp_24\n",
        "model_predictions = [\n",
        "    mis_23_24,lm_23_24,dp_23_24\n",
        "]\n",
        "\n",
        "predictions = np.array(model_predictions)\n",
        "\n",
        "final_predictions = []\n",
        "consensus_scores = []\n",
        "\n",
        "for i in range(predictions.shape[1]):\n",
        "    votes = predictions[:, i]\n",
        "    majority_vote = Counter(votes).most_common(1)[0][0]\n",
        "    final_predictions.append(majority_vote)\n",
        "\n",
        "    agreement = np.mean(votes == majority_vote)\n",
        "    consensus_scores.append(agreement)\n",
        "\n",
        "print(\"Average Consensus:\", f\"{np.mean(consensus_scores):.0%}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}